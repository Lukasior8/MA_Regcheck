---
title: "Analysis MA results"
author: "Luki"
date: "2024-09-19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#todo:
- evtl processing noch einmal überarbeiten, für 2. datensatz ohne comp.
- herausfinden welche Werte man angeben MUSS (pwert, xwert , % etc..)
- kappa auf Basis von diesem ausrechnen für übereinstimmung gpt & olmo
- Assumption checks herausfinden welche und ausrechnen
- weitere analyse....

- Analysis File beschönigen.

- das ziel ist ja: Synthetic peer review: Using large language models to automatically detect deviations from preregistrations

dh. insbesondere die "n" sind interessant um eine Aussage darüber zu machen wie viel % davon entdeckt werden und viel viel fals positive etc....und es wäre auch noch nice zu wissen welche Kategorien man brauchen kann und welche nicht...

# Dependencies

```{r}
library(tidyverse)
library("readxl")
library(janitor)
library(stringr)
library(openxlsx)
library(dplyr)
library(irr)
library(psych)
library(formattable)
library(kableExtra)
library(gt)
library(irr)





```


#Read  data


```{r}
getwd() 


comparison_data_processed <-read_excel("../Data/processed/comparison_data_processed.xlsx")


data_gpt_olmo_processed <-read_excel("../Data/processed/data_gpt_olmo_processed.xlsx")
```
# Category Counts
## category counts total

hier noch die absoluten zahlen ohne.oo hinschreiben...

```{r}

# Step 1: Select the columns that end with "_comp"
comp_columns <- comparison_data_processed %>%
  select(ends_with("_comp"))

# Step 2: Create a new dataset that counts the occurrences of each category across all "_comp" columns
data_category_counts_total <- comp_columns %>%
  summarise(
    Identical_YES = sum(comp_columns == "Y", na.rm = TRUE),
    Identical_NO = sum(comp_columns == "N", na.rm = TRUE),
    Identical_X = sum(comp_columns == "X", na.rm = TRUE),
    Different_Y_to_N = sum(comp_columns == "P", na.rm = TRUE),
    Different_N_to_Y = sum(comp_columns == "Q", na.rm = TRUE),
    GPT_Y_Olmo_X = sum(comp_columns == "FY", na.rm = TRUE),
    GPT_N_Olmo_X = sum(comp_columns == "FN", na.rm = TRUE),
    GPT_X_Olmo_N = sum(comp_columns == "FXN", na.rm = TRUE),
    GPT_X_Olmo_Y = sum(comp_columns == "FXY", na.rm = TRUE),
    n_answers_total = sum(
      comp_columns == "Y" | comp_columns == "N" | comp_columns == "X" |
      comp_columns == "P" | comp_columns == "Q" | comp_columns == "FY" |
      comp_columns == "FN" | comp_columns == "FXN" | comp_columns == "FXY", na.rm = TRUE
    )
  )

# Step 3: Add percentages as a second row
data_category_counts_percent <- data_category_counts_total %>%
  mutate(across(-n_answers_total, ~ round((.x / data_category_counts_total$n_answers_total) * 100, 2))) %>%
  mutate(n_answers_total = 100)


# Step 4: Combine the two with row labels
data_category_counts_total <- rbind(
  cbind(type = "counts_total_absolute", data_category_counts_total),
  cbind(type = "counts_total_percent", data_category_counts_percent)
)

# Step 4: Print the combined table with both counts and percentages
print(data_category_counts_total)


write.xlsx(data_category_counts_total, "../Data/processed/data_category_counts_total.xlsx")

```



### 3x3 confusionsmatrix

```{r}

# Angenommen, deine 2x11-Tabelle heißt "data_category_counts_total"
# und enthält die Zählungen für jede Kategorie (z.B. Y, N, X).

# Extrahiere nur die relevanten Werte für Y, N, X (jeweils für tatsächliche und vorhergesagte Werte)
actual_Y <- data_category_counts_total$Identical_YES[1]  # Zähle die Y-Zuordnungen
actual_N <- data_category_counts_total$Identical_NO[1]   # Zähle die N-Zuordnungen
actual_X <- data_category_counts_total$Identical_X[1]    # Zähle die X-Zuordnungen

pred_Y_to_N <- data_category_counts_total$Different_Y_to_N[1]  # Y -> N Zählungen
pred_N_to_Y <- data_category_counts_total$Different_N_to_Y[1]  # N -> Y Zählungen
pred_Y_to_X <- data_category_counts_total$GPT_Y_Olmo_X[1]      # Y -> X Zählungen
pred_N_to_X <- data_category_counts_total$GPT_N_Olmo_X[1]      # N -> X Zählungen
pred_X_to_N <- data_category_counts_total$GPT_X_Olmo_N[1]      # X -> N Zählungen
pred_X_to_Y <- data_category_counts_total$GPT_X_Olmo_Y[1]      # X -> Y Zählungen

# Erstelle eine leere 3x3-Matrix
confusion_matrix <- matrix(0, nrow = 3, ncol = 3, 
                           dimnames = list(c("Actual_Y", "Actual_N", "Actual_X"), 
                                           c("Predicted_Y", "Predicted_N", "Predicted_X")))

# Fülle die Konfusionsmatrix basierend auf den Zählungen aus
confusion_matrix["Actual_Y", "Predicted_Y"] <- actual_Y
confusion_matrix["Actual_N", "Predicted_N"] <- actual_N
confusion_matrix["Actual_X", "Predicted_X"] <- actual_X

# Füge die 'False Positives' und 'False Negatives' hinzu
confusion_matrix["Actual_Y", "Predicted_N"] <- pred_Y_to_N
confusion_matrix["Actual_N", "Predicted_Y"] <- pred_N_to_Y
confusion_matrix["Actual_Y", "Predicted_X"] <- pred_Y_to_X
confusion_matrix["Actual_X", "Predicted_Y"] <- pred_X_to_Y
confusion_matrix["Actual_N", "Predicted_X"] <- pred_N_to_X
confusion_matrix["Actual_X", "Predicted_N"] <- pred_X_to_N

# Ausgabe der 3x3 Konfusionsmatrix
print(confusion_matrix)



```



## category counts per item

```{r}


# Step 1: Select the columns that end with "_comp"
comp_columns <- comparison_data_processed %>%
  select(ends_with("_comp"))

# Step 2: Create a dataset that counts occurrences of each category for each column using reframe()
data_category_counts_per_item <- comp_columns %>%
  reframe(across(everything(), ~ tibble(
    Identical_YES = sum(. == "Y", na.rm = TRUE),
    Identical_NO = sum(. == "N", na.rm = TRUE),
    Identical_X = sum(. == "X", na.rm = TRUE),
    Different_Y_to_N = sum(. == "P", na.rm = TRUE),
    Different_N_to_Y = sum(. == "Q", na.rm = TRUE),
    GPT_Y_Olmo_X = sum(. == "FY", na.rm = TRUE),
    GPT_N_Olmo_X = sum(. == "FN", na.rm = TRUE),
    GPT_X_Olmo_N = sum(. == "FXN", na.rm = TRUE),
    GPT_X_Olmo_Y = sum(. == "FXY", na.rm = TRUE)
  )))

# Step 3: Transpose the data so that each category has its own column
data_category_counts_per_item <- data_category_counts_per_item %>%
  pivot_longer(cols = everything(), names_to = "Column", values_to = "Counts") %>%
  unnest_wider(Counts)

# Step 4: Print the result
print(data_category_counts_per_item)


write.xlsx(data_category_counts_per_item, "../Data/processed/data_category_counts_per_item.xlsx")


```

# sensitivity & specifity 



## Sensitivität und Spezifität für jede Klasse (One-vs-Rest)

```{r}

print(confusion_matrix)



# Berechnung der Sensitivität und Spezifität für jede Klasse (One-vs-Rest)

# Y-Klasse (Sensitivität und Spezifität)
TP_Y <- confusion_matrix['Actual_Y', 'Predicted_Y']
FN_Y <- sum(confusion_matrix['Actual_Y', ]) - TP_Y
FP_Y <- sum(confusion_matrix[, 'Predicted_Y']) - TP_Y
TN_Y <- sum(confusion_matrix) - (TP_Y + FN_Y + FP_Y)

sensitivity_Y <- TP_Y / (TP_Y + FN_Y)
specificity_Y <- TN_Y / (TN_Y + FP_Y)

# N-Klasse (Sensitivität und Spezifität)
TP_N <- confusion_matrix['Actual_N', 'Predicted_N']
FN_N <- sum(confusion_matrix['Actual_N', ]) - TP_N
FP_N <- sum(confusion_matrix[, 'Predicted_N']) - TP_N
TN_N <- sum(confusion_matrix) - (TP_N + FN_N + FP_N)

sensitivity_N <- TP_N / (TP_N + FN_N)
specificity_N <- TN_N / (TN_N + FP_N)

# X-Klasse (Sensitivität und Spezifität)
TP_X <- confusion_matrix['Actual_X', 'Predicted_X']
FN_X <- sum(confusion_matrix['Actual_X', ]) - TP_X
FP_X <- sum(confusion_matrix[, 'Predicted_X']) - TP_X
TN_X <- sum(confusion_matrix) - (TP_X + FN_X + FP_X)

sensitivity_X <- TP_X / (TP_X + FN_X)
specificity_X <- TN_X / (TN_X + FP_X)

# Ergebnisse anzeigen
results <- data.frame(
  Class = c("Y", "N", "X"),
  Sensitivity = c(sensitivity_Y, sensitivity_N, sensitivity_X),
  Specificity = c(specificity_Y, specificity_N, specificity_X)
)

print(results)

```



## Gesamtleistung: Macro- und Micro-Averaging

```{r}
# Macro-Averaging (Mittelwert der Sensitivitäten und Spezifitäten)
macro_sensitivity <- mean(c(sensitivity_Y, sensitivity_N, sensitivity_X))
macro_specificity <- mean(c(specificity_Y, specificity_N, specificity_X))

# Micro-Averaging (Summe der TP, FN, FP, TN über alle Klassen)
TP_total <- TP_Y + TP_N + TP_X
FN_total <- FN_Y + FN_N + FN_X
FP_total <- FP_Y + FP_N + FP_X
TN_total <- TN_Y + TN_N + TN_X

micro_sensitivity <- TP_total / (TP_total + FN_total)
micro_specificity <- TN_total / (TN_total + FP_total)

# Ergebnisse für Macro- und Micro-Averaging anzeigen
overall_results <- data.frame(
  Metric = c("Macro Sensitivity", "Macro Specificity", "Micro Sensitivity", "Micro Specificity"),
  Value = c(macro_sensitivity, macro_specificity, micro_sensitivity, micro_specificity)
)

print(overall_results)




```



## 

```{r}



```




## 

```{r}



```



# Kappa retest

## test, retest korrelation Cohens Kappa per category

```{r}


# Step 1: Filter the dataset into test and retest subsets
test_data <- comparison_data_processed %>% filter(test_retest == "Test")
retest_data <- comparison_data_processed %>% filter(test_retest == "RETEST")

# Step 2: Match test and retest based on 'psp'(so this only contains the 30 test-retst Psp rows)
retest_merged_data <- merge(test_data, retest_data, by = "psp", suffixes = c("_test", "_retest"))

# Step 3: Get all columns ending with '_comp'
comp_columns <- grep("_comp$", names(comparison_data_processed), value = TRUE)

# Step 4: Loop through each '_comp' column and calculate Cohen's Kappa
retest_kappa_results <- lapply(comp_columns, function(col_name) {
  # Cohen's Kappa requires the two sets of categorical data (Test and RETEST)
  test_col <- retest_merged_data[[paste0(col_name, "_test")]]
  retest_col <- retest_merged_data[[paste0(col_name, "_retest")]]
  
  # Calculate Cohen's Kappa for the current column
  kappa <- cohen.kappa(cbind(test_col, retest_col))
  
  # Return the column name and the kappa value
  data.frame(Column = col_name, Kappa = kappa$kappa)
})

# Combine the results into a single data frame
retest_kappa_summary <- do.call(rbind, retest_kappa_results)

# Step 5: Display the results
print(retest_kappa_summary)


# die drei antweoten mit NAin kappa_summary sind eigentlich perfekt übereinstimemnd, aber da immer nur die gleiche Antwort, dh de geliche Buchstaben eingesetzt wurde gibt es per Definition eine perfekte Übereinstimmung, da es keine Variabilität gibt.


#write to disk forjamovi
write.xlsx(retest_merged_data, file = "../Data/processed/retest_merged_data.xlsx")







```


## versuch 2 mit zusätzlich z wert, agreement % , p wert
```{r}



# Step 1: Filter the dataset into test and retest subsets
test_data <- comparison_data_processed %>% filter(test_retest == "Test")
retest_data <- comparison_data_processed %>% filter(test_retest == "RETEST")

# Step 2: Match test and retest based on 'psp' (so this only contains the 30 test-retest psp rows)
retest_merged_data <- merge(test_data, retest_data, by = "psp", suffixes = c("_test", "_retest"))

# Step 3: Get all columns ending with '_comp'
comp_columns <- grep("_comp$", names(comparison_data_processed), value = TRUE)

# Step 4: Loop through each '_comp' column and calculate Cohen's Kappa using the irr package
retest_kappa_results <- lapply(comp_columns, function(col_name) {
  # Cohen's Kappa requires the two sets of categorical data (Test and RETEST)
  test_col <- retest_merged_data[[paste0(col_name, "_test")]]
  retest_col <- retest_merged_data[[paste0(col_name, "_retest")]]
  
  # Überprüfung auf gültige Daten (Nicht-NA-Daten) in beiden Spalten
  if (all(is.na(test_col)) || all(is.na(retest_col))) {
    # Falls alle Daten NA sind, gib eine Zeile mit NA zurück
    return(data.frame(Column = col_name, Kappa = NA, z_value = NA, 
                      Agreement_Percentage = NA, p_value = NA))
  }
  
  # Berechnung von Cohen's Kappa mit dem irr-Paket
  kappa_result <- kappa2(data.frame(test_col, retest_col), "unweighted")
  
  # Extrahiere den Kappa-Wert und p-Wert
  kappa_value <- kappa_result$value
  p_value <- kappa_result$p.value
  agreement_percentage <- mean(test_col == retest_col, na.rm = TRUE) * 100
  
  # Berechnung der erwarteten Übereinstimmung (p_e)
  test_table <- table(test_col, retest_col)
  n <- sum(test_table)  # Anzahl der Paare
  p_e <- sum(rowSums(test_table) * colSums(test_table)) / (n^2)
  
  # Berechnung der beobachteten Übereinstimmung (p_0)
  p_0 <- agreement_percentage / 100
  
  # Berechnung des Standardfehlers (SE)
  if (p_e == 1) {
    se_kappa <- NA  # Vermeide Division durch 0, wenn p_e = 1 (perfekte Übereinstimmung)
    z_value <- NA
  } else {
    se_kappa <- sqrt((p_0 * (1 - p_0)) / (n * (1 - p_e)^2))
  
    # Berechnung des z-Werts
    z_value <- kappa_value / se_kappa
  }
  
  # Return the column name, Kappa, z-value, agreement %, and p-value
  return(data.frame(Column = col_name, Kappa = kappa_value, z_value = z_value, 
                    Agreement_Percentage = agreement_percentage, p_value = p_value))
})

# Step 5: Combine the results into a single data frame
retest_kappa_summary <- do.call(rbind, retest_kappa_results)

# Step 6: Display the results
print(retest_kappa_summary)

# Optional: Speichern der zusammengeführten Test-Retest-Daten in einer Excel-Datei
write.xlsx(retest_merged_data, file = "../Data/processed/retest_merged_data.xlsx")


```


## test, retest korrelation gesamt kappa...

```{r}

# Schritt 4: Erstelle zwei Vektoren mit allen Test- und Retest-Werten aus den "_comp"-Spalten
all_test_values <- unlist(retest_merged_data[paste0(comp_columns, "_test")])
all_retest_values <- unlist(retest_merged_data[paste0(comp_columns, "_retest")])

# Schritt 5: Berechne Cohen's Kappa für alle Daten zusammen
retest_kappa_all <- cohen.kappa(cbind(all_test_values, all_retest_values))

# Schritt 6: Zeige das Gesamtergebnis an
retest_kappa_all

```


##test, retest korrelation gesamt kappa...v2 mit p wert etc..

p wert noch leer:/

```{r}
# Lade notwendige Bibliotheken
library(dplyr)
library(irr)  # Für Cohen's Kappa aus dem irr Paket
library(openxlsx)  # Für das Schreiben der Datei

# Step 1: Filter the dataset into test and retest subsets
test_data <- comparison_data_processed %>% filter(test_retest == "Test")
retest_data <- comparison_data_processed %>% filter(test_retest == "RETEST")

# Step 2: Match test and retest based on 'psp' (so this only contains the 30 test-retest psp rows)
retest_merged_data <- merge(test_data, retest_data, by = "psp", suffixes = c("_test", "_retest"))

# Step 3: Get all columns ending with '_comp'
comp_columns <- grep("_comp$", names(comparison_data_processed), value = TRUE)

# Schritt 4: Erstelle zwei Vektoren mit allen Test- und Retest-Werten aus den "_comp"-Spalten
all_test_values <- unlist(retest_merged_data[paste0(comp_columns, "_test")])
all_retest_values <- unlist(retest_merged_data[paste0(comp_columns, "_retest")])

# Schritt 5: Berechne Cohen's Kappa für alle Daten zusammen mit dem irr-Paket
kappa_result_all <- kappa2(data.frame(all_test_values, all_retest_values), "unweighted")

# Extrahiere den Kappa-Wert und den p-Wert
kappa_value <- kappa_result_all$value
p_value <- kappa_result_all$p.value
agreement_percentage <- mean(all_test_values == all_retest_values, na.rm = TRUE) * 100

# Berechnung der erwarteten Übereinstimmung (p_e)
test_table <- table(all_test_values, all_retest_values)
n <- sum(test_table)  # Anzahl der Paare
p_e <- sum(rowSums(test_table) * colSums(test_table)) / (n^2)

# Berechnung der beobachteten Übereinstimmung (p_0)
p_0 <- agreement_percentage / 100

# Berechnung des Standardfehlers (SE)
if (p_e == 1) {
  se_kappa <- NA  # Vermeide Division durch 0, wenn p_e = 1 (perfekte Übereinstimmung)
  z_value <- NA
} else {
  se_kappa <- sqrt((p_0 * (1 - p_0)) / (n * (1 - p_e)^2))
  
  # Berechnung des z-Werts
  z_value <- kappa_value / se_kappa
}

# Schritt 6: Zeige das Gesamtergebnis an
retest_kappa_all_summary <- data.frame(
  Kappa = kappa_value,
  Agreement_Percentage = agreement_percentage,
  z_value = z_value,
  p_value = p_value
)

# Zeige das Gesamtergebnis an
print(retest_kappa_all_summary)

```




##  test-retest Überblick kappas

```{r}
# Schritt 1: Rundung der Kappa-Werte auf 2 Nachkommastellen
retest_kappa_summary <- retest_kappa_summary %>%
  mutate(Kappa = round(Kappa, 2))

# Schritt 2: Aufteilen der Tabelle in Kategorien basierend auf Präfixen (pre, c_pre, pap, c_pap, match)
pre_kappa <- retest_kappa_summary %>% filter(grepl("^(pre|c_pre)", Column)) %>% mutate(Source = "pre")
pap_kappa <- retest_kappa_summary %>% filter(grepl("^(pap|c_pap)", Column)) %>% mutate(Source = "pap")
match_kappa <- retest_kappa_summary %>% filter(grepl("^match", Column)) %>% mutate(Source = "match")

# Schritt 3: Setze Zeilennummern für eine saubere Darstellung in jeder Tabelle
pre_kappa <- pre_kappa %>% mutate(Row = row_number())
pap_kappa <- pap_kappa %>% mutate(Row = row_number())
match_kappa <- match_kappa %>% mutate(Row = row_number())

# Schritt 4: Zusammenfügen der drei Tabellen nebeneinander basierend auf der Zeilennummer
# Verwende full_join, um die Tabellen nach den Zeilen zu verbinden
retest_kappa_summary_overview <- pre_kappa %>%
  full_join(pap_kappa, by = "Row", suffix = c("_pre", "_pap")) %>%
  full_join(match_kappa, by = "Row", suffix = c("", "_match"))

# Schritt 5: Auswahl und Umbenennung der Spalten für Übersichtlichkeit
# Stellen Sie sicher, dass die Spalten korrekt benannt und verfügbar sind
retest_kappa_summary_overview <- retest_kappa_summary_overview %>%
  select(Row, 
         Column_pre = Column_pre, Kappa_pre = Kappa_pre, 
         Column_pap = Column_pap, Kappa_pap = Kappa_pap, 
         Column_match = Column, Kappa_match = Kappa) 








```









## test retest färbung überblick

```{r}

# Färbung der Kappa-Werte direkt in der Tabelle
formattable(retest_kappa_summary_overview, list(
  Kappa_pre = color_tile("red", "green"),  # Kappa_pre-Spalte von dunkelrot bis dunkelgrün
  Kappa_pap = color_tile("red", "green"),  # Kappa_pap-Spalte von dunkelrot bis dunkelgrün
  Kappa_match = color_tile("red", "green") # Kappa_match-Spalte von dunkelrot bis dunkelgrün
))


```





#Kappa interrater gpt zu olmo


- wieso anderes kappa als? bei jamovi?
- sollte ich im gesamten script die selbe mehto wählen?
- wie ist die differenz zu interpretieren?

## Kappa interrater gpt zu olmo per category

```{r}



# Function to calculate Cohen's Kappa, p-value, z-value, and percentage agreement
calculate_kappa_stats <- function(data, column_gpt, column_olmo) {
  # Ensure both columns exist
  if(column_gpt %in% colnames(data) & column_olmo %in% colnames(data)) {
    # Calculate Cohen's Kappa
    kappa_result <- psych::cohen.kappa(cbind(data[[column_gpt]], data[[column_olmo]]))
    
    # Extract values
    kappa_value <- kappa_result$kappa
    p_value <- kappa_result$p.value
    z_value <- kappa_result$z
    percent_agreement <- sum(data[[column_gpt]] == data[[column_olmo]], na.rm = TRUE) / nrow(data) * 100
    
    return(c(kappa_value, p_value, z_value, percent_agreement))
  } else {
    return(rep(NA, 4))
  }
}

# Get all columns that have _gpt or _olmo suffix
gpt_columns <- grep("_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns <- gsub("_gpt$", "_olmo", gpt_columns)

# Create a dataframe to store the results
kappa_results <- data.frame(Variable = gpt_columns, Kappa = NA, P_Value = NA, Z_Value = NA, Percent_Agreement = NA)

# Loop over all _gpt and _olmo column pairs and calculate statistics
for(i in 1:length(gpt_columns)) {
  kappa_stats <- calculate_kappa_stats(data_gpt_olmo_processed, gpt_columns[i], olmo_columns[i])
  kappa_results[i, 2:5] <- kappa_stats
}

# Show the results
print(kappa_results)



```





##Kappa interrater gpt zu olmo  per category 2. ansatz

```{r}


# Function to calculate Cohen's Kappa, p-value, z-value, and percentage agreement using 'irr' package
calculate_kappa_irr_v2 <- function(data, column_gpt, column_olmo) {
  # Ensure both columns exist
  if(column_gpt %in% colnames(data) & column_olmo %in% colnames(data)) {
    # Remove rows with NA values to avoid errors in kappa calculation
    valid_data_v2 <- data[!is.na(data[[column_gpt]]) & !is.na(data[[column_olmo]]), ]
    
    # Calculate Cohen's Kappa using the 'kappa2' function from the irr package
    kappa_result_v2 <- irr::kappa2(valid_data_v2[, c(column_gpt, column_olmo)], "unweighted")
    
    # Extract values
    kappa_value_v2 <- kappa_result_v2$value
    p_value_v2 <- kappa_result_v2$p.value
    z_value_v2 <- kappa_result_v2$statistic
    percent_agreement_v2 <- sum(valid_data_v2[[column_gpt]] == valid_data_v2[[column_olmo]]) / nrow(valid_data_v2) * 100
    
    return(c(kappa_value_v2, p_value_v2, z_value_v2, percent_agreement_v2))
  } else {
    return(rep(NA, 4))
  }
}

# Get all columns that have _gpt or _olmo suffix
gpt_columns_v2 <- grep("_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns_v2 <- gsub("_gpt$", "_olmo", gpt_columns_v2)

# Create a dataframe to store the results
kappa_results_v2 <- data.frame(Variable = gpt_columns_v2, Kappa = NA, P_Value = NA, Z_Value = NA, Percent_Agreement = NA)

# Loop over all _gpt and _olmo column pairs and calculate statistics
for(i in 1:length(gpt_columns_v2)) {
  kappa_stats_v2 <- calculate_kappa_irr_v2(data_gpt_olmo_processed, gpt_columns_v2[i], olmo_columns_v2[i])
  kappa_results_v2[i, 2:5] <- kappa_stats_v2
}

# Show the results
print(kappa_results_v2)

```





## Kappa interrater gpt zu olmo gesamt

```{r}

# Spalten mit _gpt und _olmo suffix finden
gpt_columns <- grep("_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns <- gsub("_gpt$", "_olmo", gpt_columns)

# Überprüfen, ob die entsprechenden _olmo-Spalten existieren
valid_gpt_columns <- gpt_columns[olmo_columns %in% colnames(data_gpt_olmo_processed)]
valid_olmo_columns <- olmo_columns[olmo_columns %in% colnames(data_gpt_olmo_processed)]

# Sicherstellen, dass nur gültige Paare verwendet werden
if(length(valid_gpt_columns) == 0) {
  stop("Keine gültigen Paare von GPT- und OLMO-Spalten gefunden!")
}

# Kombinieren der Spalten in einem DataFrame für den Gesamtvergleich
combined_data <- data.frame()

for(i in 1:length(valid_gpt_columns)) {
  # Füge die Werte der GPT und OLMO Spalten zur kombinierten Datensatz hinzu
  combined_data <- rbind(combined_data, data.frame(GPT = data_gpt_olmo_processed[[valid_gpt_columns[i]]],
                                                   OLMO = data_gpt_olmo_processed[[valid_olmo_columns[i]]]))
}

# Entferne NA-Werte
combined_data <- na.omit(combined_data)

# Berechne Cohen's Kappa für den gesamten Test
kappa_result <- irr::kappa2(combined_data, "unweighted")

# Berechne den Prozentsatz der Übereinstimmung
percent_agreement <- sum(combined_data$GPT == combined_data$OLMO) / nrow(combined_data) * 100

# Ergebnisse anzeigen
cat("Cohen's Kappa für den gesamten Test:\n")
cat("Kappa:", kappa_result$value, "\n")
cat("p-Wert:", kappa_result$p.value, "\n")
cat("z-Wert:", kappa_result$statistic, "\n")
cat("Prozentuale Übereinstimmung:", percent_agreement, "%\n")


```




## besser nur bei promt version 4? im Vergleich zu den anderen?

```{r}
# Spalten mit _gpt und _olmo suffix finden
gpt_columns_PromptV <- grep("_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns_PromptV <- gsub("_gpt$", "_olmo", gpt_columns_PromptV)

# Überprüfen, ob die entsprechenden _olmo-Spalten existieren
valid_gpt_columns_PromptV <- gpt_columns_PromptV[olmo_columns_PromptV %in% colnames(data_gpt_olmo_processed)]
valid_olmo_columns_PromptV <- olmo_columns_PromptV[olmo_columns_PromptV %in% colnames(data_gpt_olmo_processed)]

# Sicherstellen, dass nur gültige Paare verwendet werden
if(length(valid_gpt_columns_PromptV) == 0) {
  stop("Keine gültigen Paare von GPT- und OLMO-Spalten gefunden!")
}

# Definiere die verschiedenen Versionen, die in prompt_version auftreten (A, B, C, D)
versions_PromptV <- c("A", "B", "C", "D")

# Schleife durch jede Version und berechne das Kappa für jede
for (version_PromptV in versions_PromptV) {
  # Filtere die Zeilen, die zur aktuellen Version gehören
  version_data_PromptV <- data_gpt_olmo_processed[data_gpt_olmo_processed$prompt_version == version_PromptV, ]
  
  # Kombinieren der Spalten in einem DataFrame für den Vergleich der aktuellen Version
  combined_data_PromptV <- data.frame()
  
  for(i in 1:length(valid_gpt_columns_PromptV)) {
    # Füge die Werte der GPT und OLMO Spalten zur kombinierten Datensatz hinzu
    combined_data_PromptV <- rbind(combined_data_PromptV, data.frame(GPT = version_data_PromptV[[valid_gpt_columns_PromptV[i]]],
                                                                     OLMO = version_data_PromptV[[valid_olmo_columns_PromptV[i]]]))
  }
  
  # Entferne NA-Werte
  combined_data_PromptV <- na.omit(combined_data_PromptV)
  
  # Berechne Cohen's Kappa für die aktuelle Version
  kappa_result_PromptV <- irr::kappa2(combined_data_PromptV, "unweighted")
  
  # Berechne den Prozentsatz der Übereinstimmung
  percent_agreement_PromptV <- sum(combined_data_PromptV$GPT == combined_data_PromptV$OLMO) / nrow(combined_data_PromptV) * 100
  
  # Ergebnisse anzeigen
  cat("\nCohen's Kappa für Version", version_PromptV, ":\n")
  cat("Kappa:", kappa_result_PromptV$value, "\n")
  cat("p-Wert:", kappa_result_PromptV$p.value, "\n")
  cat("z-Wert:", kappa_result_PromptV$statistic, "\n")
  cat("Prozentuale Übereinstimmung:", percent_agreement_PromptV, "%\n")
}



```


## differenz zw promt versionen? 



```


## 

```{r}



```


## 

```{r}



```


## 

```{r}



```




## 

```{r}



```





# Session info

```{r}

sessionInfo()

```


