---
title: "Analysis MA results"
author: "Luki"
date: "2024-09-19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#todo:
- 
- herausfinden welche Werte man angeben MUSS (pwert, xwert , % etc..)
- kappa auf Basis von diesem ausrechnen für übereinstimmung gpt & olmo


- Assumption checks herausfinden welche und ausrechnen
->>>< also zb ob es problematisch ist dass teilweise fast nur y oder fast nur n oder so ist.....


-> why df isch p wert so gross! hmm vilech wäge nidrige 
- weitere analyse....

- Analysis File beschönigen.

- das ziel ist ja: Synthetic peer review: Using large language models to automatically detect deviations from preregistrations

dh. insbesondere die "n" sind interessant um eine Aussage darüber zu machen wie viel % davon entdeckt werden und viel viel fals positive etc....und es wäre auch noch nice zu wissen welche Kategorien man brauchen kann und welche nicht...


extraction vs compare prompt..

# Dependencies

```{r}
library(tidyverse)
library("readxl")
library(janitor)
library(stringr)
library(openxlsx)
library(dplyr)
library(irr)
library(psych)
library(formattable)
library(kableExtra)
library(gt)
library(irr)
library(caret) 
library(flextable)
library(ggplot2)






```


#Read  data


```{r}
getwd() 


comparison_data_processed <-read_excel("../Data/processed/comparison_data_processed.xlsx")


data_gpt_olmo_processed <-read_excel("../Data/processed/data_gpt_olmo_processed.xlsx")


data_gpt_ve_processed <-read_excel("../Data/processed/data_gpt_ve_processed.xlsx")

# Filterung des Datensatzes, um Zeilen mit master_exclude == "exclude" auszuschließen
data_gpt_ve_filtered <- data_gpt_ve_processed %>%
  filter(master_exclude != "exclude")

```
# Category COUNTS
## category counts total

hier noch die absoluten zahlen ohne.oo hinschreiben...

```{r}

# Step 1: Select the columns that end with "_comp"
comp_columns <- comparison_data_processed %>%
  select(ends_with("_comp"))

# Step 2: Create a new dataset that counts the occurrences of each category across all "_comp" columns
data_category_counts_total <- comp_columns %>%
  summarise(
    Identical_YES = sum(comp_columns == "Y", na.rm = TRUE),
    Identical_NO = sum(comp_columns == "N", na.rm = TRUE),
    Identical_X = sum(comp_columns == "X", na.rm = TRUE),
    Different_Y_to_N = sum(comp_columns == "P", na.rm = TRUE),
    Different_N_to_Y = sum(comp_columns == "Q", na.rm = TRUE),
    GPT_Y_Olmo_X = sum(comp_columns == "FY", na.rm = TRUE),
    GPT_N_Olmo_X = sum(comp_columns == "FN", na.rm = TRUE),
    GPT_X_Olmo_N = sum(comp_columns == "FXN", na.rm = TRUE),
    GPT_X_Olmo_Y = sum(comp_columns == "FXY", na.rm = TRUE),
    n_answers_total = sum(
      comp_columns == "Y" | comp_columns == "N" | comp_columns == "X" |
      comp_columns == "P" | comp_columns == "Q" | comp_columns == "FY" |
      comp_columns == "FN" | comp_columns == "FXN" | comp_columns == "FXY", na.rm = TRUE
    )
  )

# Step 3: Add percentages as a second row
data_category_counts_percent <- data_category_counts_total %>%
  mutate(across(-n_answers_total, ~ round((.x / data_category_counts_total$n_answers_total) * 100, 2))) %>%
  mutate(n_answers_total = 100)


# Step 4: Combine the two with row labels
data_category_counts_total <- rbind(
  cbind(type = "counts_total_absolute", data_category_counts_total),
  cbind(type = "counts_total_percent", data_category_counts_percent)
)

# Step 4: Print the combined table with both counts and percentages
print(data_category_counts_total)



```



### 3x3 confusionsmatrix absolut

hier müsste ich evventuell noch zusätzlich die Anzahl vvon Olmo... als Vergleich.

```{r}

# Angenommen, deine 2x11-Tabelle heißt "data_category_counts_total"
# und enthält die Zählungen für jede Kategorie (z.B. Y, N, X).

# Extrahiere nur die relevanten Werte für Y, N, X (jeweils für tatsächliche und vorhergesagte Werte)
actual_Y <- data_category_counts_total$Identical_YES[1]  # Zähle die Y-Zuordnungen
actual_N <- data_category_counts_total$Identical_NO[1]   # Zähle die N-Zuordnungen
actual_X <- data_category_counts_total$Identical_X[1]    # Zähle die X-Zuordnungen

pred_Y_to_N <- data_category_counts_total$Different_Y_to_N[1]  # Y -> N Zählungen
pred_N_to_Y <- data_category_counts_total$Different_N_to_Y[1]  # N -> Y Zählungen
pred_Y_to_X <- data_category_counts_total$GPT_Y_Olmo_X[1]      # Y -> X Zählungen
pred_N_to_X <- data_category_counts_total$GPT_N_Olmo_X[1]      # N -> X Zählungen
pred_X_to_N <- data_category_counts_total$GPT_X_Olmo_N[1]      # X -> N Zählungen
pred_X_to_Y <- data_category_counts_total$GPT_X_Olmo_Y[1]      # X -> Y Zählungen

# Erstelle eine leere 3x3-Matrix
confusion_matrix <- matrix(0, nrow = 3, ncol = 3, 
                           dimnames = list(c("Actual_Y", "Actual_N", "Actual_X"), 
                                           c("Predicted_Y", "Predicted_N", "Predicted_X")))

# Fülle die Konfusionsmatrix basierend auf den Zählungen aus
confusion_matrix["Actual_Y", "Predicted_Y"] <- actual_Y
confusion_matrix["Actual_N", "Predicted_N"] <- actual_N
confusion_matrix["Actual_X", "Predicted_X"] <- actual_X

# Füge die 'False Positives' und 'False Negatives' hinzu
confusion_matrix["Actual_Y", "Predicted_N"] <- pred_Y_to_N
confusion_matrix["Actual_N", "Predicted_Y"] <- pred_N_to_Y
confusion_matrix["Actual_Y", "Predicted_X"] <- pred_Y_to_X
confusion_matrix["Actual_X", "Predicted_Y"] <- pred_X_to_Y
confusion_matrix["Actual_N", "Predicted_X"] <- pred_N_to_X
confusion_matrix["Actual_X", "Predicted_N"] <- pred_X_to_N

# Ausgabe der 3x3 Konfusionsmatrix
print(confusion_matrix)



```
bei den category counts total sieht man welche kategorie insgesamt viel verwendet wurde…. Aber halt nicht so aussagekräftig, da n kommt auch bei olmo seltener vor...ncith so aussagekräfitg... man müsste prozentual zu allen berechnungen( also dass y,n,x gleich häufig vorkommen) aber man sieht schon, dass nur ca 50 & richtig predictet... halt sehr schlecht... 





###  confusion matrix absolut inkl. summen!
```{r}
# Konfusionsmatrix anzeigen
print(confusion_matrix)

# Berechnung der Summe der Zeilen und Spalten
row_sums <- rowSums(confusion_matrix)  # Summe über die Zeilen (horizontal)
col_sums <- colSums(confusion_matrix)  # Summe über die Spalten (vertikal)
n_total <- sum(confusion_matrix)       # Gesamtsumme

# Erstelle eine erweiterte Konfusionsmatrix, die Zeilen- und Spaltensummen enthält
confusion_matrix_with_sums <- rbind(confusion_matrix, "Sum" = col_sums)  # Spaltensummen als neue Zeile hinzufügen
confusion_matrix_with_sums <- cbind(confusion_matrix_with_sums, "Sum" = c(row_sums, n_total))  # Zeilensummen und Gesamtsumme hinzufügen

# Ergebnis als Matrix anzeigen, um alle Summen zu überprüfen
library(knitr)
library(kableExtra)

# Zeige die erweiterte Konfusionsmatrix mit Summen im APA-Stil an (ohne doppelte Kopfzeile)
kable(confusion_matrix_with_sums, format = "html", caption = "Confusion Matrix with Row and Column Sums (APA style)") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F) %>%
  row_spec(0, bold = TRUE, extra_css = "border-bottom: 1px solid black;") %>% # Horizontale Linie nach der Kopfzeile
  row_spec(nrow(confusion_matrix_with_sums), extra_css = "border-top: 1px solid black;") # Linie über der letzten Zeile (Summe)

```








# confusionmatrix in percent
```{r}
# Angenommen, deine 2x11-Tabelle heißt "data_category_counts_total"
# und enthält die Prozentsätze für jede Kategorie (z.B. Y, N, X).

# Extrahiere nur die relevanten Werte für Y, N, X (jeweils für tatsächliche und vorhergesagte Werte)
actual_Y_in_percent <- data_category_counts_total$Identical_YES[2]  # Prozentsatz der Y-Zuordnungen
actual_N_in_percent <- data_category_counts_total$Identical_NO[2]   # Prozentsatz der N-Zuordnungen
actual_X_in_percent <- data_category_counts_total$Identical_X[2]    # Prozentsatz der X-Zuordnungen

pred_Y_to_N_in_percent <- data_category_counts_total$Different_Y_to_N[2]  # Y -> N Prozentsatz
pred_N_to_Y_in_percent <- data_category_counts_total$Different_N_to_Y[2]  # N -> Y Prozentsatz
pred_Y_to_X_in_percent <- data_category_counts_total$GPT_Y_Olmo_X[2]      # Y -> X Prozentsatz
pred_N_to_X_in_percent <- data_category_counts_total$GPT_N_Olmo_X[2]      # N -> X Prozentsatz
pred_X_to_N_in_percent <- data_category_counts_total$GPT_X_Olmo_N[2]      # X -> N Prozentsatz
pred_X_to_Y_in_percent <- data_category_counts_total$GPT_X_Olmo_Y[2]      # X -> Y Prozentsatz

# Erstelle eine leere 3x3-Matrix
confusion_matrix_in_percent <- matrix(0, nrow = 3, ncol = 3, 
                           dimnames = list(c("Actual_Y", "Actual_N", "Actual_X"), 
                                           c("Predicted_Y", "Predicted_N", "Predicted_X")))

# Fülle die Konfusionsmatrix basierend auf den Prozentsätzen aus
confusion_matrix_in_percent["Actual_Y", "Predicted_Y"] <- actual_Y_in_percent
confusion_matrix_in_percent["Actual_N", "Predicted_N"] <- actual_N_in_percent
confusion_matrix_in_percent["Actual_X", "Predicted_X"] <- actual_X_in_percent

# Füge die 'False Positives' und 'False Negatives' hinzu
confusion_matrix_in_percent["Actual_Y", "Predicted_N"] <- pred_Y_to_N_in_percent
confusion_matrix_in_percent["Actual_N", "Predicted_Y"] <- pred_N_to_Y_in_percent
confusion_matrix_in_percent["Actual_Y", "Predicted_X"] <- pred_Y_to_X_in_percent
confusion_matrix_in_percent["Actual_X", "Predicted_Y"] <- pred_X_to_Y_in_percent
confusion_matrix_in_percent["Actual_N", "Predicted_X"] <- pred_N_to_X_in_percent
confusion_matrix_in_percent["Actual_X", "Predicted_N"] <- pred_X_to_N_in_percent

# Ausgabe der 3x3 Konfusionsmatrix in Prozent
print(confusion_matrix_in_percent)

# Berechnung der Summe der Zeilen und Spalten
row_sums_in_percent <- rowSums(confusion_matrix_in_percent)  # Summe über die Zeilen (horizontal)
col_sums_in_percent <- colSums(confusion_matrix_in_percent)  # Summe über die Spalten (vertikal)
n_total_in_percent <- sum(confusion_matrix_in_percent)       # Gesamtsumme

# Erstelle eine erweiterte Konfusionsmatrix, die Zeilen- und Spaltensummen enthält
confusion_matrix_with_sums_in_percent <- rbind(confusion_matrix_in_percent, "Sum" = col_sums_in_percent)  # Spaltensummen als neue Zeile hinzufügen
confusion_matrix_with_sums_in_percent <- cbind(confusion_matrix_with_sums_in_percent, "Sum" = c(row_sums_in_percent, n_total_in_percent))  # Zeilensummen und Gesamtsumme hinzufügen

# Ergebnis als Matrix anzeigen, um alle Summen zu überprüfen
library(knitr)
library(kableExtra)

# Zeige die erweiterte Konfusionsmatrix mit Summen im APA-Stil an (ohne doppelte Kopfzeile)
kable(confusion_matrix_with_sums_in_percent, format = "html", caption = "Confusion Matrix with Row and Column Sums (Percent, APA style)") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F) %>%
  row_spec(0, bold = TRUE, extra_css = "border-bottom: 1px solid black;") %>% # Horizontale Linie nach der Kopfzeile
  row_spec(nrow(confusion_matrix_with_sums_in_percent), extra_css = "border-top: 1px solid black;") # Linie über der letzten Zeile (Summe)


```





## category counts per item

```{r}


# Step 1: Select the columns that end with "_comp"
comp_columns <- comparison_data_processed %>%
  select(ends_with("_comp"))

# Step 2: Create a dataset that counts occurrences of each category for each column using reframe()
data_category_counts_per_item <- comp_columns %>%
  reframe(across(everything(), ~ tibble(
    Identical_YES = sum(. == "Y", na.rm = TRUE),
    Identical_NO = sum(. == "N", na.rm = TRUE),
    Identical_X = sum(. == "X", na.rm = TRUE),
    Different_Y_to_N = sum(. == "P", na.rm = TRUE),
    Different_N_to_Y = sum(. == "Q", na.rm = TRUE),
    GPT_Y_Olmo_X = sum(. == "FY", na.rm = TRUE),
    GPT_N_Olmo_X = sum(. == "FN", na.rm = TRUE),
    GPT_X_Olmo_N = sum(. == "FXN", na.rm = TRUE),
    GPT_X_Olmo_Y = sum(. == "FXY", na.rm = TRUE)
  )))

# Step 3: Transpose the data so that each category has its own column
data_category_counts_per_item <- data_category_counts_per_item %>%
  pivot_longer(cols = everything(), names_to = "Column", values_to = "Counts") %>%
  unnest_wider(Counts)

# Step 4: Print the result
print(data_category_counts_per_item)


write.xlsx(data_category_counts_per_item, "../Data/processed/data_category_counts_per_item.xlsx")


```

# Quality MEASURES



## Sensitivität und Spezifität für jede Klasse (One-vs-Rest)


hier könnte man noch sensitivität für jedes Item...!

```{r}

print(confusion_matrix)
n_total <- sum(confusion_matrix)

# Berechnung der Sensitivität, Spezifität und Präzision für jede Klasse

# Y-Klasse
TP_Y <- confusion_matrix['Actual_Y', 'Predicted_Y']
FN_Y <- sum(confusion_matrix['Actual_Y', ]) - TP_Y
FP_Y <- sum(confusion_matrix[, 'Predicted_Y']) - TP_Y
TN_Y <- sum(confusion_matrix) - (TP_Y + FN_Y + FP_Y)

sensitivity_Y <- TP_Y / (TP_Y + FN_Y)
specificity_Y <- TN_Y / (TN_Y + FP_Y)
precision_Y <- TP_Y / (TP_Y + FP_Y)

# N-Klasse
TP_N <- confusion_matrix['Actual_N', 'Predicted_N']
FN_N <- sum(confusion_matrix['Actual_N', ]) - TP_N
FP_N <- sum(confusion_matrix[, 'Predicted_N']) - TP_N
TN_N <- sum(confusion_matrix) - (TP_N + FN_N + FP_N)

sensitivity_N <- TP_N / (TP_N + FN_N)
specificity_N <- TN_N / (TN_N + FP_N)
precision_N <- TP_N / (TP_N + FP_N)

# X-Klasse
TP_X <- confusion_matrix['Actual_X', 'Predicted_X']
FN_X <- sum(confusion_matrix['Actual_X', ]) - TP_X
FP_X <- sum(confusion_matrix[, 'Predicted_X']) - TP_X
TN_X <- sum(confusion_matrix) - (TP_X + FN_X + FP_X)

sensitivity_X <- TP_X / (TP_X + FN_X)
specificity_X <- TN_X / (TN_X + FP_X)
precision_X <- TP_X / (TP_X + FP_X)

# Ergebnisse in einer Tabelle anzeigen und auf 2 Nachkommastellen runden
results <- data.frame(
  Class = c("Y", "N", "X"),
  Sensitivity = round(c(sensitivity_Y, sensitivity_N, sensitivity_X), 2),
  Specificity = round(c(specificity_Y, specificity_N, specificity_X), 2),
  Precision = round(c(precision_Y, precision_N, precision_X), 2)
)

print(results)
```

Sensitivity (Sensitivität) gibt an, wie gut das Modell positive Fälle (z.B. "Y") korrekt erkennt. Es ist das Verhältnis von richtig-positiven Vorhersagen zu allen tatsächlichen positiven Fällen. Höhere Werte zeigen eine bessere Erkennungsrate der positiven Fälle.

Specificity (Spezifität) gibt an, wie gut das Modell negative Fälle (z.B. "Nicht-Y") korrekt erkennt. Es ist das Verhältnis von richtig-negativen Vorhersagen zu allen tatsächlichen negativen Fällen. Höhere Werte bedeuten, dass das Modell weniger falsch-positive Vorhersagen macht.

Precision (Präzision) gibt an, wie viele der als positiv (z.B. "Y") vorhergesagten Fälle tatsächlich positiv sind. Es ist das Verhältnis von richtig-positiven Vorhersagen zu allen positiven Vorhersagen. Hohe Präzision bedeutet, dass das Modell selten falsch-positive Vorhersagen macht.

## Gesamtleistung: Macro- und Micro-Averaging

```{r}
## Berechnung von Macro-Averaging (Mittelwert der Sensitivitäten, Spezifitäten und Präzisionen)
macro_sensitivity <- mean(c(sensitivity_Y, sensitivity_N, sensitivity_X))
macro_specificity <- mean(c(specificity_Y, specificity_N, specificity_X))
macro_precision <- mean(c(precision_Y, precision_N, precision_X))

# Micro-Averaging (Summe der TP, FN, FP, TN über alle Klassen)
TP_total <- TP_Y + TP_N + TP_X
FN_total <- FN_Y + FN_N + FN_X
FP_total <- FP_Y + FP_N + FP_X
TN_total <- TN_Y + TN_N + TN_X

micro_sensitivity <- TP_total / (TP_total + FN_total)
micro_specificity <- TN_total / (TN_total + FP_total)
micro_precision <- TP_total / (TP_total + FP_total)



# Berechnung der Accuracy (Genauigkeit)s
accuracy1 <- (TP_total + TN_total) / (TP_total + TN_total + FP_total + FN_total)

accuracy2 <- TP_total/(n_total)

balanced_accuracy <- (sensitivity_Y + sensitivity_N + sensitivity_X)/3

# Ergebnisse für Macro, Micro und Accuracy anzeigen
overall_results <- data.frame(
  Metric = c("Macro Sensitivity", "Macro Specificity", "Macro Precision",
             "Micro Sensitivity", "Micro Specificity", "Micro Precision",
            "accuracy1","accuracy2","balanced_accuracy"),
  Value = c(macro_sensitivity, macro_specificity, macro_precision,
            micro_sensitivity, micro_specificity, micro_precision,
            accuracy1,accuracy2,balanced_accuracy)
)

print(overall_results)


```







## 

```{r}



```



# Kappa RETEST
habe ich hier jeweils nur gpt genommen? oder auch olmo ausversehen?!

## test, retest korrelation Cohens Kappa per category

```{r}


# Step 1: Filter the dataset into test and retest subsets
test_data <- comparison_data_processed %>% filter(test_retest == "Test")
retest_data <- comparison_data_processed %>% filter(test_retest == "RETEST")

# Step 2: Match test and retest based on 'psp'(so this only contains the 30 test-retst Psp rows)
retest_merged_data <- merge(test_data, retest_data, by = "psp", suffixes = c("_test", "_retest"))

# Step 3: Get all columns ending with '_comp'
comp_columns <- grep("_comp$", names(comparison_data_processed), value = TRUE)

# Step 4: Loop through each '_comp' column and calculate Cohen's Kappa
retest_kappa_results <- lapply(comp_columns, function(col_name) {
  # Cohen's Kappa requires the two sets of categorical data (Test and RETEST)
  test_col <- retest_merged_data[[paste0(col_name, "_test")]]
  retest_col <- retest_merged_data[[paste0(col_name, "_retest")]]
  
  # Calculate Cohen's Kappa for the current column
  kappa <- cohen.kappa(cbind(test_col, retest_col))
  
  # Return the column name and the kappa value
  data.frame(Column = col_name, Kappa = kappa$kappa)
})

# Combine the results into a single data frame
retest_kappa_summary <- do.call(rbind, retest_kappa_results)

# Step 5: Display the results
print(retest_kappa_summary)


# die drei antweoten mit NAin kappa_summary sind eigentlich perfekt übereinstimemnd, aber da immer nur die gleiche Antwort, dh de geliche Buchstaben eingesetzt wurde gibt es per Definition eine perfekte Übereinstimmung, da es keine Variabilität gibt.


#write to disk forjamovi
write.xlsx(retest_merged_data, file = "../Data/processed/retest_merged_data.xlsx")







```


## versuch 2 mit zusätzlich z wert, agreement % , p wert
```{r}



# Step 1: Filter the dataset into test and retest subsets
test_data <- comparison_data_processed %>% filter(test_retest == "Test")
retest_data <- comparison_data_processed %>% filter(test_retest == "RETEST")

# Step 2: Match test and retest based on 'psp' (so this only contains the 30 test-retest psp rows)
retest_merged_data <- merge(test_data, retest_data, by = "psp", suffixes = c("_test", "_retest"))

# Step 3: Get all columns ending with '_comp'
comp_columns <- grep("_comp$", names(comparison_data_processed), value = TRUE)

# Step 4: Loop through each '_comp' column and calculate Cohen's Kappa using the irr package
retest_kappa_results <- lapply(comp_columns, function(col_name) {
  # Cohen's Kappa requires the two sets of categorical data (Test and RETEST)
  test_col <- retest_merged_data[[paste0(col_name, "_test")]]
  retest_col <- retest_merged_data[[paste0(col_name, "_retest")]]
  
  # Überprüfung auf gültige Daten (Nicht-NA-Daten) in beiden Spalten
  if (all(is.na(test_col)) || all(is.na(retest_col))) {
    # Falls alle Daten NA sind, gib eine Zeile mit NA zurück
    return(data.frame(Column = col_name, Kappa = NA, z_value = NA, 
                      Agreement_Percentage = NA, p_value = NA))
  }
  
  # Berechnung von Cohen's Kappa mit dem irr-Paket
  kappa_result <- kappa2(data.frame(test_col, retest_col), "unweighted")
  
  # Extrahiere den Kappa-Wert und p-Wert
  kappa_value <- kappa_result$value
  p_value <- kappa_result$p.value
  agreement_percentage <- mean(test_col == retest_col, na.rm = TRUE) * 100
  
  # Berechnung der erwarteten Übereinstimmung (p_e)
  test_table <- table(test_col, retest_col)
  n <- sum(test_table)  # Anzahl der Paare
  p_e <- sum(rowSums(test_table) * colSums(test_table)) / (n^2)
  
  # Berechnung der beobachteten Übereinstimmung (p_0)
  p_0 <- agreement_percentage / 100
  
  # Berechnung des Standardfehlers (SE)
  if (p_e == 1) {
    se_kappa <- NA  # Vermeide Division durch 0, wenn p_e = 1 (perfekte Übereinstimmung)
    z_value <- NA
  } else {
    se_kappa <- sqrt((p_0 * (1 - p_0)) / (n * (1 - p_e)^2))
  
    # Berechnung des z-Werts
    z_value <- kappa_value / se_kappa
  }
  
  # Return the column name, Kappa, z-value, agreement %, and p-value
  return(data.frame(Column = col_name, Kappa = kappa_value, z_value = z_value, 
                    Agreement_Percentage = agreement_percentage, p_value = p_value))
})

# Step 5: Combine the results into a single data frame
retest_kappa_summary <- do.call(rbind, retest_kappa_results)

# Step 6: Display the results
print(retest_kappa_summary)




```
## average retest-kappa over items

```{r}

# Berechnung des Durchschnitts der Spalte 'Kappa'
mean_kappa <- mean(retest_kappa_summary$Kappa, na.rm = TRUE)

# Ausgabe des Ergebnisses
mean_kappa



```
Ob der durchschnittliche Kappa-Wert (average Kappa) oder das Gesamtkappa größer ist, hängt von der Verteilung der Übereinstimmungen und Nicht-Übereinstimmungen innerhalb der einzelnen Items ab. Es gibt bestimmte Situationen, in denen der eine Wert größer ist als der andere, und das hängt von der Struktur der Daten ab.

## test, retest korrelation gesamt kappa...

```{r}

# Schritt 4: Erstelle zwei Vektoren mit allen Test- und Retest-Werten aus den "_comp"-Spalten
all_test_values <- unlist(retest_merged_data[paste0(comp_columns, "_test")])
all_retest_values <- unlist(retest_merged_data[paste0(comp_columns, "_retest")])

# Schritt 5: Berechne Cohen's Kappa für alle Daten zusammen
retest_kappa_all <- cohen.kappa(cbind(all_test_values, all_retest_values))

# Schritt 6: Zeige das Gesamtergebnis an
retest_kappa_all

```
weighted macht heir absolut keinen sinn...

##test, retest korrelation gesamt kappa...v2 mit p wert etc..

p wert noch leer:/

```{r}
# Funktion zur Berechnung des 95%-Konfidenzintervalls für Cohen's Kappa
calculate_kappa_confidence_interval <- function(kappa_value, se_kappa) {
  if (!is.na(se_kappa)) {
    ci_lower <- kappa_value - 1.96 * se_kappa
    ci_upper <- kappa_value + 1.96 * se_kappa
    return(c(ci_lower, ci_upper))
  } else {
    return(c(NA, NA))  # Kein CI, falls SE nicht berechnet werden kann
  }
}

# Berechnung der erwarteten Übereinstimmung (p_e)
test_table <- table(all_test_values, all_retest_values)
n <- sum(test_table)  # Anzahl der Paare
p_e <- sum(rowSums(test_table) * colSums(test_table)) / (n^2)

# Berechnung der beobachteten Übereinstimmung (p_0)
p_0 <- agreement_percentage / 100

# Berechnung des Standardfehlers (SE)
if (p_e == 1) {
  se_kappa <- NA  # Vermeide Division durch 0, wenn p_e = 1 (perfekte Übereinstimmung)
  z_value <- NA
} else {
  se_kappa <- sqrt((p_0 * (1 - p_0)) / (n * (1 - p_e)^2))
  
  # Berechnung des z-Werts
  z_value <- kappa_value / se_kappa
}

# Berechne das 95%-Konfidenzintervall für Cohen's Kappa
ci_kappa <- calculate_kappa_confidence_interval(kappa_value, se_kappa)

# Schritt 6: Zeige das Gesamtergebnis an, einschließlich des Konfidenzintervalls
retest_kappa_all_summary <- data.frame(
  Kappa = kappa_value,
  Agreement_Percentage = agreement_percentage,
  z_value = z_value,
  p_value = p_value,
  CI_Lower = ci_kappa[1],
  CI_Upper = ci_kappa[2],
  n = n  # Anzahl der Beobachtungen
)

# Zeige das Gesamtergebnis an
print(retest_kappa_all_summary)


```
naja z wert und p wert muss ich noch anschauen...




##  test-retest Überblick kappas

```{r}
# Schritt 1: Rundung der Kappa-Werte auf 2 Nachkommastellen
retest_kappa_summary <- retest_kappa_summary %>%
  mutate(Kappa = round(Kappa, 2))

# Schritt 2: Aufteilen der Tabelle in Kategorien basierend auf Präfixen (pre, c_pre, pap, c_pap, match)
pre_kappa <- retest_kappa_summary %>% filter(grepl("^(pre|c_pre)", Column)) %>% mutate(Source = "pre")
pap_kappa <- retest_kappa_summary %>% filter(grepl("^(pap|c_pap)", Column)) %>% mutate(Source = "pap")
match_kappa <- retest_kappa_summary %>% filter(grepl("^match", Column)) %>% mutate(Source = "match")

# Schritt 3: Setze Zeilennummern für eine saubere Darstellung in jeder Tabelle
pre_kappa <- pre_kappa %>% mutate(Row = row_number())
pap_kappa <- pap_kappa %>% mutate(Row = row_number())
match_kappa <- match_kappa %>% mutate(Row = row_number())

# Schritt 4: Zusammenfügen der drei Tabellen nebeneinander basierend auf der Zeilennummer
# Verwende full_join, um die Tabellen nach den Zeilen zu verbinden
retest_kappa_summary_overview <- pre_kappa %>%
  full_join(pap_kappa, by = "Row", suffix = c("_pre", "_pap")) %>%
  full_join(match_kappa, by = "Row", suffix = c("", "_match"))

# Schritt 5: Auswahl und Umbenennung der Spalten für Übersichtlichkeit
# Stellen Sie sicher, dass die Spalten korrekt benannt und verfügbar sind
retest_kappa_summary_overview <- retest_kappa_summary_overview %>%
  select(Row, 
         Column_pre = Column_pre, Kappa_pre = Kappa_pre, 
         Column_pap = Column_pap, Kappa_pap = Kappa_pap, 
         Column_match = Column, Kappa_match = Kappa) 








```









## test retest färbung überblick

```{r}

# Färbung der Kappa-Werte direkt in der Tabelle
formattable(retest_kappa_summary_overview, list(
  Kappa_pre = color_tile("red", "green"),  # Kappa_pre-Spalte von dunkelrot bis dunkelgrün
  Kappa_pap = color_tile("red", "green"),  # Kappa_pap-Spalte von dunkelrot bis dunkelgrün
  Kappa_match = color_tile("red", "green") # Kappa_match-Spalte von dunkelrot bis dunkelgrün
))


```





#Kappa INTERRATER gpt zu olmo


- 
- sollte ich im gesamten script die selbe mehto wählen?
- wie ist die differenz zu interpretieren?
- kann man es überhaupt ausrechnen für die text variablen? also etwa die einzigen guten kapps?--> nein das muss ich noch anschauen...

## Kappa interrater gpt zu olmo per ITEM/category

```{r}



# Function to calculate Cohen's Kappa, p-value, z-value, and percentage agreement
calculate_kappa_stats <- function(data, column_gpt, column_olmo) {
  # Ensure both columns exist
  if(column_gpt %in% colnames(data) & column_olmo %in% colnames(data)) {
    # Calculate Cohen's Kappa
    kappa_result <- psych::cohen.kappa(cbind(data[[column_gpt]], data[[column_olmo]]))
    
    # Extract values
    kappa_value <- kappa_result$kappa
    p_value <- kappa_result$p.value
    z_value <- kappa_result$z
    percent_agreement <- sum(data[[column_gpt]] == data[[column_olmo]], na.rm = TRUE) / nrow(data) * 100
    
    return(c(kappa_value, p_value, z_value, percent_agreement))
  } else {
    return(rep(NA, 4))
  }
}

# Get all columns that have _gpt or _olmo suffix
gpt_columns <- grep("_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns <- gsub("_gpt$", "_olmo", gpt_columns)

# Create a dataframe to store the results
kappa_results <- data.frame(Variable = gpt_columns, Kappa = NA, P_Value = NA, Z_Value = NA, Percent_Agreement = NA)

# Loop over all _gpt and _olmo column pairs and calculate statistics
for(i in 1:length(gpt_columns)) {
  kappa_stats <- calculate_kappa_stats(data_gpt_olmo_processed, gpt_columns[i], olmo_columns[i])
  kappa_results[i, 2:5] <- kappa_stats
}

# Show the results
print(kappa_results)



```


achtung kappa result und results!!


##Kappa interrater gpt zu olmo  per ITEM 2. ansatz

```{r}


# Function to calculate Cohen's Kappa, p-value, z-value, and percentage agreement using 'irr' package
calculate_kappa_irr_v2 <- function(data, column_gpt, column_olmo) {
  # Ensure both columns exist
  if(column_gpt %in% colnames(data) & column_olmo %in% colnames(data)) {
    # Remove rows with NA values to avoid errors in kappa calculation
    valid_data_v2 <- data[!is.na(data[[column_gpt]]) & !is.na(data[[column_olmo]]), ]
    
    # Calculate Cohen's Kappa using the 'kappa2' function from the irr package
    kappa_result_v2 <- irr::kappa2(valid_data_v2[, c(column_gpt, column_olmo)], "unweighted")
    
    # Extract values
    kappa_value_v2 <- kappa_result_v2$value
    p_value_v2 <- kappa_result_v2$p.value
    z_value_v2 <- kappa_result_v2$statistic
    percent_agreement_v2 <- sum(valid_data_v2[[column_gpt]] == valid_data_v2[[column_olmo]]) / nrow(valid_data_v2) * 100
    
    return(c(kappa_value_v2, p_value_v2, z_value_v2, percent_agreement_v2))
  } else {
    return(rep(NA, 4))
  }
}

# Get all columns that have _gpt or _olmo suffix
gpt_columns_v2 <- grep("_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns_v2 <- gsub("_gpt$", "_olmo", gpt_columns_v2)

# Create a dataframe to store the results
kappa_results_v2 <- data.frame(Variable = gpt_columns_v2, Kappa = NA, P_Value = NA, Z_Value = NA, Percent_Agreement = NA)

# Loop over all _gpt and _olmo column pairs and calculate statistics
for(i in 1:length(gpt_columns_v2)) {
  kappa_stats_v2 <- calculate_kappa_irr_v2(data_gpt_olmo_processed, gpt_columns_v2[i], olmo_columns_v2[i])
  kappa_results_v2[i, 2:5] <- kappa_stats_v2
}

# Show the results
print(kappa_results_v2)

```





## Average Interater kappa over ITEMS...


hmm ich dachte dies sollte gleich hoch sein wie kappa total?
```{r}
# Überprüfen, ob die Spalte "Kappa" im Datensatz kappa_results existiert
if("Kappa" %in% colnames(kappa_results)) {
  # Berechne den Durchschnitt der Spalte Kappa
  Average_Kappa_over_categories <- mean(kappa_results$Kappa, na.rm = TRUE)
  
  # Ausgabe des Ergebnisses
  cat("Durchschnittlicher Kappa-Wert über alle Kategorien:", Average_Kappa_over_categories, "\n")
} else {
  stop("Die Spalte 'Kappa' existiert nicht im Datensatz kappa_results!")
}


```




# 
```{r}


```




# Kappa interrater gpt zu olmo  per category 3. ansatz
```{r}
# Spalten mit _gpt und _olmo suffix finden
gpt_columns_per_item_v3 <- grep("_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns_per_item_v3 <- gsub("_gpt$", "_olmo", gpt_columns_per_item_v3)

# Überprüfen, ob die entsprechenden _olmo-Spalten existieren
valid_gpt_columns_per_item_v3 <- gpt_columns_per_item_v3[olmo_columns_per_item_v3 %in% colnames(data_gpt_olmo_processed)]
valid_olmo_columns_per_item_v3 <- olmo_columns_per_item_v3[olmo_columns_per_item_v3 %in% colnames(data_gpt_olmo_processed)]

# Sicherstellen, dass nur gültige Paare verwendet werden
if(length(valid_gpt_columns_per_item_v3) == 0) {
  stop("Keine gültigen Paare von GPT- und OLMO-Spalten gefunden!")
}

# Erstelle einen DataFrame, um die Ergebnisse für jede Spalte zu speichern
kappa_results_per_item_v3 <- data.frame(Item = character(),
                                        Kappa = numeric(),
                                        P_Value = numeric(),
                                        Z_Value = numeric(),
                                        Percent_Agreement = numeric(),
                                        stringsAsFactors = FALSE)

# Berechnung für jede Spalte einzeln
for(i in 1:length(valid_gpt_columns_per_item_v3)) {
  gpt_column_per_item_v3 <- data_gpt_olmo_processed[[valid_gpt_columns_per_item_v3[i]]]
  olmo_column_per_item_v3 <- data_gpt_olmo_processed[[valid_olmo_columns_per_item_v3[i]]]
  
  # Kombiniere GPT und OLMO Spalten für den Vergleich
  combined_data_per_item_v3 <- data.frame(GPT = gpt_column_per_item_v3,
                                          OLMO = olmo_column_per_item_v3)

  # Entferne NA-Werte
  combined_data_per_item_v3 <- na.omit(combined_data_per_item_v3)
  
  # Berechne Cohen's Kappa
  kappa_result_per_item_v3 <- irr::kappa2(combined_data_per_item_v3, "unweighted")
  
  # Berechne den Prozentsatz der Übereinstimmung
  percent_agreement_per_item_v3 <- sum(combined_data_per_item_v3$GPT == combined_data_per_item_v3$OLMO) / nrow(combined_data_per_item_v3) * 100
  
  # Ergebnisse in den DataFrame hinzufügen
  kappa_results_per_item_v3 <- rbind(kappa_results_per_item_v3, data.frame(Item = valid_gpt_columns_per_item_v3[i],
                                                                           Kappa = kappa_result_per_item_v3$value,
                                                                           P_Value = kappa_result_per_item_v3$p.value,
                                                                           Z_Value = kappa_result_per_item_v3$statistic,
                                                                           Percent_Agreement = percent_agreement_per_item_v3))
}

# Ergebnisse anzeigen
print(kappa_results_per_item_v3)


```




# 
```{r}


```



# gpt /olmo interrater nur comparison/nur extraction  prompt (gesamtdatenset)


```{r}
# Funktion zur Berechnung des 95%-Konfidenzintervalls für Cohen's Kappa
calculate_kappa_confidence_interval <- function(kappa_value, se_kappa) {
  if (!is.na(se_kappa)) {
    ci_lower <- kappa_value - 1.96 * se_kappa
    ci_upper <- kappa_value + 1.96 * se_kappa
    return(c(ci_lower, ci_upper))
  } else {
    return(c(NA, NA))  # Kein CI, falls SE nicht berechnet werden kann
  }
}

# Extrahiere die GPT und OLMO Spalten für beide Vergleiche (Extraction und Comparison)
gpt_columns_extract <- grep("^(c_pre|pre|c_pap|pap).*_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns_extract <- gsub("_gpt$", "_olmo", gpt_columns_extract)

gpt_columns_comp <- grep("^(c_match|match).*_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns_comp <- gsub("_gpt$", "_olmo", gpt_columns_comp)

# Überprüfen, ob die entsprechenden OLMO-Spalten existieren (für beide Gruppen)
valid_gpt_columns_extract <- gpt_columns_extract[olmo_columns_extract %in% colnames(data_gpt_olmo_processed)]
valid_olmo_columns_extract <- olmo_columns_extract[olmo_columns_extract %in% colnames(data_gpt_olmo_processed)]

valid_gpt_columns_comp <- gpt_columns_comp[olmo_columns_comp %in% colnames(data_gpt_olmo_processed)]
valid_olmo_columns_comp <- olmo_columns_comp[olmo_columns_comp %in% colnames(data_gpt_olmo_processed)]

# Sicherstellen, dass nur gültige Paare verwendet werden
if(length(valid_gpt_columns_extract) == 0) {
  stop("Keine gültigen Paare von GPT- und OLMO-Spalten für Extraction gefunden!")
}
if(length(valid_gpt_columns_comp) == 0) {
  stop("Keine gültigen Paare von GPT- und OLMO-Spalten für Comparison gefunden!")
}

# Daten für beide Vergleiche kombinieren und NA-Werte entfernen
combined_data_extract <- data.frame()
combined_data_comp <- data.frame()

# Kombinierte Daten für den Extraction-Vergleich
for(i in 1:length(valid_gpt_columns_extract)) {
  combined_data_extract <- rbind(combined_data_extract, data.frame(GPT = data_gpt_olmo_processed[[valid_gpt_columns_extract[i]]],
                                                                   OLMO = data_gpt_olmo_processed[[valid_olmo_columns_extract[i]]]))
}
combined_data_extract <- na.omit(combined_data_extract)

# Kombinierte Daten für den Comparison-Vergleich
for(i in 1:length(valid_gpt_columns_comp)) {
  combined_data_comp <- rbind(combined_data_comp, data.frame(GPT = data_gpt_olmo_processed[[valid_gpt_columns_comp[i]]],
                                                             OLMO = data_gpt_olmo_processed[[valid_olmo_columns_comp[i]]]))
}
combined_data_comp <- na.omit(combined_data_comp)

# Berechne Cohen's Kappa für Extraction
kappa_result_extract <- irr::kappa2(combined_data_extract, "unweighted")
percent_agreement_extract <- sum(combined_data_extract$GPT == combined_data_extract$OLMO) / nrow(combined_data_extract) * 100
n_extract <- nrow(combined_data_extract)

# Berechnung des Standardfehlers für Extraction
p_e_extract <- sum(rowSums(table(combined_data_extract$GPT, combined_data_extract$OLMO)) * colSums(table(combined_data_extract$GPT, combined_data_extract$OLMO))) / (n_extract^2)
p_0_extract <- percent_agreement_extract / 100

if (p_e_extract == 1) {
  se_kappa_extract <- NA
  z_value_extract <- NA
} else {
  se_kappa_extract <- sqrt((p_0_extract * (1 - p_0_extract)) / (n_extract * (1 - p_e_extract)^2))
  z_value_extract <- kappa_result_extract$value / se_kappa_extract
}

# Konfidenzintervall für Kappa (Extraction)
ci_extract <- calculate_kappa_confidence_interval(kappa_result_extract$value, se_kappa_extract)

# Berechne Cohen's Kappa für Comparison
kappa_result_comp <- irr::kappa2(combined_data_comp, "unweighted")
percent_agreement_comp <- sum(combined_data_comp$GPT == combined_data_comp$OLMO) / nrow(combined_data_comp) * 100
n_comp <- nrow(combined_data_comp)

# Berechnung des Standardfehlers für Comparison
p_e_comp <- sum(rowSums(table(combined_data_comp$GPT, combined_data_comp$OLMO)) * colSums(table(combined_data_comp$GPT, combined_data_comp$OLMO))) / (n_comp^2)
p_0_comp <- percent_agreement_comp / 100

if (p_e_comp == 1) {
  se_kappa_comp <- NA
  z_value_comp <- NA
} else {
  se_kappa_comp <- sqrt((p_0_comp * (1 - p_0_comp)) / (n_comp * (1 - p_e_comp)^2))
  z_value_comp <- kappa_result_comp$value / se_kappa_comp
}

# Konfidenzintervall für Kappa (Comparison)
ci_comp <- calculate_kappa_confidence_interval(kappa_result_comp$value, se_kappa_comp)

# Ergebnisse anzeigen
cat("Cohen's Kappa für Extraction (gesamt_kappa_extraction):\n")
cat("Kappa:", kappa_result_extract$value, "\n")
cat("95%-Konfidenzintervall:", sprintf("[%.3f, %.3f]", ci_extract[1], ci_extract[2]), "\n")
cat("p-Wert:", kappa_result_extract$p.value, "\n")
cat("z-Wert:", z_value_extract, "\n")
cat("Prozentuale Übereinstimmung:", percent_agreement_extract, "%\n")
cat("Anzahl der Vergleiche (n):", n_extract, "\n")

cat("\nCohen's Kappa für Comparison (gesamt_kappa_comparison):\n")
cat("Kappa:", kappa_result_comp$value, "\n")
cat("95%-Konfidenzintervall:", sprintf("[%.3f, %.3f]", ci_comp[1], ci_comp[2]), "\n")
cat("p-Wert:", kappa_result_comp$p.value, "\n")
cat("z-Wert:", z_value_comp, "\n")
cat("Prozentuale Übereinstimmung:", percent_agreement_comp, "%\n")
cat("Anzahl der Vergleiche (n):", n_comp, "\n")


```


## Kappa interrater gpt zu olmo gesamt

```{r}
# Funktion zur Berechnung des 95%-Konfidenzintervalls für Cohen's Kappa
calculate_kappa_confidence_interval <- function(kappa_value, se_kappa) {
  if (!is.na(se_kappa)) {
    ci_lower <- kappa_value - 1.96 * se_kappa
    ci_upper <- kappa_value + 1.96 * se_kappa
    return(c(ci_lower, ci_upper))
  } else {
    return(c(NA, NA))  # Kein CI, falls SE nicht berechnet werden kann
  }
}

# Spalten mit _gpt und _olmo Suffix finden
gpt_columns <- grep("_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns <- gsub("_gpt$", "_olmo", gpt_columns)

# Überprüfen, ob die entsprechenden _olmo-Spalten existieren
valid_gpt_columns <- gpt_columns[olmo_columns %in% colnames(data_gpt_olmo_processed)]
valid_olmo_columns <- olmo_columns[olmo_columns %in% colnames(data_gpt_olmo_processed)]

# Sicherstellen, dass nur gültige Paare verwendet werden
if(length(valid_gpt_columns) == 0) {
  stop("Keine gültigen Paare von GPT- und OLMO-Spalten gefunden!")
}

# Kombinieren der Spalten in einem DataFrame für den Gesamtvergleich
combined_data <- data.frame()

for(i in 1:length(valid_gpt_columns)) {
  # Füge die Werte der GPT und OLMO Spalten zur kombinierten Datensatz hinzu
  combined_data <- rbind(combined_data, data.frame(GPT = data_gpt_olmo_processed[[valid_gpt_columns[i]]],
                                                   OLMO = data_gpt_olmo_processed[[valid_olmo_columns[i]]]))
}

# Entferne NA-Werte
combined_data <- na.omit(combined_data)

# Berechne Cohen's Kappa für den gesamten Test
kappa_result <- irr::kappa2(combined_data, "unweighted")
n_total <- nrow(combined_data)  # Anzahl der Vergleiche

# Berechne den Prozentsatz der Übereinstimmung
percent_agreement <- sum(combined_data$GPT == combined_data$OLMO) / n_total * 100

# Berechnung des Standardfehlers
p_e <- sum(rowSums(table(combined_data$GPT, combined_data$OLMO)) * colSums(table(combined_data$GPT, combined_data$OLMO))) / (n_total^2)
p_0 <- percent_agreement / 100

if (p_e == 1) {
  se_kappa <- NA
  z_value <- NA
} else {
  se_kappa <- sqrt((p_0 * (1 - p_0)) / (n_total * (1 - p_e)^2))
  z_value <- kappa_result$value / se_kappa
}

# Berechne das 95%-Konfidenzintervall für Kappa
ci_kappa <- calculate_kappa_confidence_interval(kappa_result$value, se_kappa)

# Ergebnisse anzeigen
cat("Cohen's Kappa für den gesamten Test:\n")
cat("Kappa:", kappa_result$value, "\n")
cat("95%-Konfidenzintervall:", sprintf("[%.3f, %.3f]", ci_kappa[1], ci_kappa[2]), "\n")
cat("p-Wert:", kappa_result$p.value, "\n")
cat("z-Wert:", z_value, "\n")
cat("Prozentuale Übereinstimmung:", percent_agreement, "%\n")
cat("Anzahl der Vergleiche (n):", n_total, "\n")



```




## besser nur bei promt version d? im Vergleich zu den anderen?

hier noch n reinnehmen

```{r}
# Funktion zur Berechnung des 95%-Konfidenzintervalls für Cohen's Kappa
calculate_kappa_confidence_interval <- function(kappa_value, se_kappa) {
  if (!is.na(se_kappa)) {
    ci_lower <- kappa_value - 1.96 * se_kappa
    ci_upper <- kappa_value + 1.96 * se_kappa
    return(c(ci_lower, ci_upper))
  } else {
    return(c(NA, NA))  # Kein CI, falls SE nicht berechnet werden kann
  }
}

# Spalten mit _gpt und _olmo Suffix finden
gpt_columns_PromptV <- grep("_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns_PromptV <- gsub("_gpt$", "_olmo", gpt_columns_PromptV)

# Überprüfen, ob die entsprechenden _olmo-Spalten existieren
valid_gpt_columns_PromptV <- gpt_columns_PromptV[olmo_columns_PromptV %in% colnames(data_gpt_olmo_processed)]
valid_olmo_columns_PromptV <- olmo_columns_PromptV[olmo_columns_PromptV %in% colnames(data_gpt_olmo_processed)]

# Sicherstellen, dass nur gültige Paare verwendet werden
if(length(valid_gpt_columns_PromptV) == 0) {
  stop("Keine gültigen Paare von GPT- und OLMO-Spalten gefunden!")
}

# Definiere die verschiedenen Versionen, die in prompt_version auftreten (A, B, C, D)
versions_PromptV <- c("A", "B", "C", "D")

# Schleife durch jede Version und berechne das Kappa für jede
for (version_PromptV in versions_PromptV) {
  # Filtere die Zeilen, die zur aktuellen Version gehören
  version_data_PromptV <- data_gpt_olmo_processed[data_gpt_olmo_processed$prompt_version == version_PromptV, ]
  
  # Kombinieren der Spalten in einem DataFrame für den Vergleich der aktuellen Version
  combined_data_PromptV <- data.frame()
  
  for(i in 1:length(valid_gpt_columns_PromptV)) {
    # Füge die Werte der GPT und OLMO Spalten zur kombinierten Datensatz hinzu
    combined_data_PromptV <- rbind(combined_data_PromptV, data.frame(GPT = version_data_PromptV[[valid_gpt_columns_PromptV[i]]],
                                                                     OLMO = version_data_PromptV[[valid_olmo_columns_PromptV[i]]]))
  }
  
  # Entferne NA-Werte
  combined_data_PromptV <- na.omit(combined_data_PromptV)
  
  # Berechne Cohen's Kappa für die aktuelle Version
  kappa_result_PromptV <- irr::kappa2(combined_data_PromptV, "unweighted")
  n_PromptV <- nrow(combined_data_PromptV)  # Anzahl der Vergleiche
  
  # Berechne den Prozentsatz der Übereinstimmung
  percent_agreement_PromptV <- sum(combined_data_PromptV$GPT == combined_data_PromptV$OLMO) / n_PromptV * 100
  
  # Berechnung des Standardfehlers
  p_e_PromptV <- sum(rowSums(table(combined_data_PromptV$GPT, combined_data_PromptV$OLMO)) * colSums(table(combined_data_PromptV$GPT, combined_data_PromptV$OLMO))) / (n_PromptV^2)
  p_0_PromptV <- percent_agreement_PromptV / 100
  
  if (p_e_PromptV == 1) {
    se_kappa_PromptV <- NA
    z_value_PromptV <- NA
  } else {
    se_kappa_PromptV <- sqrt((p_0_PromptV * (1 - p_0_PromptV)) / (n_PromptV * (1 - p_e_PromptV)^2))
    z_value_PromptV <- kappa_result_PromptV$value / se_kappa_PromptV
  }
  
  # Berechne das 95%-Konfidenzintervall für Kappa
  ci_kappa_PromptV <- calculate_kappa_confidence_interval(kappa_result_PromptV$value, se_kappa_PromptV)
  
  # Ergebnisse anzeigen
  cat("\nCohen's Kappa für Version", version_PromptV, ":\n")
  cat("Kappa:", kappa_result_PromptV$value, "\n")
  cat("95%-Konfidenzintervall:", sprintf("[%.3f, %.3f]", ci_kappa_PromptV[1], ci_kappa_PromptV[2]), "\n")
  cat("p-Wert:", kappa_result_PromptV$p.value, "\n")
  cat("z-Wert:", z_value_PromptV, "\n")
  cat("Prozentuale Übereinstimmung:", percent_agreement_PromptV, "%\n")
  cat("Anzahl der Vergleiche (n):", n_PromptV, "\n")
}




```





## kappa interrater gpt olmo zusammengefügt

```{r}


# Erstelle den DataFrame mit den Kappa-Ergebnissen, Zahlen auf 2 Nachkommastellen gerundet
kappas_total_zusammengeführt <- data.frame(
  Test = c("Gesamter Test", "Extraction", "Comparison", "Version A", "Version B", "Version C", "Version D"),
  Kappa = round(c(0.3129825, 0.312971, 0.2757249, 0.2820128, 0.2892539, 0.2424722, 0.333917), 2),
  CI_Lower = round(c(0.308, 0.307, 0.260, 0.266, 0.276, 0.226, 0.328), 2),
  CI_Upper = round(c(0.318, 0.319, 0.292, 0.298, 0.303, 0.259, 0.340), 2),
  Percent_Agreement = round(c(54.92, 53.31, 58.65, 51.50, 51.84, 48.29, 57.11), 2),
  N_Comparisons = c(36955, 25831, 11124, 3590, 5374, 3703, 24288)
)

# Erstelle die Tabelle mit flextable
ft <- flextable(kappas_total_zusammengeführt)

# Setze die Spaltenüberschriften im APA-Stil
ft <- set_header_labels(ft, 
                        Test = "Test",
                        Kappa = "Kappa",
                        CI_Lower = "95% CI Lower",
                        CI_Upper = "95% CI Upper",
                        Percent_Agreement = "Percent Agreement (%)",
                        N_Comparisons = "N")

# Wende APA-konforme Formatierung an
ft <- theme_vanilla(ft) # Minimalistisches Design ohne vertikale Linien
ft <- fontsize(ft, part = "all", size = 12) # Schriftgröße auf 12 setzen
ft <- bold(ft, part = "header") # Kopfzeile fett machen
ft <- align(ft, part = "all", align = "center") # Text zentrieren
ft <- autofit(ft) # Automatisch die Spaltenbreiten anpassen

# Optional: Tabelle als Word-Dokument speichern
# save_as_docx(ft, path = "kappas_total_zusammengeführt.docx")

# Zeige die Tabelle an
ft



```




# Kappa NUR Y & N 

## kappa y_n all categories

```{r}
# Function to calculate Cohen's Kappa, p-value, z-value, percentage agreement, and number of comparisons using 'irr' package
calculate_kappa_irr_filtered_debug <- function(data, column_gpt, column_olmo) {
  # Ensure both columns exist
  if (column_gpt %in% colnames(data) & column_olmo %in% colnames(data)) {
    # Filter rows where the OLMO column is either "Y" or "N"
    valid_data <- data[data[[column_olmo]] %in% c("y", "n"), ]
    
    # Debug: Print number of valid rows for the comparison
    print(paste("Number of valid rows for comparison (", column_gpt, " & ", column_olmo, "): ", nrow(valid_data)))
    
    # Check if there are any valid rows left for comparison
    if (nrow(valid_data) > 0) {
      # Remove rows with NA values in the selected columns
      valid_data <- valid_data[!is.na(valid_data[[column_gpt]]) & !is.na(valid_data[[column_olmo]]), ]
      
      # Debug: Print number of rows after removing NAs
      print(paste("Number of valid rows after removing NAs: ", nrow(valid_data)))
      
      # Check again if there are rows left after removing NAs
      if (nrow(valid_data) > 0) {
        # Calculate Cohen's Kappa using the 'kappa2' function from the irr package
        kappa_result <- irr::kappa2(valid_data[, c(column_gpt, column_olmo)], "unweighted")
        
        # Extract values
        kappa_value <- kappa_result$value
        p_value <- kappa_result$p.value
        z_value <- kappa_result$statistic
        percent_agreement <- sum(valid_data[[column_gpt]] == valid_data[[column_olmo]]) / nrow(valid_data) * 100
        
        # Return the results including the number of comparisons (N)
        return(c(kappa_value, p_value, z_value, percent_agreement, nrow(valid_data)))
      } else {
        # No valid comparisons available after NA removal
        print("No valid rows left after removing NAs.")
        return(rep(NA, 5))
      }
    } else {
      # No valid comparisons based on OLMO filter
      print("No valid rows based on OLMO filter (Y/N).")
      return(rep(NA, 5))
    }
  } else {
    print(paste("Columns", column_gpt, "or", column_olmo, "do not exist in the dataset."))
    return(rep(NA, 5))
  }
}

# Get all columns that have _gpt or _olmo suffix
gpt_columns <- grep("_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns <- gsub("_gpt$", "_olmo", gpt_columns)

# Create a dataframe to store the results
kappa_results_filtered_debug <- data.frame(
  Variable = gpt_columns,
  Kappa = NA,
  P_Value = NA,
  Z_Value = NA,
  Percent_Agreement = NA,
  N_Comparisons = NA
)

# Loop over all _gpt and _olmo column pairs and calculate statistics
for (i in 1:length(gpt_columns)) {
  print(paste("Processing columns:", gpt_columns[i], "and", olmo_columns[i]))
  kappa_stats_filtered_debug <- calculate_kappa_irr_filtered_debug(data_gpt_olmo_processed, gpt_columns[i], olmo_columns[i])
  kappa_results_filtered_debug[i, 2:6] <- kappa_stats_filtered_debug
}

# Show the results
print(kappa_results_filtered_debug)



```



## kappa y & n total

```{r}

# Funktion zur Berechnung des 95%-Konfidenzintervalls für Cohen's Kappa
calculate_kappa_confidence_interval <- function(kappa_value, se_kappa) {
  if (!is.na(se_kappa)) {
    ci_lower <- kappa_value - 1.96 * se_kappa
    ci_upper <- kappa_value + 1.96 * se_kappa
    return(c(ci_lower, ci_upper))
  } else {
    return(c(NA, NA))  # Kein CI, falls SE nicht berechnet werden kann
  }
}

# Funktion zur Berechnung des Standardfehlers und der erwarteten Übereinstimmung
calculate_standard_error <- function(data) {
  # Erstelle die Kontingenztabelle
  contingency_table <- table(data$GPT, data$OLMO)
  
  # Berechne die Zeilen- und Spaltensummen der Kontingenztabelle
  row_sums <- rowSums(contingency_table)
  col_sums <- colSums(contingency_table)
  
  # Anzahl der Paare
  n <- sum(contingency_table)
  
  # Berechnung der erwarteten Übereinstimmung (p_e)
  p_e <- sum(row_sums * col_sums) / (n^2)
  
  # Berechne den beobachteten Prozentsatz der Übereinstimmung (p_0)
  percent_agreement <- sum(diag(contingency_table)) / n * 100
  p_0 <- percent_agreement / 100
  
  return(list(p_e = p_e, p_0 = p_0, n = n))
}

# Funktion zur Berechnung von Cohen's Kappa, p-Wert, z-Wert, prozentuale Übereinstimmung, Anzahl der Vergleiche und Konfidenzintervall
calculate_overall_kappa <- function(data, gpt_columns, olmo_columns) {
  # Erstelle einen leeren DataFrame für die relevanten Vergleichswerte
  combined_data <- data.frame(GPT = character(), OLMO = character(), stringsAsFactors = FALSE)
  
  # Schleife über alle GPT und OLMO Spalten, um gültige Zeilen zu sammeln
  for (i in 1:length(gpt_columns)) {
    if (gpt_columns[i] %in% colnames(data) & olmo_columns[i] %in% colnames(data)) {
      # Filtere nur Zeilen, in denen der OLMO-Wert "y" oder "n" ist
      valid_rows <- data[data[[olmo_columns[i]]] %in% c("y", "n"), ]
      
      # Wähle die relevanten GPT- und OLMO-Werte aus und entferne NA-Werte
      valid_rows <- valid_rows[!is.na(valid_rows[[gpt_columns[i]]]) & !is.na(valid_rows[[olmo_columns[i]]]), ]
      
      # Füge die gültigen GPT- und OLMO-Werte in den kombinierten DataFrame ein
      combined_data <- rbind(combined_data, data.frame(GPT = valid_rows[[gpt_columns[i]]], OLMO = valid_rows[[olmo_columns[i]]]))
    }
  }
  
  # Überprüfe, ob genügend Datenpunkte vorhanden sind, um Kappa zu berechnen
  if (nrow(combined_data) > 0) {
    # Berechne Cohen's Kappa über alle kombinierten Zellen
    kappa_result <- irr::kappa2(combined_data, "unweighted")
    
    # Extrahiere Kappa-Wert, p-Wert und z-Wert
    kappa_value <- kappa_result$value
    p_value <- kappa_result$p.value
    z_value <- kappa_result$statistic
    percent_agreement <- sum(combined_data$GPT == combined_data$OLMO) / nrow(combined_data) * 100
    n_comparisons <- nrow(combined_data)
    
    # Berechne den Standardfehler und die erwartete Übereinstimmung
    agreement_info <- calculate_standard_error(combined_data)
    p_e <- agreement_info$p_e
    p_0 <- agreement_info$p_0
    
    # Berechne den Standardfehler
    if (p_e == 1) {
      se_kappa <- NA
    } else {
      se_kappa <- sqrt((p_0 * (1 - p_0)) / (n_comparisons * (1 - p_e)^2))
    }
    
    # Berechne das 95%-Konfidenzintervall für Kappa
    ci_kappa <- calculate_kappa_confidence_interval(kappa_value, se_kappa)
    
    # Rückgabe der Ergebnisse einschließlich des Konfidenzintervalls
    return(data.frame(Kappa_Gesamt = kappa_value, P_Value = p_value, Z_Value = z_value, Percent_Agreement = percent_agreement, 
                      N_Comparisons = n_comparisons, CI_Lower = ci_kappa[1], CI_Upper = ci_kappa[2]))
  } else {
    # Rückgabe von NA, wenn keine gültigen Vergleiche vorhanden sind
    return(data.frame(Kappa_Gesamt = NA, P_Value = NA, Z_Value = NA, Percent_Agreement = NA, N_Comparisons = 0, CI_Lower = NA, CI_Upper = NA))
  }
}

# Hole alle Spalten mit _gpt oder _olmo Suffix
gpt_columns <- grep("_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns <- gsub("_gpt$", "_olmo", gpt_columns)

# Berechne das Gesamtkappa über alle relevanten Zellen
kappa_gesamt_yn <- calculate_overall_kappa(data_gpt_olmo_processed, gpt_columns, olmo_columns)

# Zeige das Gesamtergebnis an
print(kappa_gesamt_yn)



```
# i thought it would be better, but its actually worse...

hmm ich ersuhe jetz noch einen kürzeren promt mit ausgewählten fragen... dann muss ich nur diese ausgewählte fragen noch testen beim ursprünglichen datensatz und dann kann ich diesn vergleichen mit dem neuen gekürzeten...

und eventuell noch die sensitivity etc. nur für y/n zählen lassen!!! 

ausserdem sollte ich wohl noch die ratings auch auswerten(alos die nummern obwohldiese ziemlich scheisse isind


und noch schauen ob es einen unterschied macht ob experiment oder nicht....)


## ich könnte noch kappa pro psp machen.... dh..

```{r}



```

# NEW
## kappa new per category
```{r}


# Definiere die zu vergleichenden Variablenpaare
variables_to_compare_ve <- c("match_dv_0", "match_dv_1", "match_dv_2", "match_dv_3",
                             "match_dcp_1", "match_dcp_2", "match_sm_1", "match_sm_2", "match_sm_3")

# Liste zur Speicherung der Ergebnisse
results_ve <- data.frame(Variable_ve = character(), 
                         Kappa_ve = numeric(), 
                         N_ve = numeric(),
                         Agreement_Percent_ve = numeric(),
                         Z_Value_ve = numeric(),
                         P_Value_ve = numeric(), 
                         stringsAsFactors = FALSE)

# Berechnung von Cohen's Kappa und weiteren Kennzahlen für jede Variable
for (variable in variables_to_compare_ve) {
  # Definiere die Spaltennamen für GPT und Olmo
  gpt_column <- paste0(variable, "_gpt")
  olmo_column <- paste0(variable, "_olmo")
  
  # Selektiere die relevanten Spalten und entferne Zeilen mit NA-Werten nur in diesen Spalten
  comparison_data <- data_gpt_ve_filtered %>%
    select(gpt_column, olmo_column) %>%
    filter(!is.na(.[[gpt_column]]) & !is.na(.[[olmo_column]]))
  
  # Berechne Cohen's Kappa für die aktuelle Variable
  kappa_obj_ve <- kappa2(comparison_data)
  kappa_value_ve <- kappa_obj_ve$value
  
  # Anzahl der Vergleiche (N)
  n_comparisons_ve <- nrow(comparison_data)
  
  # Berechne den prozentualen Anteil der Übereinstimmungen
  agreement_percent_ve <- mean(comparison_data[[gpt_column]] == comparison_data[[olmo_column]]) * 100

  # Berechne Z-Wert und p-Wert (Standardmäßig aus dem Kappa-Objekt)
  z_value_ve <- kappa_obj_ve$statistic # Z-Wert aus dem Kappa-Objekt
  p_value_ve <- kappa_obj_ve$p.value   # p-Wert aus dem Kappa-Objekt
  
  # Ergebnisse speichern mit den Endungen _ve
  results_ve <- rbind(results_ve, data.frame(Variable_ve = variable, 
                                             Kappa_ve = kappa_value_ve, 
                                             N_ve = n_comparisons_ve,
                                             Agreement_Percent_ve = agreement_percent_ve,
                                             Z_Value_ve = z_value_ve,
                                             P_Value_ve = p_value_ve))
}

# Ausgabe der Ergebnisse als DataFrame
print(results_ve)

```
p sind nicht signifikant?!

## kappa ve total
```{r}
# Funktion zur Berechnung des Konfidenzintervalls für Kappa
calculate_kappa_confidence_interval <- function(kappa_value, n, alpha = 0.05) {
  # Standardfehler von Kappa
  standard_error <- sqrt((1 - kappa_value^2) / n)
  
  # Z-Wert für das gewünschte Konfidenzniveau (z.B. 95%)
  z_value <- qnorm(1 - alpha / 2)
  
  # Berechnung der unteren und oberen Grenze des Konfidenzintervalls
  ci_lower <- kappa_value - z_value * standard_error
  ci_upper <- kappa_value + z_value * standard_error
  
  return(c(ci_lower, ci_upper))
}

# Daten filtern: Nur Zeilen behalten, die nicht "exclude" in master_exclude haben
data_gpt_ve_total_filtered <- data_gpt_ve_processed %>%
  filter(master_exclude != "exclude")

# Definiere die zu vergleichenden Variablenpaare
variables_to_compare_ve_total <- c("match_dv_0", "match_dv_1", "match_dv_2", "match_dv_3",
                                   "match_dcp_1", "match_dcp_2", "match_sm_1", "match_sm_2", "match_sm_3")

# Liste zur Speicherung der kombinierten Ratings
combined_data_ve_total <- data.frame(GPT_Rating_ve_total = character(), 
                                     Olmo_Rating_ve_total = character(), 
                                     stringsAsFactors = FALSE)

# Kombinieren der Daten aus allen relevanten Variablenpaaren
for (variable in variables_to_compare_ve_total) {
  # Definiere die Spaltennamen für GPT und Olmo
  gpt_column <- paste0(variable, "_gpt")
  olmo_column <- paste0(variable, "_olmo")
  
  # Selektiere die relevanten Spalten und entferne Zeilen mit NA-Werten nur in diesen Spalten
  comparison_data <- data_gpt_ve_total_filtered %>%
    select(gpt_column, olmo_column) %>%
    filter(!is.na(.[[gpt_column]]) & !is.na(.[[olmo_column]])) %>%
    rename(GPT_Rating_ve_total = gpt_column, Olmo_Rating_ve_total = olmo_column)
  
  # Füge die gefilterten Daten zum kombinierten Datensatz hinzu
  combined_data_ve_total <- rbind(combined_data_ve_total, comparison_data)
}

# Berechnung des Gesamt-Kappa über alle kombinierten Kategorien hinweg
kappa_obj_ve_total <- kappa2(combined_data_ve_total)
kappa_value_ve_total <- kappa_obj_ve_total$value

# Berechnung des Konfidenzintervalls für Kappa (manuell)
ci_kappa_ve_total <- calculate_kappa_confidence_interval(kappa_value_ve_total, nrow(combined_data_ve_total))

# Berechnung zusätzlicher Kennzahlen
n_comparisons_ve_total <- nrow(combined_data_ve_total)  # Anzahl der Vergleiche (N)
agreement_percent_ve_total <- mean(combined_data_ve_total$GPT_Rating_ve_total == combined_data_ve_total$Olmo_Rating_ve_total) * 100
z_value_ve_total <- kappa_obj_ve_total$statistic  # Z-Wert aus dem Kappa-Objekt
p_value_ve_total <- kappa_obj_ve_total$p.value    # p-Wert aus dem Kappa-Objekt

# Ergebnisse auf 2 Nachkommastellen runden
results_ve_total <- data.frame(Variable_ve_total = "Overall",
                               Kappa_ve_total = round(kappa_value_ve_total, 2),
                               CI_Lower_ve_total = round(ci_kappa_ve_total[1], 2),  # Untere Grenze des CIs
                               CI_Upper_ve_total = round(ci_kappa_ve_total[2], 2),  # Obere Grenze des CIs
                               N_ve_total = n_comparisons_ve_total,
                               Agreement_Percent_ve_total = round(agreement_percent_ve_total, 2),
                               Z_Value_ve_total = round(z_value_ve_total, 2),
                               P_Value_ve_total = round(p_value_ve_total, 2))

print(results_ve_total)



```



## confusion matrix VE
```{r}



# Definiere die zu vergleichenden Variablenpaare
variables_to_compare_ve_total <- c("match_dv_0", "match_dv_1", "match_dv_2", "match_dv_3",
                                   "match_dcp_1", "match_dcp_2", "match_sm_1", "match_sm_2", "match_sm_3")

# Liste zur Speicherung der kombinierten Ratings
combined_data_ve_total <- data.frame(GPT_Rating_ve_total = character(), 
                                     Olmo_Rating_ve_total = character(), 
                                     stringsAsFactors = FALSE)

# Kombinieren der Daten aus allen relevanten Variablenpaaren
for (variable in variables_to_compare_ve_total) {
  # Definiere die Spaltennamen für GPT und Olmo
  gpt_column <- paste0(variable, "_gpt")
  olmo_column <- paste0(variable, "_olmo")
  
  # Selektiere die relevanten Spalten und entferne Zeilen mit NA-Werten nur in diesen Spalten
  comparison_data_ve_total <- data_gpt_ve_total_filtered %>%
    select(gpt_column, olmo_column) %>%
    filter(!is.na(.[[gpt_column]]) & !is.na(.[[olmo_column]])) %>%
    rename(GPT_Rating_ve_total = gpt_column, Olmo_Rating_ve_total = olmo_column)
  
  # Füge die gefilterten Daten zum kombinierten Datensatz hinzu
  combined_data_ve_total <- rbind(combined_data_ve_total, comparison_data_ve_total)
}

# Erstellen der Confusion Matrix mit _ve Endungen
conf_matrix_ve <- table(combined_data_ve_total$Olmo_Rating_ve_total, combined_data_ve_total$GPT_Rating_ve_total)

# Anzeige der Confusion Matrix mit _ve Endung
print("Confusion Matrix (ve):")
print(conf_matrix_ve)

# Berechnung weiterer Metriken: Sensitivität, Spezifität, etc. mit _ve Endung
confusion_summary_ve <- confusionMatrix(conf_matrix_ve)

# Ausgabe der detaillierten Ergebnisse mit _ve Endung
print("Detailed Confusion Matrix Metrics (ve):")
print(confusion_summary_ve)


```
<!-- n, n = 73: Olmo und GPT stimmen beide überein und haben n (Nein) vorhergesagt. -->
<!-- n, y = 155: Olmo hat n (Nein) angegeben, aber GPT hat y (Ja) vorhergesagt (False Positive). -->
<!-- y, n = 41: Olmo hat y (Ja) angegeben, aber GPT hat n (Nein) vorhergesagt (False Negative). -->
<!-- y, y = 199: Olmo und GPT stimmen beide überein und haben y (Ja) vorhergesagt. -->



## new frickes ü


```{r}


# Definiere die zu vergleichenden Variablenpaare
variables_to_compare_ve_total <- c("match_dv_0", "match_dv_1", "match_dv_2", "match_dv_3",
                                   "match_dcp_1", "match_dcp_2", "match_sm_1", "match_sm_2", "match_sm_3")

# Liste zur Speicherung der kombinierten Ratings
combined_data_ve_total <- data.frame(GPT_Rating_ve_total = character(), 
                                     Olmo_Rating_ve_total = character(), 
                                     stringsAsFactors = FALSE)

# Kombinieren der Daten aus allen relevanten Variablenpaaren
for (variable in variables_to_compare_ve_total) {
  # Definiere die Spaltennamen für GPT und Olmo
  gpt_column <- paste0(variable, "_gpt")
  olmo_column <- paste0(variable, "_olmo")
  
  # Selektiere die relevanten Spalten und entferne Zeilen mit NA-Werten nur in diesen Spalten
  comparison_data_ve_total <- data_gpt_ve_total_filtered %>%
    select(gpt_column, olmo_column) %>%
    filter(!is.na(.[[gpt_column]]) & !is.na(.[[olmo_column]])) %>%
    rename(GPT_Rating_ve_total = gpt_column, Olmo_Rating_ve_total = olmo_column)
  
  # Füge die gefilterten Daten zum kombinierten Datensatz hinzu
  combined_data_ve_total <- rbind(combined_data_ve_total, comparison_data_ve_total)
}

# Berechnung von Fricke's Ü-Koeffizient
calculate_fricke_ue <- function(true_values, predicted_values) {
  # Erstelle eine Kreuztabelle (Contingency Table)
  contingency_table <- table(true_values, predicted_values)
  
  # Beobachtete Übereinstimmungen (C_o)
  C_o <- sum(diag(contingency_table))
  
  # Maximale Anzahl der Übereinstimmungen (C_max)
  C_max <- sum(contingency_table)
  
  # Erwartete Übereinstimmungen unter Zufallsbedingungen (C_er)
  row_totals <- rowSums(contingency_table)
  col_totals <- colSums(contingency_table)
  C_er <- sum(row_totals * col_totals) / C_max
  
  # Berechnung von Fricke's Ü-Koeffizient
  Ue <- (C_o - C_er) / (C_max - C_er)
  return(Ue)
}

# Berechnung von Fricke's Ü-Koeffizient für die kombinierten Daten
fricke_ue_value <- calculate_fricke_ue(combined_data_ve_total$Olmo_Rating_ve_total, combined_data_ve_total$GPT_Rating_ve_total)

# Ausgabe des Ergebnisses
print(paste("Fricke's Ü-Koeffizient (ve_total):", round(fricke_ue_value, 4)))



```






##  test retest 
```{r}



# Step 1: Filter the dataset into test and retest subsets
test_data_ve <- data_gpt_ve_filtered %>% filter(test_retest == "Test")
retest_data_ve <- data_gpt_ve_filtered %>% filter(test_retest == "RETEST")

# Step 2: Match test and retest based on 'psp' (this will contain only the  test-retest psp rows)
retest_merged_data_ve <- merge(test_data_ve, retest_data_ve, by = "psp", suffixes = c("_test_ve", "_retest_ve"))

# Step 3: Get all columns ending with '-gpt' or '_gpt'
comp_columns_ve <- grep("[-_]gpt$", names(data_gpt_ve_filtered), value = TRUE)

# Step 4: Loop through each 'gpt' column and calculate Cohen's Kappa using the irr package
retest_kappa_results_ve <- lapply(comp_columns_ve, function(col_name) {
  # Cohen's Kappa requires the two sets of categorical data (Test and RETEST)
  test_col_ve <- retest_merged_data_ve[[paste0(col_name, "_test_ve")]]
  retest_col_ve <- retest_merged_data_ve[[paste0(col_name, "_retest_ve")]]
  
  # Überprüfung auf gültige Daten (Nicht-NA-Daten) in beiden Spalten
  if (all(is.na(test_col_ve)) || all(is.na(retest_col_ve))) {
    # Falls alle Daten NA sind, gib eine Zeile mit NA zurück
    return(data.frame(Column = col_name, Kappa = NA, z_value = NA, 
                      Agreement_Percentage = NA, p_value = NA))
  }
  
  # Berechnung von Cohen's Kappa mit dem irr-Paket
  kappa_result_ve <- kappa2(data.frame(test_col_ve, retest_col_ve), weight = "unweighted")
  
  # Extrahiere den Kappa-Wert und p-Wert
  kappa_value_ve <- kappa_result_ve$value
  p_value_ve <- kappa_result_ve$p.value
  agreement_percentage_ve <- mean(test_col_ve == retest_col_ve, na.rm = TRUE) * 100
  
  # Berechnung der erwarteten Übereinstimmung (p_e)
  test_table_ve <- table(test_col_ve, retest_col_ve)
  n_ve <- sum(test_table_ve)  # Anzahl der Paare
  p_e_ve <- sum(rowSums(test_table_ve) * colSums(test_table_ve)) / (n_ve^2)
  
  # Berechnung der beobachteten Übereinstimmung (p_0)
  p_0_ve <- agreement_percentage_ve / 100
  
  # Berechnung des Standardfehlers (SE)
  if (p_e_ve == 1) {
    se_kappa_ve <- NA  # Vermeide Division durch 0, wenn p_e = 1 (perfekte Übereinstimmung)
    z_value_ve <- NA
  } else {
    se_kappa_ve <- sqrt((p_0_ve * (1 - p_0_ve)) / (n_ve * (1 - p_e_ve)^2))
  
    # Berechnung des z-Werts
    z_value_ve <- kappa_value_ve / se_kappa_ve
  }
  
  # Return the column name, Kappa, z-value, agreement %, and p-value
  return(data.frame(Column = col_name, Kappa = kappa_value_ve, z_value = z_value_ve, 
                    Agreement_Percentage = agreement_percentage_ve, p_value = p_value_ve))
})

# Step 5: Combine the results into a single data frame
retest_kappa_summary_ve <- do.call(rbind, retest_kappa_results_ve)

# Step 6: Display the results
print(retest_kappa_summary_ve)


# Berechnung des gesamten Kappa-Werts über alle "_gpt"-Spalten

# Schritt 7: Erstelle zwei Vektoren mit allen Test- und Retest-Werten aus den "_gpt"-Spalten
all_test_values_ve <- unlist(retest_merged_data_ve[paste0(comp_columns_ve, "_test_ve")])
all_retest_values_ve <- unlist(retest_merged_data_ve[paste0(comp_columns_ve, "_retest_ve")])

# Schritt 8: Berechne Cohen's Kappa für alle Daten zusammen
retest_kappa_all_ve <- cohen.kappa(cbind(all_test_values_ve, all_retest_values_ve))

# Schritt 9: Zeige das Gesamtergebnis an
print(retest_kappa_all_ve)


```   
weighted macht heir absolut keinen sinn...


# SAMPLE SIZE...

## kappa sample size comp match
```{r}
# Funktion zur Berechnung des Konfidenzintervalls für Kappa
calculate_kappa_confidence_interval <- function(kappa_value, n, alpha = 0.05) {
  # Standardfehler von Kappa
  standard_error <- sqrt((1 - kappa_value^2) / n)
  
  # Z-Wert für das gewünschte Konfidenzniveau (z.B. 95%)
  z_value <- qnorm(1 - alpha / 2)
  
  # Berechnung der unteren und oberen Grenze des Konfidenzintervalls
  ci_lower <- kappa_value - z_value * standard_error
  ci_upper <- kappa_value + z_value * standard_error
  
  return(c(ci_lower, ci_upper))
}

# Vergleichs-Spalten für c_match und match extrahieren
gpt_columns_comp <- grep("^(c_match|match).*_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns_comp <- gsub("_gpt$", "_olmo", gpt_columns_comp)

# Überprüfen, ob die entsprechenden OLMO-Spalten existieren
valid_gpt_columns_comp <- gpt_columns_comp[olmo_columns_comp %in% colnames(data_gpt_olmo_processed)]
valid_olmo_columns_comp <- olmo_columns_comp[olmo_columns_comp %in% colnames(data_gpt_olmo_processed)]

# Sicherstellen, dass nur gültige Paare verwendet werden
if(length(valid_gpt_columns_comp) == 0) {
  stop("Keine gültigen Paare von GPT- und OLMO-Spalten für Comparison gefunden!")
}

# Daten für Comparison kombinieren und NA-Werte entfernen
combined_data_comp <- data.frame()
for(i in 1:length(valid_gpt_columns_comp)) {
  combined_data_comp <- rbind(combined_data_comp, data.frame(GPT = data_gpt_olmo_processed[[valid_gpt_columns_comp[i]]],
                                                             OLMO = data_gpt_olmo_processed[[valid_olmo_columns_comp[i]]],
                                                             size_kb_sum = data_gpt_olmo_processed$size_kb_sum))
}
combined_data_comp <- na.omit(combined_data_comp)

# Berechne die Terzile basierend auf der Spalte 'size_kb_sum'
tertile_cutoffs <- quantile(combined_data_comp$size_kb_sum, probs = c(1/3, 2/3))

# Einteilung in small, medium und large Terzile
combined_data_comp$tertile <- cut(combined_data_comp$size_kb_sum, 
                                  breaks = c(-Inf, tertile_cutoffs[1], tertile_cutoffs[2], Inf), 
                                  labels = c("small", "medium", "large"))

# Funktion zum Berechnen von Cohen's Kappa für eine bestimmte Terzile
compute_kappa_for_tertile <- function(data, tertile_label) {
  # Filtere Daten für das gegebene Terzil
  data_filtered <- subset(data, tertile == tertile_label)
  
  # Berechne Cohen's Kappa
  kappa_result <- irr::kappa2(data_filtered[, c("GPT", "OLMO")], "unweighted")
  kappa_value <- kappa_result$value
  
  # Berechne das Konfidenzintervall
  ci_kappa <- calculate_kappa_confidence_interval(kappa_value, nrow(data_filtered))
  
  # Berechne den Prozentsatz der Übereinstimmung
  percent_agreement <- sum(data_filtered$GPT == data_filtered$OLMO) / nrow(data_filtered) * 100
  
  # Ergebnisse runden und anzeigen
  cat("Cohen's Kappa für", tertile_label, ":\n")
  cat("Kappa:", round(kappa_value, 2), "\n")
  cat("Konfidenzintervall:", round(ci_kappa[1], 2), "-", round(ci_kappa[2], 2), "\n")
  cat("p-Wert:", round(kappa_result$p.value, 2), "\n")
  cat("z-Wert:", round(kappa_result$statistic, 2), "\n")
  cat("Prozentuale Übereinstimmung:", round(percent_agreement, 2), "%\n\n")
}

# Berechne Cohen's Kappa für small, medium und large
compute_kappa_for_tertile(combined_data_comp, "small")
compute_kappa_for_tertile(combined_data_comp, "medium")
compute_kappa_for_tertile(combined_data_comp, "large")



```



## kappa sample size nur exgtraction prereg
```{r}
# Funktion zur Berechnung des Konfidenzintervalls für Kappa
calculate_kappa_confidence_interval <- function(kappa_value, n, alpha = 0.05) {
  # Standardfehler von Kappa
  standard_error <- sqrt((1 - kappa_value^2) / n)
  
  # Z-Wert für das gewünschte Konfidenzniveau (z.B. 95%)
  z_value <- qnorm(1 - alpha / 2)
  
  # Berechnung der unteren und oberen Grenze des Konfidenzintervalls
  ci_lower <- kappa_value - z_value * standard_error
  ci_upper <- kappa_value + z_value * standard_error
  
  return(c(ci_lower, ci_upper))
}

# Extrahiere die GPT und OLMO Spalten für c_pre und pre
gpt_columns_extract_prereg <- grep("^(c_pre|pre).*_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns_extract_prereg <- gsub("_gpt$", "_olmo", gpt_columns_extract_prereg)

# Überprüfen, ob die entsprechenden OLMO-Spalten existieren
valid_gpt_columns_extract_prereg <- gpt_columns_extract_prereg[olmo_columns_extract_prereg %in% colnames(data_gpt_olmo_processed)]
valid_olmo_columns_extract_prereg <- olmo_columns_extract_prereg[olmo_columns_extract_prereg %in% colnames(data_gpt_olmo_processed)]

# Sicherstellen, dass nur gültige Paare verwendet werden
if(length(valid_gpt_columns_extract_prereg) == 0) {
  stop("Keine gültigen Paare von GPT- und OLMO-Spalten für extract_prereg gefunden!")
}

# Daten für den Vergleich kombinieren und NA-Werte entfernen
combined_data_extract_prereg <- data.frame()
for(i in 1:length(valid_gpt_columns_extract_prereg)) {
  combined_data_extract_prereg <- rbind(combined_data_extract_prereg, data.frame(GPT = data_gpt_olmo_processed[[valid_gpt_columns_extract_prereg[i]]],
                                                                                OLMO = data_gpt_olmo_processed[[valid_olmo_columns_extract_prereg[i]]],
                                                                                size_kb_prereg = data_gpt_olmo_processed$size_kb_prereg))
}
combined_data_extract_prereg <- na.omit(combined_data_extract_prereg)

# Berechne die Terzile basierend auf der Spalte 'size_kb_prereg'
tertile_cutoffs_prereg <- quantile(combined_data_extract_prereg$size_kb_prereg, probs = c(1/3, 2/3))

# Einteilung in small, medium und large Terzile
combined_data_extract_prereg$tertile_prereg <- cut(combined_data_extract_prereg$size_kb_prereg, 
                                                  breaks = c(-Inf, tertile_cutoffs_prereg[1], tertile_cutoffs_prereg[2], Inf), 
                                                  labels = c("small", "medium", "large"))

# Funktion zum Berechnen von Cohen's Kappa für eine bestimmte Terzile
compute_kappa_for_tertile_prereg <- function(data, tertile_label) {
  # Filtere Daten für das gegebene Terzil
  data_filtered <- subset(data, tertile_prereg == tertile_label)
  
  # Berechne Cohen's Kappa
  kappa_result <- irr::kappa2(data_filtered[, c("GPT", "OLMO")], "unweighted")
  kappa_value <- kappa_result$value
  
  # Berechne das Konfidenzintervall
  ci_kappa <- calculate_kappa_confidence_interval(kappa_value, nrow(data_filtered))
  
  # Berechne den Prozentsatz der Übereinstimmung
  percent_agreement <- sum(data_filtered$GPT == data_filtered$OLMO) / nrow(data_filtered) * 100
  
  # Ergebnisse runden und anzeigen
  cat("Cohen's Kappa für", tertile_label, "in extract_prereg:\n")
  cat("Kappa:", round(kappa_value, 2), "\n")
  cat("Konfidenzintervall:", round(ci_kappa[1], 2), "-", round(ci_kappa[2], 2), "\n")
  cat("p-Wert:", round(kappa_result$p.value, 2), "\n")
  cat("z-Wert:", round(kappa_result$statistic, 2), "\n")
  cat("Prozentuale Übereinstimmung:", round(percent_agreement, 2), "%\n\n")
}

# Berechne Cohen's Kappa für small, medium und large in extract_prereg
compute_kappa_for_tertile_prereg(combined_data_extract_prereg, "small")
compute_kappa_for_tertile_prereg(combined_data_extract_prereg, "medium")
compute_kappa_for_tertile_prereg(combined_data_extract_prereg, "large")


```


## kappa sample size nur extraction pubstud
```{r}
# Funktion zur Berechnung des Konfidenzintervalls für Kappa
calculate_kappa_confidence_interval <- function(kappa_value, n, alpha = 0.05) {
  # Standardfehler von Kappa
  standard_error <- sqrt((1 - kappa_value^2) / n)
  
  # Z-Wert für das gewünschte Konfidenzniveau (z.B. 95%)
  z_value <- qnorm(1 - alpha / 2)
  
  # Berechnung der unteren und oberen Grenze des Konfidenzintervalls
  ci_lower <- kappa_value - z_value * standard_error
  ci_upper <- kappa_value + z_value * standard_error
  
  return(c(ci_lower, ci_upper))
}

# Extrahiere die GPT und OLMO Spalten für c_pap und pap
gpt_columns_extract_pubstud <- grep("^(c_pap|pap).*_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns_extract_pubstud <- gsub("_gpt$", "_olmo", gpt_columns_extract_pubstud)

# Überprüfen, ob die entsprechenden OLMO-Spalten existieren
valid_gpt_columns_extract_pubstud <- gpt_columns_extract_pubstud[olmo_columns_extract_pubstud %in% colnames(data_gpt_olmo_processed)]
valid_olmo_columns_extract_pubstud <- olmo_columns_extract_pubstud[olmo_columns_extract_pubstud %in% colnames(data_gpt_olmo_processed)]

# Sicherstellen, dass nur gültige Paare verwendet werden
if(length(valid_gpt_columns_extract_pubstud) == 0) {
  stop("Keine gültigen Paare von GPT- und OLMO-Spalten für extract_pubstud gefunden!")
}

# Daten für den Vergleich kombinieren und NA-Werte entfernen
combined_data_extract_pubstud <- data.frame()
for(i in 1:length(valid_gpt_columns_extract_pubstud)) {
  combined_data_extract_pubstud <- rbind(combined_data_extract_pubstud, data.frame(GPT = data_gpt_olmo_processed[[valid_gpt_columns_extract_pubstud[i]]],
                                                                                  OLMO = data_gpt_olmo_processed[[valid_olmo_columns_extract_pubstud[i]]],
                                                                                  size_kb_pubstud = data_gpt_olmo_processed$size_kb_pubstud))
}
combined_data_extract_pubstud <- na.omit(combined_data_extract_pubstud)

# Berechne die Terzile basierend auf der Spalte 'size_kb_pubstud'
tertile_cutoffs_pubstud <- quantile(combined_data_extract_pubstud$size_kb_pubstud, probs = c(1/3, 2/3))

# Einteilung in small, medium und large Terzile
combined_data_extract_pubstud$tertile_pubstud <- cut(combined_data_extract_pubstud$size_kb_pubstud, 
                                                    breaks = c(-Inf, tertile_cutoffs_pubstud[1], tertile_cutoffs_pubstud[2], Inf), 
                                                    labels = c("small", "medium", "large"))

# Funktion zum Berechnen von Cohen's Kappa für eine bestimmte Terzile
compute_kappa_for_tertile_pubstud <- function(data, tertile_label) {
  # Filtere Daten für das gegebene Terzil
  data_filtered <- subset(data, tertile_pubstud == tertile_label)
  
  # Berechne Cohen's Kappa
  kappa_result <- irr::kappa2(data_filtered[, c("GPT", "OLMO")], "unweighted")
  kappa_value <- kappa_result$value
  
  # Berechne das Konfidenzintervall
  ci_kappa <- calculate_kappa_confidence_interval(kappa_value, nrow(data_filtered))
  
  # Berechne den Prozentsatz der Übereinstimmung
  percent_agreement <- sum(data_filtered$GPT == data_filtered$OLMO) / nrow(data_filtered) * 100
  
  # Ergebnisse runden und anzeigen
  cat("Cohen's Kappa für", tertile_label, "in extract_pubstud:\n")
  cat("Kappa:", round(kappa_value, 2), "\n")
  cat("Konfidenzintervall:", round(ci_kappa[1], 2), "-", round(ci_kappa[2], 2), "\n")
  cat("p-Wert:", round(kappa_result$p.value, 2), "\n")
  cat("z-Wert:", round(kappa_result$statistic, 2), "\n")
  cat("Prozentuale Übereinstimmung:", round(percent_agreement, 2), "%\n\n")
}

# Berechne Cohen's Kappa für small, medium und large in extract_pubstud
compute_kappa_for_tertile_pubstud(combined_data_extract_pubstud, "small")
compute_kappa_for_tertile_pubstud(combined_data_extract_pubstud, "medium")
compute_kappa_for_tertile_pubstud(combined_data_extract_pubstud, "large")



```


# tabelle zusammengeführt
```{r}
# Erstelle die Daten für die Tabelle im gewünschten Format (K = ... [95% CI])
terzile <- c("small", "medium", "large")
match_kappa <- c("K = 0.30 [0.27, 0.33]", "K = 0.25 [0.22, 0.28]", "K = 0.28 [0.25, 0.31]")
prereg_kappa <- c("K = 0.31 [0.28, 0.34]", "K = 0.36 [0.33, 0.39]", "K = 0.29 [0.26, 0.32]")
pubstud_kappa <- c("K = 0.32 [0.29, 0.35]", "K = 0.30 [0.27, 0.32]", "K = 0.30 [0.27, 0.33]")

# Erstelle ein Dataframe für die Tabelle
df <- data.frame(
  Terzile = terzile,
  Match = match_kappa,
  Extract_Prereg = prereg_kappa,
  Extract_Pubstud = pubstud_kappa
)

# Erstelle die Tabelle mit flextable
ft <- flextable(df)

# Formatierungen und Spaltenüberschriften
ft <- theme_booktabs(ft) %>%
  autofit() %>%
  set_header_labels(
    Terzile = "Terzile",
    Match = "Match",
    Extract_Prereg = "Extract Prereg",
    Extract_Pubstud = "Extract Pubstud"
  )

# Zeige die Tabelle direkt in der R-Konsole
ft

```




# 
```{r}


```



#PER PSP

##kappa per psp 
```{r}


# Spalten mit _gpt und _olmo suffix finden
gpt_columns_peritem <- grep("_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns_peritem <- gsub("_gpt$", "_olmo", gpt_columns_peritem)

# Überprüfen, ob die entsprechenden _olmo-Spalten existieren
valid_gpt_columns_peritem <- gpt_columns_peritem[olmo_columns_peritem %in% colnames(data_gpt_olmo_processed)]
valid_olmo_columns_peritem <- olmo_columns_peritem[olmo_columns_peritem %in% colnames(data_gpt_olmo_processed)]

# Sicherstellen, dass nur gültige Paare verwendet werden
if(length(valid_gpt_columns_peritem) == 0) {
  stop("Keine gültigen Paare von GPT- und OLMO-Spalten gefunden!")
}

# Berechnung von Kappa pro Zeile basierend auf der Spalte 'psp'
psp_values <- unique(data_gpt_olmo_processed$psp)
results_peritem <- data.frame(psp = psp_values, kappa_value = NA, percent_agreement = NA)

for (i in 1:length(psp_values)) {
  # Filtere die Daten für die aktuelle Zeile/psp-Nummer
  current_data <- data_gpt_olmo_processed[data_gpt_olmo_processed$psp == psp_values[i], ]
  
  # Kombiniere die Spalten von GPT und OLMO
  combined_data_peritem <- data.frame()
  
  for(j in 1:length(valid_gpt_columns_peritem)) {
    combined_data_peritem <- rbind(combined_data_peritem, data.frame(GPT = current_data[[valid_gpt_columns_peritem[j]]],
                                                                     OLMO = current_data[[valid_olmo_columns_peritem[j]]]))
  }
  
  # Entferne NA-Werte
  combined_data_peritem <- na.omit(combined_data_peritem)
  
  # Berechne Cohen's Kappa pro Zeile/psp
  if (nrow(combined_data_peritem) > 0) {
    kappa_result_peritem <- irr::kappa2(combined_data_peritem, "unweighted")
    percent_agreement_peritem <- sum(combined_data_peritem$GPT == combined_data_peritem$OLMO) / nrow(combined_data_peritem) * 100
    results_peritem$kappa_value[i] <- kappa_result_peritem$value
    results_peritem$percent_agreement[i] <- percent_agreement_peritem
  }
}

# Ergebnisse anzeigen
cat("Kappa pro Zeile (psp):\n")
print(results_peritem)




```



## average over psp
```{r}
# Überprüfen, ob die Spalte "kappa_value" im Datensatz results_peritem existiert
if("kappa_value" %in% colnames(results_peritem)) {
  # Berechne den Durchschnitt der Spalte kappa_value
  Average_Kappa_over_psp <- mean(results_peritem$kappa_value, na.rm = TRUE)
  
  # Ausgabe des Ergebnisses
  cat("Durchschnittlicher Kappa-Wert über alle psp:", Average_Kappa_over_psp, "\n")
} else {
  stop("Die Spalte 'kappa_value' existiert nicht im Datensatz results_peritem!")
}


```



## Overview per psp
```{r}
# Überblick über die Kappa-Werte und Prozentübereinstimmung

# Datensatz nach Kappa-Werten sortieren (absteigend)
results_sorted_kappa <- results_peritem[order(-results_peritem$kappa_value), ]

# Datensatz nach Prozentübereinstimmung sortieren (absteigend)
results_sorted_agreement <- results_peritem[order(-results_peritem$percent_agreement), ]

# Kategorisierung der Kappa-Werte (Beispiel: gut >= 0.75, mittel >= 0.4, schlecht < 0.4)
results_peritem$kappa_category <- cut(results_peritem$kappa_value, 
                                      breaks = c(-Inf, 0.4, 0.75, Inf), 
                                      labels = c("schlecht", "mittel", "gut"))

# Zusammenfassung der Kategorien
summary_kappa <- table(results_peritem$kappa_category)

# Ergebnisse anzeigen
cat("Kappa-Kategorien Überblick:\n")
print(summary_kappa)

# Optional: Visualisierung der Kappa-Werte
boxplot(results_peritem$kappa_value, main = "Verteilung der Kappa-Werte", ylab = "Kappa-Wert")

# Optional: Histogramm der Kappa-Werte
hist(results_peritem$kappa_value, main = "Histogramm der Kappa-Werte", xlab = "Kappa-Wert", breaks = 10)

# Übersicht der besten und schlechtesten psp-Werte (Top 5 und Bottom 5)
cat("Top 5 PSP nach Kappa-Wert:\n")
print(head(results_sorted_kappa, 5))

cat("Bottom 5 PSP nach Kappa-Wert:\n")
print(tail(results_sorted_kappa, 5))

cat("Top 5 PSP nach Prozentübereinstimmung:\n")
print(head(results_sorted_agreement, 5))

cat("Bottom 5 PSP nach Prozentübereinstimmung:\n")
print(tail(results_sorted_agreement, 5))



```

## für analyse welche psp gut
```{r}


# Left join: data_gpt_olmo_processed und results_sorted_kappa auf Basis der Spalte 'psp' zusammenführen
analyse_per_psp <-  results_sorted_kappa %>%
  left_join(data_gpt_olmo_processed, by = "psp")

# Optional: Einen Überblick über den neuen Datensatz anzeigen
head(analyse_per_psp)

# Optional: Die Struktur des neuen Datensatzes anzeigen
str(analyse_per_psp)


```


# kappa per expeiment type 
```{r}
# Funktion zur Berechnung des Konfidenzintervalls für Kappa
calculate_kappa_confidence_interval <- function(kappa_value, n, alpha = 0.05) {
  # Standardfehler von Kappa
  standard_error <- sqrt((1 - kappa_value^2) / n)
  
  # Z-Wert für das gewünschte Konfidenzniveau (z.B. 95%)
  z_value <- qnorm(1 - alpha / 2)
  
  # Berechnung der unteren und oberen Grenze des Konfidenzintervalls
  ci_lower <- kappa_value - z_value * standard_error
  ci_upper <- kappa_value + z_value * standard_error
  
  return(c(ci_lower, ci_upper))
}

# Spalten mit _gpt und _olmo Suffix finden
gpt_columns_experimenttype <- grep("_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns_experimenttype <- gsub("_gpt$", "_olmo", gpt_columns_experimenttype)

# Überprüfen, ob die entsprechenden _olmo-Spalten existieren
valid_gpt_columns_experimenttype <- gpt_columns_experimenttype[olmo_columns_experimenttype %in% colnames(data_gpt_olmo_processed)]
valid_olmo_columns_experimenttype <- olmo_columns_experimenttype[olmo_columns_experimenttype %in% colnames(data_gpt_olmo_processed)]

# Sicherstellen, dass nur gültige Paare verwendet werden
if(length(valid_gpt_columns_experimenttype) == 0) {
  stop("Keine gültigen Paare von GPT- und OLMO-Spalten gefunden!")
}

# Definiere die verschiedenen Kategorien in der 'type' Spalte
types_experimenttype <- c("Association", "Interaction / moderated effect", "Effect", "Mediated effect", "Moderated association")

# Initialisiere eine leere Tabelle für die Ergebnisse
results_table <- data.frame(
  Experiment_Type = character(),
  Kappa = numeric(),
  Confidence_Interval = character(),
  Agreement_Percent = numeric(),
  N_Comparisons = integer(),
  stringsAsFactors = FALSE
)

# Schleife durch jede Kategorie und berechne das Kappa für jede
for (type_experimenttype in types_experimenttype) {
  # Filtere die Zeilen, die zur aktuellen Kategorie gehören
  type_data_experimenttype <- data_gpt_olmo_processed[data_gpt_olmo_processed$type == type_experimenttype, ]
  
  # Kombinieren der Spalten in einem DataFrame für den Vergleich der aktuellen Kategorie
  combined_data_experimenttype <- data.frame()
  
  for(i in 1:length(valid_gpt_columns_experimenttype)) {
    # Füge die Werte der GPT und OLMO Spalten zur kombinierten Datensatz hinzu
    combined_data_experimenttype <- rbind(combined_data_experimenttype, 
                                          data.frame(GPT = type_data_experimenttype[[valid_gpt_columns_experimenttype[i]]],
                                                     OLMO = type_data_experimenttype[[valid_olmo_columns_experimenttype[i]]]))
  }
  
  # Überprüfen, ob die Daten eine ausreichende Anzahl von Spalten haben
  nc <- ncol(combined_data_experimenttype)
  
  if (!is.null(nc) && nc > 1) {
    # Entferne NA-Werte
    combined_data_experimenttype <- na.omit(combined_data_experimenttype)
    
    # Berechne Cohen's Kappa für die aktuelle Kategorie
    if (nrow(combined_data_experimenttype) > 0) {
      kappa_result_experimenttype <- irr::kappa2(combined_data_experimenttype, "unweighted")
      kappa_value_experimenttype <- kappa_result_experimenttype$value
      
      # Berechne das Konfidenzintervall
      ci_kappa_experimenttype <- calculate_kappa_confidence_interval(kappa_value_experimenttype, nrow(combined_data_experimenttype))
      
      # Berechne den Prozentsatz der Übereinstimmung
      percent_agreement_experimenttype <- sum(combined_data_experimenttype$GPT == combined_data_experimenttype$OLMO) / nrow(combined_data_experimenttype) * 100
      
      # Füge die Ergebnisse der Tabelle hinzu
      results_table <- rbind(results_table, data.frame(
        Experiment_Type = type_experimenttype,
        Kappa = round(kappa_value_experimenttype, 2),
        Confidence_Interval = paste0("[", round(ci_kappa_experimenttype[1], 2), ", ", round(ci_kappa_experimenttype[2], 2), "]"),
        Agreement_Percent = round(percent_agreement_experimenttype, 2),
        N_Comparisons = nrow(combined_data_experimenttype)
      ))
      
      # Ergebnisse anzeigen
      cat("\nCohen's Kappa für Kategorie", type_experimenttype, ":\n")
      cat("Kappa_experimenttype:", round(kappa_value_experimenttype, 2), "\n")
      cat("Konfidenzintervall_experimenttype:", round(ci_kappa_experimenttype[1], 2), "-", round(ci_kappa_experimenttype[2], 2), "\n")
      cat("Prozentuale Übereinstimmung_experimenttype:", round(percent_agreement_experimenttype, 2), "%\n")
      cat("Anzahl der Vergleiche:", nrow(combined_data_experimenttype), "\n")
    } else {
      cat("Keine ausreichenden Daten für die Kategorie", type_experimenttype, "\n")
    }
  } else {
    cat("Nicht genügend Spalten für die Kategorie", type_experimenttype, "\n")
  }
}

# Zeige die Tabelle mit den Ergebnissen an
print(results_table)


```


# protoype olmo
```{r}
#prototypische Kategorien

# Spalten, die auf _olmo enden, finden
olmo_columns <- grep("_olmo$", colnames(data_gpt_olmo_processed), value = TRUE)

# Erstelle einen leeren Vektor für die prototypischen Kategorien
prototyp_olmo <- vector("list", length(olmo_columns))

# Berechne die prototypische Kategorie (häufigste Kategorie) für jede _olmo-Spalte
for (i in seq_along(olmo_columns)) {
  column <- olmo_columns[i]
  
  # Häufigkeiten der Kategorien in der aktuellen Spalte berechnen
  category_frequencies <- table(data_gpt_olmo_processed[[column]])
  
  if (length(category_frequencies) > 0) {
    # Die häufigste Kategorie (prototypische Kategorie) bestimmen
    most_frequent_category <- names(which.max(category_frequencies))
    # Speichern der prototypischen Kategorie
    prototyp_olmo[[i]] <- most_frequent_category
  } else {
    # Falls die Spalte leer ist oder keine Kategorien vorhanden sind
    prototyp_olmo[[i]] <- NA
  }
}

# Erstelle ein DataFrame mit den prototypischen Kategorien
prototyp_olmo_df <- data.frame(column_name = olmo_columns, prototyp_olmo = unlist(prototyp_olmo))

# prototyp_olmo_df anzeigen
print(prototyp_olmo_df)


```




# sevtl protoype gpt .... fast stärker mit guten studien??
```{r}


```




# versuchsperson extrahieren
```{r}
# Erstellen eines neuen Datensatzes mit den relevanten Spalten (plus Versuchsperson)
data_gpt_olmo_numeric <- data_gpt_olmo_processed[, c("psp", "pre_dcp_1_text_gpt", "pre_dcp_1_text_olmo", "pap_dcp_1_text_gpt", "pap_dcp_1_text_olmo")]

# Umwandlung der Spalten in numerische Werte
data_gpt_olmo_numeric$pre_dcp_1_text_gpt_numeric <- as.numeric(data_gpt_olmo_numeric$pre_dcp_1_text_gpt)
data_gpt_olmo_numeric$pre_dcp_1_text_olmo_numeric <- as.numeric(data_gpt_olmo_numeric$pre_dcp_1_text_olmo)
data_gpt_olmo_numeric$pap_dcp_1_text_gpt_numeric <- as.numeric(data_gpt_olmo_numeric$pap_dcp_1_text_gpt)
data_gpt_olmo_numeric$pap_dcp_1_text_olmo_numeric <- as.numeric(data_gpt_olmo_numeric$pap_dcp_1_text_olmo)

# Logik für übereinstimmend_pre
data_gpt_olmo_numeric$uebereinstimmend_pre <- ifelse(
  data_gpt_olmo_numeric$pre_dcp_1_text_gpt == "exclude", NA, 
  ifelse(!is.na(data_gpt_olmo_numeric$pre_dcp_1_text_gpt_numeric) & is.na(data_gpt_olmo_numeric$pre_dcp_1_text_olmo_numeric), "INVENT", 
    ifelse(data_gpt_olmo_numeric$pre_dcp_1_text_gpt_numeric == data_gpt_olmo_numeric$pre_dcp_1_text_olmo_numeric, "IDENTICAL", "DIFFERENT")
  )
)

# Logik für übereinstimmend_pap
data_gpt_olmo_numeric$uebereinstimmend_pap <- ifelse(
  data_gpt_olmo_numeric$pap_dcp_1_text_gpt == "exclude", NA, 
  ifelse(!is.na(data_gpt_olmo_numeric$pap_dcp_1_text_gpt_numeric) & is.na(data_gpt_olmo_numeric$pap_dcp_1_text_olmo_numeric), "INVENT", 
    ifelse(data_gpt_olmo_numeric$pap_dcp_1_text_gpt_numeric == data_gpt_olmo_numeric$pap_dcp_1_text_olmo_numeric, "IDENTICAL", "DIFFERENT")
  )
)

# Berechnung der Abweichung für pre
data_gpt_olmo_numeric$abweichung_pre <- data_gpt_olmo_numeric$pre_dcp_1_text_gpt_numeric - data_gpt_olmo_numeric$pre_dcp_1_text_olmo_numeric

# Berechnung der Abweichung für pap
data_gpt_olmo_numeric$abweichung_pap <- data_gpt_olmo_numeric$pap_dcp_1_text_gpt_numeric - data_gpt_olmo_numeric$pap_dcp_1_text_olmo_numeric

# Ausgabe eines Teils des Datensatzes zur Überprüfung
head(data_gpt_olmo_numeric)




```




# kuchendiagramm erstellen
```{r}
# Zählen der Häufigkeiten für übereinstimmend_pre
pre_counts <- as.data.frame(table(data_gpt_olmo_numeric$uebereinstimmend_pre, useNA = "ifany"))
colnames(pre_counts) <- c("Kategorie", "Anzahl")

# Zählen der Häufigkeiten für übereinstimmend_pap
pap_counts <- as.data.frame(table(data_gpt_olmo_numeric$uebereinstimmend_pap, useNA = "ifany"))
colnames(pap_counts) <- c("Kategorie", "Anzahl")

# Kuchendiagramm für übereinstimmend_pre
plot_pre <- ggplot(pre_counts, aes(x = "", y = Anzahl, fill = Kategorie)) +
  geom_bar(width = 1, stat = "identity") +
  coord_polar("y", start = 0) +
  theme_void() +
  labs(title = "Übereinstimmung PRE") +
  scale_fill_manual(values = c("NA" = "grey", "INVENT" = "orange", "IDENTICAL" = "green", "DIFFERENT" = "red"))

# Kuchendiagramm für übereinstimmend_pap
plot_pap <- ggplot(pap_counts, aes(x = "", y = Anzahl, fill = Kategorie)) +
  geom_bar(width = 1, stat = "identity") +
  coord_polar("y", start = 0) +
  theme_void() +
  labs(title = "Übereinstimmung PAP") +
  scale_fill_manual(values = c("NA" = "grey", "INVENT" = "orange", "IDENTICAL" = "green", "DIFFERENT" = "red"))

# Anzeigen der beiden Diagramme
print(plot_pre)
print(plot_pap)

```




# N zahlen
```{r}
# Berechnung der absoluten Zahlen und Prozentsätze für übereinstimmend_pre
pre_counts <- as.data.frame(table(data_gpt_olmo_numeric$uebereinstimmend_pre, useNA = "ifany"))
colnames(pre_counts) <- c("Kategorie", "Anzahl")
pre_counts$Prozentsatz <- round(100 * pre_counts$Anzahl / sum(pre_counts$Anzahl), 2)

# Berechnung der absoluten Zahlen und Prozentsätze für übereinstimmend_pap
pap_counts <- as.data.frame(table(data_gpt_olmo_numeric$uebereinstimmend_pap, useNA = "ifany"))
colnames(pap_counts) <- c("Kategorie", "Anzahl")
pap_counts$Prozentsatz <- round(100 * pap_counts$Anzahl / sum(pap_counts$Anzahl), 2)

# Berechnung der durchschnittlichen Abweichung für abweichung_pre
durchschnittliche_abweichung_pre <- mean(data_gpt_olmo_numeric$abweichung_pre, na.rm = TRUE)

# Berechnung der durchschnittlichen Abweichung für abweichung_pap
durchschnittliche_abweichung_pap <- mean(data_gpt_olmo_numeric$abweichung_pap, na.rm = TRUE)

# Ausgabe der Ergebnisse für übereinstimmend_pre
cat("Übereinstimmend PRE - Absolute Zahlen und Prozentsätze:\n")
print(pre_counts)

# Ausgabe der Ergebnisse für übereinstimmend_pap
cat("Übereinstimmend PAP - Absolute Zahlen und Prozentsätze:\n")
print(pap_counts)

# Ausgabe der durchschnittlichen Abweichung
cat("Durchschnittliche Abweichung PRE:", durchschnittliche_abweichung_pre, "\n")
cat("Durchschnittliche Abweichung PAP:", durchschnittliche_abweichung_pap, "\n")


```




# 
```{r}


```





# Session info_
```{r}

sessionInfo()

```


