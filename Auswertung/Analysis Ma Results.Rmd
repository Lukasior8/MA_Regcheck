---
title: "Analysis MA results"
author: "Luki"
date: "2024-09-19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#todo:
- 
- herausfinden welche Werte man angeben MUSS (pwert, xwert , % etc..)
- kappa auf Basis von diesem ausrechnen für übereinstimmung gpt & olmo


- Assumption checks herausfinden welche und ausrechnen
->>>< also zb ob es problematisch ist dass teilweise fast nur y oder fast nur n oder so ist.....


-> why df isch p wert so gross! hmm vilech wäge nidrige 
- weitere analyse....

- Analysis File beschönigen.

- das ziel ist ja: Synthetic peer review: Using large language models to automatically detect deviations from preregistrations

dh. insbesondere die "n" sind interessant um eine Aussage darüber zu machen wie viel % davon entdeckt werden und viel viel fals positive etc....und es wäre auch noch nice zu wissen welche Kategorien man brauchen kann und welche nicht...


extraction vs compare prompt..

# Dependencies

```{r}
library(tidyverse)
library("readxl")
library(janitor)
library(stringr)
library(openxlsx)
library(dplyr)
library(irr)
library(psych)
library(formattable)
library(kableExtra)
library(gt)
library(irr)
library(caret) 





```


#Read  data


```{r}
getwd() 


comparison_data_processed <-read_excel("../Data/processed/comparison_data_processed.xlsx")


data_gpt_olmo_processed <-read_excel("../Data/processed/data_gpt_olmo_processed.xlsx")


data_gpt_ve_processed <-read_excel("../Data/processed/data_gpt_ve_processed.xlsx")

# Filterung des Datensatzes, um Zeilen mit master_exclude == "exclude" auszuschließen
data_gpt_ve_filtered <- data_gpt_ve_processed %>%
  filter(master_exclude != "exclude")

```
# Category Counts
## category counts total

hier noch die absoluten zahlen ohne.oo hinschreiben...

```{r}

# Step 1: Select the columns that end with "_comp"
comp_columns <- comparison_data_processed %>%
  select(ends_with("_comp"))

# Step 2: Create a new dataset that counts the occurrences of each category across all "_comp" columns
data_category_counts_total <- comp_columns %>%
  summarise(
    Identical_YES = sum(comp_columns == "Y", na.rm = TRUE),
    Identical_NO = sum(comp_columns == "N", na.rm = TRUE),
    Identical_X = sum(comp_columns == "X", na.rm = TRUE),
    Different_Y_to_N = sum(comp_columns == "P", na.rm = TRUE),
    Different_N_to_Y = sum(comp_columns == "Q", na.rm = TRUE),
    GPT_Y_Olmo_X = sum(comp_columns == "FY", na.rm = TRUE),
    GPT_N_Olmo_X = sum(comp_columns == "FN", na.rm = TRUE),
    GPT_X_Olmo_N = sum(comp_columns == "FXN", na.rm = TRUE),
    GPT_X_Olmo_Y = sum(comp_columns == "FXY", na.rm = TRUE),
    n_answers_total = sum(
      comp_columns == "Y" | comp_columns == "N" | comp_columns == "X" |
      comp_columns == "P" | comp_columns == "Q" | comp_columns == "FY" |
      comp_columns == "FN" | comp_columns == "FXN" | comp_columns == "FXY", na.rm = TRUE
    )
  )

# Step 3: Add percentages as a second row
data_category_counts_percent <- data_category_counts_total %>%
  mutate(across(-n_answers_total, ~ round((.x / data_category_counts_total$n_answers_total) * 100, 2))) %>%
  mutate(n_answers_total = 100)


# Step 4: Combine the two with row labels
data_category_counts_total <- rbind(
  cbind(type = "counts_total_absolute", data_category_counts_total),
  cbind(type = "counts_total_percent", data_category_counts_percent)
)

# Step 4: Print the combined table with both counts and percentages
print(data_category_counts_total)



```



### 3x3 confusionsmatrix

hier müsste ich evventuell noch zusätzlich die Anzahl vvon Olmo... als Vergleich.

```{r}

# Angenommen, deine 2x11-Tabelle heißt "data_category_counts_total"
# und enthält die Zählungen für jede Kategorie (z.B. Y, N, X).

# Extrahiere nur die relevanten Werte für Y, N, X (jeweils für tatsächliche und vorhergesagte Werte)
actual_Y <- data_category_counts_total$Identical_YES[1]  # Zähle die Y-Zuordnungen
actual_N <- data_category_counts_total$Identical_NO[1]   # Zähle die N-Zuordnungen
actual_X <- data_category_counts_total$Identical_X[1]    # Zähle die X-Zuordnungen

pred_Y_to_N <- data_category_counts_total$Different_Y_to_N[1]  # Y -> N Zählungen
pred_N_to_Y <- data_category_counts_total$Different_N_to_Y[1]  # N -> Y Zählungen
pred_Y_to_X <- data_category_counts_total$GPT_Y_Olmo_X[1]      # Y -> X Zählungen
pred_N_to_X <- data_category_counts_total$GPT_N_Olmo_X[1]      # N -> X Zählungen
pred_X_to_N <- data_category_counts_total$GPT_X_Olmo_N[1]      # X -> N Zählungen
pred_X_to_Y <- data_category_counts_total$GPT_X_Olmo_Y[1]      # X -> Y Zählungen

# Erstelle eine leere 3x3-Matrix
confusion_matrix <- matrix(0, nrow = 3, ncol = 3, 
                           dimnames = list(c("Actual_Y", "Actual_N", "Actual_X"), 
                                           c("Predicted_Y", "Predicted_N", "Predicted_X")))

# Fülle die Konfusionsmatrix basierend auf den Zählungen aus
confusion_matrix["Actual_Y", "Predicted_Y"] <- actual_Y
confusion_matrix["Actual_N", "Predicted_N"] <- actual_N
confusion_matrix["Actual_X", "Predicted_X"] <- actual_X

# Füge die 'False Positives' und 'False Negatives' hinzu
confusion_matrix["Actual_Y", "Predicted_N"] <- pred_Y_to_N
confusion_matrix["Actual_N", "Predicted_Y"] <- pred_N_to_Y
confusion_matrix["Actual_Y", "Predicted_X"] <- pred_Y_to_X
confusion_matrix["Actual_X", "Predicted_Y"] <- pred_X_to_Y
confusion_matrix["Actual_N", "Predicted_X"] <- pred_N_to_X
confusion_matrix["Actual_X", "Predicted_N"] <- pred_X_to_N

# Ausgabe der 3x3 Konfusionsmatrix
print(confusion_matrix)



```
bei den category counts total sieht man welche kategorie insgesamt viel verwendet wurde…. Aber halt nicht so aussagekräftig, da n kommt auch bei olmo seltener vor...ncith so aussagekräfitg... man müsste prozentual zu allen berechnungen( also dass y,n,x gleich häufig vorkommen) aber man sieht schon, dass nur ca 50 & richtig predictet... halt sehr schlecht... 


## category counts per item

```{r}


# Step 1: Select the columns that end with "_comp"
comp_columns <- comparison_data_processed %>%
  select(ends_with("_comp"))

# Step 2: Create a dataset that counts occurrences of each category for each column using reframe()
data_category_counts_per_item <- comp_columns %>%
  reframe(across(everything(), ~ tibble(
    Identical_YES = sum(. == "Y", na.rm = TRUE),
    Identical_NO = sum(. == "N", na.rm = TRUE),
    Identical_X = sum(. == "X", na.rm = TRUE),
    Different_Y_to_N = sum(. == "P", na.rm = TRUE),
    Different_N_to_Y = sum(. == "Q", na.rm = TRUE),
    GPT_Y_Olmo_X = sum(. == "FY", na.rm = TRUE),
    GPT_N_Olmo_X = sum(. == "FN", na.rm = TRUE),
    GPT_X_Olmo_N = sum(. == "FXN", na.rm = TRUE),
    GPT_X_Olmo_Y = sum(. == "FXY", na.rm = TRUE)
  )))

# Step 3: Transpose the data so that each category has its own column
data_category_counts_per_item <- data_category_counts_per_item %>%
  pivot_longer(cols = everything(), names_to = "Column", values_to = "Counts") %>%
  unnest_wider(Counts)

# Step 4: Print the result
print(data_category_counts_per_item)


write.xlsx(data_category_counts_per_item, "../Data/processed/data_category_counts_per_item.xlsx")


```

# sensitivity & specifity 



## Sensitivität und Spezifität für jede Klasse (One-vs-Rest)


hier könnte man noch sensitivität für jedes Item...!

```{r}

print(confusion_matrix)
n_total <- sum(confusion_matrix)

# Berechnung der Sensitivität, Spezifität und Präzision für jede Klasse

# Y-Klasse
TP_Y <- confusion_matrix['Actual_Y', 'Predicted_Y']
FN_Y <- sum(confusion_matrix['Actual_Y', ]) - TP_Y
FP_Y <- sum(confusion_matrix[, 'Predicted_Y']) - TP_Y
TN_Y <- sum(confusion_matrix) - (TP_Y + FN_Y + FP_Y)

sensitivity_Y <- TP_Y / (TP_Y + FN_Y)
specificity_Y <- TN_Y / (TN_Y + FP_Y)
precision_Y <- TP_Y / (TP_Y + FP_Y)

# N-Klasse
TP_N <- confusion_matrix['Actual_N', 'Predicted_N']
FN_N <- sum(confusion_matrix['Actual_N', ]) - TP_N
FP_N <- sum(confusion_matrix[, 'Predicted_N']) - TP_N
TN_N <- sum(confusion_matrix) - (TP_N + FN_N + FP_N)

sensitivity_N <- TP_N / (TP_N + FN_N)
specificity_N <- TN_N / (TN_N + FP_N)
precision_N <- TP_N / (TP_N + FP_N)

# X-Klasse
TP_X <- confusion_matrix['Actual_X', 'Predicted_X']
FN_X <- sum(confusion_matrix['Actual_X', ]) - TP_X
FP_X <- sum(confusion_matrix[, 'Predicted_X']) - TP_X
TN_X <- sum(confusion_matrix) - (TP_X + FN_X + FP_X)

sensitivity_X <- TP_X / (TP_X + FN_X)
specificity_X <- TN_X / (TN_X + FP_X)
precision_X <- TP_X / (TP_X + FP_X)

# Ergebnisse in einer Tabelle anzeigen
results <- data.frame(
  Class = c("Y", "N", "X"),
  Sensitivity = c(sensitivity_Y, sensitivity_N, sensitivity_X),
  Specificity = c(specificity_Y, specificity_N, specificity_X),
  Precision = c(precision_Y, precision_N, precision_X)
)

print(results)

```

Sensitivity (Sensitivität) gibt an, wie gut das Modell positive Fälle (z.B. "Y") korrekt erkennt. Es ist das Verhältnis von richtig-positiven Vorhersagen zu allen tatsächlichen positiven Fällen. Höhere Werte zeigen eine bessere Erkennungsrate der positiven Fälle.

Specificity (Spezifität) gibt an, wie gut das Modell negative Fälle (z.B. "Nicht-Y") korrekt erkennt. Es ist das Verhältnis von richtig-negativen Vorhersagen zu allen tatsächlichen negativen Fällen. Höhere Werte bedeuten, dass das Modell weniger falsch-positive Vorhersagen macht.

Precision (Präzision) gibt an, wie viele der als positiv (z.B. "Y") vorhergesagten Fälle tatsächlich positiv sind. Es ist das Verhältnis von richtig-positiven Vorhersagen zu allen positiven Vorhersagen. Hohe Präzision bedeutet, dass das Modell selten falsch-positive Vorhersagen macht.

## Gesamtleistung: Macro- und Micro-Averaging

```{r}
## Berechnung von Macro-Averaging (Mittelwert der Sensitivitäten, Spezifitäten und Präzisionen)
macro_sensitivity <- mean(c(sensitivity_Y, sensitivity_N, sensitivity_X))
macro_specificity <- mean(c(specificity_Y, specificity_N, specificity_X))
macro_precision <- mean(c(precision_Y, precision_N, precision_X))

# Micro-Averaging (Summe der TP, FN, FP, TN über alle Klassen)
TP_total <- TP_Y + TP_N + TP_X
FN_total <- FN_Y + FN_N + FN_X
FP_total <- FP_Y + FP_N + FP_X
TN_total <- TN_Y + TN_N + TN_X

micro_sensitivity <- TP_total / (TP_total + FN_total)
micro_specificity <- TN_total / (TN_total + FP_total)
micro_precision <- TP_total / (TP_total + FP_total)



# Berechnung der Accuracy (Genauigkeit)s
accuracy1 <- (TP_total + TN_total) / (TP_total + TN_total + FP_total + FN_total)

accuracy2 <- TP_total/(n_total)

balanced_accuracy <- (sensitivity_Y + sensitivity_N + sensitivity_X)/3

# Ergebnisse für Macro, Micro und Accuracy anzeigen
overall_results <- data.frame(
  Metric = c("Macro Sensitivity", "Macro Specificity", "Macro Precision",
             "Micro Sensitivity", "Micro Specificity", "Micro Precision",
            "accuracy1","accuracy2","balanced_accuracy"),
  Value = c(macro_sensitivity, macro_specificity, macro_precision,
            micro_sensitivity, micro_specificity, micro_precision,
            accuracy1,accuracy2,balanced_accuracy)
)

print(overall_results)


```



## 

```{r}



```




## 

```{r}



```



# Kappa retest
habe ich hier jeweils nur gpt genommen? oder auch olmo ausversehen?!

## test, retest korrelation Cohens Kappa per category

```{r}


# Step 1: Filter the dataset into test and retest subsets
test_data <- comparison_data_processed %>% filter(test_retest == "Test")
retest_data <- comparison_data_processed %>% filter(test_retest == "RETEST")

# Step 2: Match test and retest based on 'psp'(so this only contains the 30 test-retst Psp rows)
retest_merged_data <- merge(test_data, retest_data, by = "psp", suffixes = c("_test", "_retest"))

# Step 3: Get all columns ending with '_comp'
comp_columns <- grep("_comp$", names(comparison_data_processed), value = TRUE)

# Step 4: Loop through each '_comp' column and calculate Cohen's Kappa
retest_kappa_results <- lapply(comp_columns, function(col_name) {
  # Cohen's Kappa requires the two sets of categorical data (Test and RETEST)
  test_col <- retest_merged_data[[paste0(col_name, "_test")]]
  retest_col <- retest_merged_data[[paste0(col_name, "_retest")]]
  
  # Calculate Cohen's Kappa for the current column
  kappa <- cohen.kappa(cbind(test_col, retest_col))
  
  # Return the column name and the kappa value
  data.frame(Column = col_name, Kappa = kappa$kappa)
})

# Combine the results into a single data frame
retest_kappa_summary <- do.call(rbind, retest_kappa_results)

# Step 5: Display the results
print(retest_kappa_summary)


# die drei antweoten mit NAin kappa_summary sind eigentlich perfekt übereinstimemnd, aber da immer nur die gleiche Antwort, dh de geliche Buchstaben eingesetzt wurde gibt es per Definition eine perfekte Übereinstimmung, da es keine Variabilität gibt.


#write to disk forjamovi
write.xlsx(retest_merged_data, file = "../Data/processed/retest_merged_data.xlsx")







```


## versuch 2 mit zusätzlich z wert, agreement % , p wert
```{r}



# Step 1: Filter the dataset into test and retest subsets
test_data <- comparison_data_processed %>% filter(test_retest == "Test")
retest_data <- comparison_data_processed %>% filter(test_retest == "RETEST")

# Step 2: Match test and retest based on 'psp' (so this only contains the 30 test-retest psp rows)
retest_merged_data <- merge(test_data, retest_data, by = "psp", suffixes = c("_test", "_retest"))

# Step 3: Get all columns ending with '_comp'
comp_columns <- grep("_comp$", names(comparison_data_processed), value = TRUE)

# Step 4: Loop through each '_comp' column and calculate Cohen's Kappa using the irr package
retest_kappa_results <- lapply(comp_columns, function(col_name) {
  # Cohen's Kappa requires the two sets of categorical data (Test and RETEST)
  test_col <- retest_merged_data[[paste0(col_name, "_test")]]
  retest_col <- retest_merged_data[[paste0(col_name, "_retest")]]
  
  # Überprüfung auf gültige Daten (Nicht-NA-Daten) in beiden Spalten
  if (all(is.na(test_col)) || all(is.na(retest_col))) {
    # Falls alle Daten NA sind, gib eine Zeile mit NA zurück
    return(data.frame(Column = col_name, Kappa = NA, z_value = NA, 
                      Agreement_Percentage = NA, p_value = NA))
  }
  
  # Berechnung von Cohen's Kappa mit dem irr-Paket
  kappa_result <- kappa2(data.frame(test_col, retest_col), "unweighted")
  
  # Extrahiere den Kappa-Wert und p-Wert
  kappa_value <- kappa_result$value
  p_value <- kappa_result$p.value
  agreement_percentage <- mean(test_col == retest_col, na.rm = TRUE) * 100
  
  # Berechnung der erwarteten Übereinstimmung (p_e)
  test_table <- table(test_col, retest_col)
  n <- sum(test_table)  # Anzahl der Paare
  p_e <- sum(rowSums(test_table) * colSums(test_table)) / (n^2)
  
  # Berechnung der beobachteten Übereinstimmung (p_0)
  p_0 <- agreement_percentage / 100
  
  # Berechnung des Standardfehlers (SE)
  if (p_e == 1) {
    se_kappa <- NA  # Vermeide Division durch 0, wenn p_e = 1 (perfekte Übereinstimmung)
    z_value <- NA
  } else {
    se_kappa <- sqrt((p_0 * (1 - p_0)) / (n * (1 - p_e)^2))
  
    # Berechnung des z-Werts
    z_value <- kappa_value / se_kappa
  }
  
  # Return the column name, Kappa, z-value, agreement %, and p-value
  return(data.frame(Column = col_name, Kappa = kappa_value, z_value = z_value, 
                    Agreement_Percentage = agreement_percentage, p_value = p_value))
})

# Step 5: Combine the results into a single data frame
retest_kappa_summary <- do.call(rbind, retest_kappa_results)

# Step 6: Display the results
print(retest_kappa_summary)




```


## test, retest korrelation gesamt kappa...

```{r}

# Schritt 4: Erstelle zwei Vektoren mit allen Test- und Retest-Werten aus den "_comp"-Spalten
all_test_values <- unlist(retest_merged_data[paste0(comp_columns, "_test")])
all_retest_values <- unlist(retest_merged_data[paste0(comp_columns, "_retest")])

# Schritt 5: Berechne Cohen's Kappa für alle Daten zusammen
retest_kappa_all <- cohen.kappa(cbind(all_test_values, all_retest_values))

# Schritt 6: Zeige das Gesamtergebnis an
retest_kappa_all

```
weighted macht heir absolut keinen sinn...

##test, retest korrelation gesamt kappa...v2 mit p wert etc..

p wert noch leer:/

```{r}
# Step 1: Filter the dataset into test and retest subsets
test_data <- comparison_data_processed %>% filter(test_retest == "Test")
retest_data <- comparison_data_processed %>% filter(test_retest == "RETEST")

# Step 2: Match test and retest based on 'psp' (so this only contains the 30 test-retest psp rows)
retest_merged_data <- merge(test_data, retest_data, by = "psp", suffixes = c("_test", "_retest"))

# Step 3: Get all columns ending with '_comp'
comp_columns <- grep("_comp$", names(comparison_data_processed), value = TRUE)

# Schritt 4: Erstelle zwei Vektoren mit allen Test- und Retest-Werten aus den "_comp"-Spalten
all_test_values <- unlist(retest_merged_data[paste0(comp_columns, "_test")])
all_retest_values <- unlist(retest_merged_data[paste0(comp_columns, "_retest")])

# Schritt 5: Berechne Cohen's Kappa für alle Daten zusammen mit dem irr-Paket
kappa_result_all <- kappa2(data.frame(all_test_values, all_retest_values), "unweighted")

# Extrahiere den Kappa-Wert und den p-Wert
kappa_value <- kappa_result_all$value
p_value <- kappa_result_all$p.value
agreement_percentage <- mean(all_test_values == all_retest_values, na.rm = TRUE) * 100

# Berechnung der erwarteten Übereinstimmung (p_e)
test_table <- table(all_test_values, all_retest_values)
n <- sum(test_table)  # Anzahl der Paare
p_e <- sum(rowSums(test_table) * colSums(test_table)) / (n^2)

# Berechnung der beobachteten Übereinstimmung (p_0)
p_0 <- agreement_percentage / 100

# Berechnung des Standardfehlers (SE)
if (p_e == 1) {
  se_kappa <- NA  # Vermeide Division durch 0, wenn p_e = 1 (perfekte Übereinstimmung)
  z_value <- NA
} else {
  se_kappa <- sqrt((p_0 * (1 - p_0)) / (n * (1 - p_e)^2))
  
  # Berechnung des z-Werts
  z_value <- kappa_value / se_kappa
}

# Schritt 6: Zeige das Gesamtergebnis an und füge `n` als zusätzliche Information ein
retest_kappa_all_summary <- data.frame(
  Kappa = kappa_value,
  Agreement_Percentage = agreement_percentage,
  z_value = z_value,
  p_value = p_value,
  n = n  # Anzahl der Beobachtungen
)

# Zeige das Gesamtergebnis an
print(retest_kappa_all_summary)

```
naja z wert und p wert muss ich noch anschauen...




##  test-retest Überblick kappas

```{r}
# Schritt 1: Rundung der Kappa-Werte auf 2 Nachkommastellen
retest_kappa_summary <- retest_kappa_summary %>%
  mutate(Kappa = round(Kappa, 2))

# Schritt 2: Aufteilen der Tabelle in Kategorien basierend auf Präfixen (pre, c_pre, pap, c_pap, match)
pre_kappa <- retest_kappa_summary %>% filter(grepl("^(pre|c_pre)", Column)) %>% mutate(Source = "pre")
pap_kappa <- retest_kappa_summary %>% filter(grepl("^(pap|c_pap)", Column)) %>% mutate(Source = "pap")
match_kappa <- retest_kappa_summary %>% filter(grepl("^match", Column)) %>% mutate(Source = "match")

# Schritt 3: Setze Zeilennummern für eine saubere Darstellung in jeder Tabelle
pre_kappa <- pre_kappa %>% mutate(Row = row_number())
pap_kappa <- pap_kappa %>% mutate(Row = row_number())
match_kappa <- match_kappa %>% mutate(Row = row_number())

# Schritt 4: Zusammenfügen der drei Tabellen nebeneinander basierend auf der Zeilennummer
# Verwende full_join, um die Tabellen nach den Zeilen zu verbinden
retest_kappa_summary_overview <- pre_kappa %>%
  full_join(pap_kappa, by = "Row", suffix = c("_pre", "_pap")) %>%
  full_join(match_kappa, by = "Row", suffix = c("", "_match"))

# Schritt 5: Auswahl und Umbenennung der Spalten für Übersichtlichkeit
# Stellen Sie sicher, dass die Spalten korrekt benannt und verfügbar sind
retest_kappa_summary_overview <- retest_kappa_summary_overview %>%
  select(Row, 
         Column_pre = Column_pre, Kappa_pre = Kappa_pre, 
         Column_pap = Column_pap, Kappa_pap = Kappa_pap, 
         Column_match = Column, Kappa_match = Kappa) 








```









## test retest färbung überblick

```{r}

# Färbung der Kappa-Werte direkt in der Tabelle
formattable(retest_kappa_summary_overview, list(
  Kappa_pre = color_tile("red", "green"),  # Kappa_pre-Spalte von dunkelrot bis dunkelgrün
  Kappa_pap = color_tile("red", "green"),  # Kappa_pap-Spalte von dunkelrot bis dunkelgrün
  Kappa_match = color_tile("red", "green") # Kappa_match-Spalte von dunkelrot bis dunkelgrün
))


```





#Kappa interrater gpt zu olmo


- wieso anderes kappa als? bei jamovi?
- sollte ich im gesamten script die selbe mehto wählen?
- wie ist die differenz zu interpretieren?

## Kappa interrater gpt zu olmo per category

```{r}



# Function to calculate Cohen's Kappa, p-value, z-value, and percentage agreement
calculate_kappa_stats <- function(data, column_gpt, column_olmo) {
  # Ensure both columns exist
  if(column_gpt %in% colnames(data) & column_olmo %in% colnames(data)) {
    # Calculate Cohen's Kappa
    kappa_result <- psych::cohen.kappa(cbind(data[[column_gpt]], data[[column_olmo]]))
    
    # Extract values
    kappa_value <- kappa_result$kappa
    p_value <- kappa_result$p.value
    z_value <- kappa_result$z
    percent_agreement <- sum(data[[column_gpt]] == data[[column_olmo]], na.rm = TRUE) / nrow(data) * 100
    
    return(c(kappa_value, p_value, z_value, percent_agreement))
  } else {
    return(rep(NA, 4))
  }
}

# Get all columns that have _gpt or _olmo suffix
gpt_columns <- grep("_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns <- gsub("_gpt$", "_olmo", gpt_columns)

# Create a dataframe to store the results
kappa_results <- data.frame(Variable = gpt_columns, Kappa = NA, P_Value = NA, Z_Value = NA, Percent_Agreement = NA)

# Loop over all _gpt and _olmo column pairs and calculate statistics
for(i in 1:length(gpt_columns)) {
  kappa_stats <- calculate_kappa_stats(data_gpt_olmo_processed, gpt_columns[i], olmo_columns[i])
  kappa_results[i, 2:5] <- kappa_stats
}

# Show the results
print(kappa_results)



```


#achtung kappa result und results!!


##Kappa interrater gpt zu olmo  per category 2. ansatz

```{r}


# Function to calculate Cohen's Kappa, p-value, z-value, and percentage agreement using 'irr' package
calculate_kappa_irr_v2 <- function(data, column_gpt, column_olmo) {
  # Ensure both columns exist
  if(column_gpt %in% colnames(data) & column_olmo %in% colnames(data)) {
    # Remove rows with NA values to avoid errors in kappa calculation
    valid_data_v2 <- data[!is.na(data[[column_gpt]]) & !is.na(data[[column_olmo]]), ]
    
    # Calculate Cohen's Kappa using the 'kappa2' function from the irr package
    kappa_result_v2 <- irr::kappa2(valid_data_v2[, c(column_gpt, column_olmo)], "unweighted")
    
    # Extract values
    kappa_value_v2 <- kappa_result_v2$value
    p_value_v2 <- kappa_result_v2$p.value
    z_value_v2 <- kappa_result_v2$statistic
    percent_agreement_v2 <- sum(valid_data_v2[[column_gpt]] == valid_data_v2[[column_olmo]]) / nrow(valid_data_v2) * 100
    
    return(c(kappa_value_v2, p_value_v2, z_value_v2, percent_agreement_v2))
  } else {
    return(rep(NA, 4))
  }
}

# Get all columns that have _gpt or _olmo suffix
gpt_columns_v2 <- grep("_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns_v2 <- gsub("_gpt$", "_olmo", gpt_columns_v2)

# Create a dataframe to store the results
kappa_results_v2 <- data.frame(Variable = gpt_columns_v2, Kappa = NA, P_Value = NA, Z_Value = NA, Percent_Agreement = NA)

# Loop over all _gpt and _olmo column pairs and calculate statistics
for(i in 1:length(gpt_columns_v2)) {
  kappa_stats_v2 <- calculate_kappa_irr_v2(data_gpt_olmo_processed, gpt_columns_v2[i], olmo_columns_v2[i])
  kappa_results_v2[i, 2:5] <- kappa_stats_v2
}

# Show the results
print(kappa_results_v2)

```


# gpt /olmo interrater nur comparison/nur extraction  prompt (gesamtdatenset)


```{r}

# Extrahiere die GPT und OLMO Spalten für beide Vergleiche (Extraction und Comparison)
gpt_columns_extract <- grep("^(c_pre|pre|c_pap|pap).*_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns_extract <- gsub("_gpt$", "_olmo", gpt_columns_extract)

gpt_columns_comp <- grep("^(c_match|match).*_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns_comp <- gsub("_gpt$", "_olmo", gpt_columns_comp)

# Überprüfen, ob die entsprechenden OLMO-Spalten existieren (für beide Gruppen)
valid_gpt_columns_extract <- gpt_columns_extract[olmo_columns_extract %in% colnames(data_gpt_olmo_processed)]
valid_olmo_columns_extract <- olmo_columns_extract[olmo_columns_extract %in% colnames(data_gpt_olmo_processed)]

valid_gpt_columns_comp <- gpt_columns_comp[olmo_columns_comp %in% colnames(data_gpt_olmo_processed)]
valid_olmo_columns_comp <- olmo_columns_comp[olmo_columns_comp %in% colnames(data_gpt_olmo_processed)]

# Sicherstellen, dass nur gültige Paare verwendet werden
if(length(valid_gpt_columns_extract) == 0) {
  stop("Keine gültigen Paare von GPT- und OLMO-Spalten für Extraction gefunden!")
}
if(length(valid_gpt_columns_comp) == 0) {
  stop("Keine gültigen Paare von GPT- und OLMO-Spalten für Comparison gefunden!")
}

# Daten für beide Vergleiche kombinieren und NA-Werte entfernen
combined_data_extract <- data.frame()
combined_data_comp <- data.frame()

# Kombinierte Daten für den Extraction-Vergleich
for(i in 1:length(valid_gpt_columns_extract)) {
  combined_data_extract <- rbind(combined_data_extract, data.frame(GPT = data_gpt_olmo_processed[[valid_gpt_columns_extract[i]]],
                                                                   OLMO = data_gpt_olmo_processed[[valid_olmo_columns_extract[i]]]))
}
combined_data_extract <- na.omit(combined_data_extract)

# Kombinierte Daten für den Comparison-Vergleich
for(i in 1:length(valid_gpt_columns_comp)) {
  combined_data_comp <- rbind(combined_data_comp, data.frame(GPT = data_gpt_olmo_processed[[valid_gpt_columns_comp[i]]],
                                                             OLMO = data_gpt_olmo_processed[[valid_olmo_columns_comp[i]]]))
}
combined_data_comp <- na.omit(combined_data_comp)

# Berechne Cohen's Kappa für Extraction
kappa_result_extract <- irr::kappa2(combined_data_extract, "unweighted")
percent_agreement_extract <- sum(combined_data_extract$GPT == combined_data_extract$OLMO) / nrow(combined_data_extract) * 100

# Berechne Cohen's Kappa für Comparison
kappa_result_comp <- irr::kappa2(combined_data_comp, "unweighted")
percent_agreement_comp <- sum(combined_data_comp$GPT == combined_data_comp$OLMO) / nrow(combined_data_comp) * 100

# Ergebnisse anzeigen
cat("Cohen's Kappa für Extraction (gesamt_kappa_extraction):\n")
cat("Kappa:", kappa_result_extract$value, "\n")
cat("p-Wert:", kappa_result_extract$p.value, "\n")
cat("z-Wert:", kappa_result_extract$statistic, "\n")
cat("Prozentuale Übereinstimmung:", percent_agreement_extract, "%\n")

cat("\nCohen's Kappa für Comparison (gesamt_kappa_comparison):\n")
cat("Kappa:", kappa_result_comp$value, "\n")
cat("p-Wert:", kappa_result_comp$p.value, "\n")
cat("z-Wert:", kappa_result_comp$statistic, "\n")
cat("Prozentuale Übereinstimmung:", percent_agreement_comp, "%\n")


```


## Kappa interrater gpt zu olmo gesamt

```{r}

# Spalten mit _gpt und _olmo suffix finden
gpt_columns <- grep("_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns <- gsub("_gpt$", "_olmo", gpt_columns)

# Überprüfen, ob die entsprechenden _olmo-Spalten existieren
valid_gpt_columns <- gpt_columns[olmo_columns %in% colnames(data_gpt_olmo_processed)]
valid_olmo_columns <- olmo_columns[olmo_columns %in% colnames(data_gpt_olmo_processed)]

# Sicherstellen, dass nur gültige Paare verwendet werden
if(length(valid_gpt_columns) == 0) {
  stop("Keine gültigen Paare von GPT- und OLMO-Spalten gefunden!")
}

# Kombinieren der Spalten in einem DataFrame für den Gesamtvergleich
combined_data <- data.frame()

for(i in 1:length(valid_gpt_columns)) {
  # Füge die Werte der GPT und OLMO Spalten zur kombinierten Datensatz hinzu
  combined_data <- rbind(combined_data, data.frame(GPT = data_gpt_olmo_processed[[valid_gpt_columns[i]]],
                                                   OLMO = data_gpt_olmo_processed[[valid_olmo_columns[i]]]))
}

# Entferne NA-Werte
combined_data <- na.omit(combined_data)

# Berechne Cohen's Kappa für den gesamten Test
kappa_result <- irr::kappa2(combined_data, "unweighted")

# Berechne den Prozentsatz der Übereinstimmung
percent_agreement <- sum(combined_data$GPT == combined_data$OLMO) / nrow(combined_data) * 100

# Ergebnisse anzeigen
cat("Cohen's Kappa für den gesamten Test:\n")
cat("Kappa:", kappa_result$value, "\n")
cat("p-Wert:", kappa_result$p.value, "\n")
cat("z-Wert:", kappa_result$statistic, "\n")
cat("Prozentuale Übereinstimmung:", percent_agreement, "%\n")


```




## besser nur bei promt version d? im Vergleich zu den anderen?

hier noch n reinnehmen

```{r}
# Spalten mit _gpt und _olmo suffix finden
gpt_columns_PromptV <- grep("_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns_PromptV <- gsub("_gpt$", "_olmo", gpt_columns_PromptV)

# Überprüfen, ob die entsprechenden _olmo-Spalten existieren
valid_gpt_columns_PromptV <- gpt_columns_PromptV[olmo_columns_PromptV %in% colnames(data_gpt_olmo_processed)]
valid_olmo_columns_PromptV <- olmo_columns_PromptV[olmo_columns_PromptV %in% colnames(data_gpt_olmo_processed)]

# Sicherstellen, dass nur gültige Paare verwendet werden
if(length(valid_gpt_columns_PromptV) == 0) {
  stop("Keine gültigen Paare von GPT- und OLMO-Spalten gefunden!")
}

# Definiere die verschiedenen Versionen, die in prompt_version auftreten (A, B, C, D)
versions_PromptV <- c("A", "B", "C", "D")

# Schleife durch jede Version und berechne das Kappa für jede
for (version_PromptV in versions_PromptV) {
  # Filtere die Zeilen, die zur aktuellen Version gehören
  version_data_PromptV <- data_gpt_olmo_processed[data_gpt_olmo_processed$prompt_version == version_PromptV, ]
  
  # Kombinieren der Spalten in einem DataFrame für den Vergleich der aktuellen Version
  combined_data_PromptV <- data.frame()
  
  for(i in 1:length(valid_gpt_columns_PromptV)) {
    # Füge die Werte der GPT und OLMO Spalten zur kombinierten Datensatz hinzu
    combined_data_PromptV <- rbind(combined_data_PromptV, data.frame(GPT = version_data_PromptV[[valid_gpt_columns_PromptV[i]]],
                                                                     OLMO = version_data_PromptV[[valid_olmo_columns_PromptV[i]]]))
  }
  
  # Entferne NA-Werte
  combined_data_PromptV <- na.omit(combined_data_PromptV)
  
  # Berechne Cohen's Kappa für die aktuelle Version
  kappa_result_PromptV <- irr::kappa2(combined_data_PromptV, "unweighted")
  
  # Berechne den Prozentsatz der Übereinstimmung
  percent_agreement_PromptV <- sum(combined_data_PromptV$GPT == combined_data_PromptV$OLMO) / nrow(combined_data_PromptV) * 100
  
  # Ergebnisse anzeigen
  cat("\nCohen's Kappa für Version", version_PromptV, ":\n")
  cat("Kappa:", kappa_result_PromptV$value, "\n")
  cat("p-Wert:", kappa_result_PromptV$p.value, "\n")
  cat("z-Wert:", kappa_result_PromptV$statistic, "\n")
  cat("Prozentuale Übereinstimmung:", percent_agreement_PromptV, "%\n")
}



```


## differenz zw promt versionen? 






# Kappa nur y & n 

## kappa y_n all categories

```{r}
# Function to calculate Cohen's Kappa, p-value, z-value, percentage agreement, and number of comparisons using 'irr' package
calculate_kappa_irr_filtered_debug <- function(data, column_gpt, column_olmo) {
  # Ensure both columns exist
  if (column_gpt %in% colnames(data) & column_olmo %in% colnames(data)) {
    # Filter rows where the OLMO column is either "Y" or "N"
    valid_data <- data[data[[column_olmo]] %in% c("y", "n"), ]
    
    # Debug: Print number of valid rows for the comparison
    print(paste("Number of valid rows for comparison (", column_gpt, " & ", column_olmo, "): ", nrow(valid_data)))
    
    # Check if there are any valid rows left for comparison
    if (nrow(valid_data) > 0) {
      # Remove rows with NA values in the selected columns
      valid_data <- valid_data[!is.na(valid_data[[column_gpt]]) & !is.na(valid_data[[column_olmo]]), ]
      
      # Debug: Print number of rows after removing NAs
      print(paste("Number of valid rows after removing NAs: ", nrow(valid_data)))
      
      # Check again if there are rows left after removing NAs
      if (nrow(valid_data) > 0) {
        # Calculate Cohen's Kappa using the 'kappa2' function from the irr package
        kappa_result <- irr::kappa2(valid_data[, c(column_gpt, column_olmo)], "unweighted")
        
        # Extract values
        kappa_value <- kappa_result$value
        p_value <- kappa_result$p.value
        z_value <- kappa_result$statistic
        percent_agreement <- sum(valid_data[[column_gpt]] == valid_data[[column_olmo]]) / nrow(valid_data) * 100
        
        # Return the results including the number of comparisons (N)
        return(c(kappa_value, p_value, z_value, percent_agreement, nrow(valid_data)))
      } else {
        # No valid comparisons available after NA removal
        print("No valid rows left after removing NAs.")
        return(rep(NA, 5))
      }
    } else {
      # No valid comparisons based on OLMO filter
      print("No valid rows based on OLMO filter (Y/N).")
      return(rep(NA, 5))
    }
  } else {
    print(paste("Columns", column_gpt, "or", column_olmo, "do not exist in the dataset."))
    return(rep(NA, 5))
  }
}

# Get all columns that have _gpt or _olmo suffix
gpt_columns <- grep("_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns <- gsub("_gpt$", "_olmo", gpt_columns)

# Create a dataframe to store the results
kappa_results_filtered_debug <- data.frame(
  Variable = gpt_columns,
  Kappa = NA,
  P_Value = NA,
  Z_Value = NA,
  Percent_Agreement = NA,
  N_Comparisons = NA
)

# Loop over all _gpt and _olmo column pairs and calculate statistics
for (i in 1:length(gpt_columns)) {
  print(paste("Processing columns:", gpt_columns[i], "and", olmo_columns[i]))
  kappa_stats_filtered_debug <- calculate_kappa_irr_filtered_debug(data_gpt_olmo_processed, gpt_columns[i], olmo_columns[i])
  kappa_results_filtered_debug[i, 2:6] <- kappa_stats_filtered_debug
}

# Show the results
print(kappa_results_filtered_debug)



```



## kappa y & n total

```{r}


# Function to calculate Cohen's Kappa, p-value, z-value, percentage agreement, and number of comparisons over all columns
calculate_overall_kappa <- function(data, gpt_columns, olmo_columns) {
  # Create an empty dataframe to store all relevant rows for comparison
  combined_data <- data.frame(GPT = character(), OLMO = character(), stringsAsFactors = FALSE)
  
  # Loop over each pair of GPT and OLMO columns to collect valid cells
  for (i in 1:length(gpt_columns)) {
    if (gpt_columns[i] %in% colnames(data) & olmo_columns[i] %in% colnames(data)) {
      # Filter only rows where the OLMO value is "Y" or "N"
      valid_rows <- data[data[[olmo_columns[i]]] %in% c("y", "n"), ]
      
      # Select the relevant GPT and OLMO values and remove NA values
      valid_rows <- valid_rows[!is.na(valid_rows[[gpt_columns[i]]]) & !is.na(valid_rows[[olmo_columns[i]]]), ]
      
      # Combine the valid GPT and OLMO values into the combined dataframe
      combined_data <- rbind(combined_data, data.frame(GPT = valid_rows[[gpt_columns[i]]], OLMO = valid_rows[[olmo_columns[i]]]))
    }
  }
  
  # Check if there are enough data points to calculate Kappa
  if (nrow(combined_data) > 0) {
    # Calculate Cohen's Kappa over all the combined cells
    kappa_result <- irr::kappa2(combined_data, "unweighted")
    
    # Extract values
    kappa_value <- kappa_result$value
    p_value <- kappa_result$p.value
    z_value <- kappa_result$statistic
    percent_agreement <- sum(combined_data$GPT == combined_data$OLMO) / nrow(combined_data) * 100
    
    # Return the results including the number of comparisons (N)
    return(data.frame(Kappa_Gesamt = kappa_value, P_Value = p_value, Z_Value = z_value, Percent_Agreement = percent_agreement, N_Comparisons = nrow(combined_data)))
  } else {
    # Return NA if no valid comparisons are available
    return(data.frame(Kappa_Gesamt = NA, P_Value = NA, Z_Value = NA, Percent_Agreement = NA, N_Comparisons = 0))
  }
}

# Get all columns that have _gpt or _olmo suffix
gpt_columns <- grep("_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns <- gsub("_gpt$", "_olmo", gpt_columns)

# Calculate the overall Kappa over all relevant cells
kappa_gesamt_yn <- calculate_overall_kappa(data_gpt_olmo_processed, gpt_columns, olmo_columns)

# Show the overall result
print(kappa_gesamt_yn)



```
# i thought it would be better, but its actually worse...

hmm ich ersuhe jetz noch einen kürzeren promt mit ausgewählten fragen... dann muss ich nur diese ausgewählte fragen noch testen beim ursprünglichen datensatz und dann kann ich diesn vergleichen mit dem neuen gekürzeten...

und eventuell noch die sensitivity etc. nur für y/n zählen lassen!!! 

ausserdem sollte ich wohl noch die ratings auch auswerten(alos die nummern obwohldiese ziemlich scheisse isind


und noch schauen ob es einen unterschied macht ob experiment oder nicht....)


## ich könnte noch kappa pro psp machen.... dh..

```{r}



```

# new!
## kappa new per category
```{r}


# Definiere die zu vergleichenden Variablenpaare
variables_to_compare_ve <- c("match_dv_0", "match_dv_1", "match_dv_2", "match_dv_3",
                             "match_dcp_1", "match_dcp_2", "match_sm_1", "match_sm_2", "match_sm_3")

# Liste zur Speicherung der Ergebnisse
results_ve <- data.frame(Variable_ve = character(), 
                         Kappa_ve = numeric(), 
                         N_ve = numeric(),
                         Agreement_Percent_ve = numeric(),
                         Z_Value_ve = numeric(),
                         P_Value_ve = numeric(), 
                         stringsAsFactors = FALSE)

# Berechnung von Cohen's Kappa und weiteren Kennzahlen für jede Variable
for (variable in variables_to_compare_ve) {
  # Definiere die Spaltennamen für GPT und Olmo
  gpt_column <- paste0(variable, "_gpt")
  olmo_column <- paste0(variable, "_olmo")
  
  # Selektiere die relevanten Spalten und entferne Zeilen mit NA-Werten nur in diesen Spalten
  comparison_data <- data_gpt_ve_filtered %>%
    select(gpt_column, olmo_column) %>%
    filter(!is.na(.[[gpt_column]]) & !is.na(.[[olmo_column]]))
  
  # Berechne Cohen's Kappa für die aktuelle Variable
  kappa_obj_ve <- kappa2(comparison_data)
  kappa_value_ve <- kappa_obj_ve$value
  
  # Anzahl der Vergleiche (N)
  n_comparisons_ve <- nrow(comparison_data)
  
  # Berechne den prozentualen Anteil der Übereinstimmungen
  agreement_percent_ve <- mean(comparison_data[[gpt_column]] == comparison_data[[olmo_column]]) * 100

  # Berechne Z-Wert und p-Wert (Standardmäßig aus dem Kappa-Objekt)
  z_value_ve <- kappa_obj_ve$statistic # Z-Wert aus dem Kappa-Objekt
  p_value_ve <- kappa_obj_ve$p.value   # p-Wert aus dem Kappa-Objekt
  
  # Ergebnisse speichern mit den Endungen _ve
  results_ve <- rbind(results_ve, data.frame(Variable_ve = variable, 
                                             Kappa_ve = kappa_value_ve, 
                                             N_ve = n_comparisons_ve,
                                             Agreement_Percent_ve = agreement_percent_ve,
                                             Z_Value_ve = z_value_ve,
                                             P_Value_ve = p_value_ve))
}

# Ausgabe der Ergebnisse als DataFrame
print(results_ve)

```
p sind nicht signifikant?!

## kappa ve totat
```{r}


# Daten filtern: Nur Zeilen behalten, die nicht "exclude" in master_exclude haben
data_gpt_ve_total_filtered <- data_gpt_ve_processed %>%
  filter(master_exclude != "exclude")

# Definiere die zu vergleichenden Variablenpaare
variables_to_compare_ve_total <- c("match_dv_0", "match_dv_1", "match_dv_2", "match_dv_3",
                                   "match_dcp_1", "match_dcp_2", "match_sm_1", "match_sm_2", "match_sm_3")

# Liste zur Speicherung der kombinierten Ratings
combined_data_ve_total <- data.frame(GPT_Rating_ve_total = character(), 
                                     Olmo_Rating_ve_total = character(), 
                                     stringsAsFactors = FALSE)

# Kombinieren der Daten aus allen relevanten Variablenpaaren
for (variable in variables_to_compare_ve_total) {
  # Definiere die Spaltennamen für GPT und Olmo
  gpt_column <- paste0(variable, "_gpt")
  olmo_column <- paste0(variable, "_olmo")
  
  # Selektiere die relevanten Spalten und entferne Zeilen mit NA-Werten nur in diesen Spalten
  comparison_data <- data_gpt_ve_total_filtered %>%
    select(gpt_column, olmo_column) %>%
    filter(!is.na(.[[gpt_column]]) & !is.na(.[[olmo_column]])) %>%
    rename(GPT_Rating_ve_total = gpt_column, Olmo_Rating_ve_total = olmo_column)
  
  # Füge die gefilterten Daten zum kombinierten Datensatz hinzu
  combined_data_ve_total <- rbind(combined_data_ve_total, comparison_data)
}

# Berechnung des Gesamt-Kappa über alle kombinierten Kategorien hinweg
kappa_obj_ve_total <- kappa2(combined_data_ve_total)
kappa_value_ve_total <- kappa_obj_ve_total$value

# Berechnung zusätzlicher Kennzahlen
n_comparisons_ve_total <- nrow(combined_data_ve_total)  # Anzahl der Vergleiche (N)
agreement_percent_ve_total <- mean(combined_data_ve_total$GPT_Rating_ve_total == combined_data_ve_total$Olmo_Rating_ve_total) * 100
z_value_ve_total <- kappa_obj_ve_total$statistic  # Z-Wert aus dem Kappa-Objekt
p_value_ve_total <- kappa_obj_ve_total$p.value    # p-Wert aus dem Kappa-Objekt

# Ausgabe der Ergebnisse
results_ve_total <- data.frame(Variable_ve_total = "Overall",
                               Kappa_ve_total = kappa_value_ve_total,
                               N_ve_total = n_comparisons_ve_total,
                               Agreement_Percent_ve_total = agreement_percent_ve_total,
                               Z_Value_ve_total = z_value_ve_total,
                               P_Value_ve_total = p_value_ve_total)

print(results_ve_total)



```



## confusion matrix VE
```{r}



# Definiere die zu vergleichenden Variablenpaare
variables_to_compare_ve_total <- c("match_dv_0", "match_dv_1", "match_dv_2", "match_dv_3",
                                   "match_dcp_1", "match_dcp_2", "match_sm_1", "match_sm_2", "match_sm_3")

# Liste zur Speicherung der kombinierten Ratings
combined_data_ve_total <- data.frame(GPT_Rating_ve_total = character(), 
                                     Olmo_Rating_ve_total = character(), 
                                     stringsAsFactors = FALSE)

# Kombinieren der Daten aus allen relevanten Variablenpaaren
for (variable in variables_to_compare_ve_total) {
  # Definiere die Spaltennamen für GPT und Olmo
  gpt_column <- paste0(variable, "_gpt")
  olmo_column <- paste0(variable, "_olmo")
  
  # Selektiere die relevanten Spalten und entferne Zeilen mit NA-Werten nur in diesen Spalten
  comparison_data_ve_total <- data_gpt_ve_total_filtered %>%
    select(gpt_column, olmo_column) %>%
    filter(!is.na(.[[gpt_column]]) & !is.na(.[[olmo_column]])) %>%
    rename(GPT_Rating_ve_total = gpt_column, Olmo_Rating_ve_total = olmo_column)
  
  # Füge die gefilterten Daten zum kombinierten Datensatz hinzu
  combined_data_ve_total <- rbind(combined_data_ve_total, comparison_data_ve_total)
}

# Erstellen der Confusion Matrix mit _ve Endungen
conf_matrix_ve <- table(combined_data_ve_total$Olmo_Rating_ve_total, combined_data_ve_total$GPT_Rating_ve_total)

# Anzeige der Confusion Matrix mit _ve Endung
print("Confusion Matrix (ve):")
print(conf_matrix_ve)

# Berechnung weiterer Metriken: Sensitivität, Spezifität, etc. mit _ve Endung
confusion_summary_ve <- confusionMatrix(conf_matrix_ve)

# Ausgabe der detaillierten Ergebnisse mit _ve Endung
print("Detailed Confusion Matrix Metrics (ve):")
print(confusion_summary_ve)


```
<!-- n, n = 73: Olmo und GPT stimmen beide überein und haben n (Nein) vorhergesagt. -->
<!-- n, y = 155: Olmo hat n (Nein) angegeben, aber GPT hat y (Ja) vorhergesagt (False Positive). -->
<!-- y, n = 41: Olmo hat y (Ja) angegeben, aber GPT hat n (Nein) vorhergesagt (False Negative). -->
<!-- y, y = 199: Olmo und GPT stimmen beide überein und haben y (Ja) vorhergesagt. -->



## new frickes ü


```{r}


# Definiere die zu vergleichenden Variablenpaare
variables_to_compare_ve_total <- c("match_dv_0", "match_dv_1", "match_dv_2", "match_dv_3",
                                   "match_dcp_1", "match_dcp_2", "match_sm_1", "match_sm_2", "match_sm_3")

# Liste zur Speicherung der kombinierten Ratings
combined_data_ve_total <- data.frame(GPT_Rating_ve_total = character(), 
                                     Olmo_Rating_ve_total = character(), 
                                     stringsAsFactors = FALSE)

# Kombinieren der Daten aus allen relevanten Variablenpaaren
for (variable in variables_to_compare_ve_total) {
  # Definiere die Spaltennamen für GPT und Olmo
  gpt_column <- paste0(variable, "_gpt")
  olmo_column <- paste0(variable, "_olmo")
  
  # Selektiere die relevanten Spalten und entferne Zeilen mit NA-Werten nur in diesen Spalten
  comparison_data_ve_total <- data_gpt_ve_total_filtered %>%
    select(gpt_column, olmo_column) %>%
    filter(!is.na(.[[gpt_column]]) & !is.na(.[[olmo_column]])) %>%
    rename(GPT_Rating_ve_total = gpt_column, Olmo_Rating_ve_total = olmo_column)
  
  # Füge die gefilterten Daten zum kombinierten Datensatz hinzu
  combined_data_ve_total <- rbind(combined_data_ve_total, comparison_data_ve_total)
}

# Berechnung von Fricke's Ü-Koeffizient
calculate_fricke_ue <- function(true_values, predicted_values) {
  # Erstelle eine Kreuztabelle (Contingency Table)
  contingency_table <- table(true_values, predicted_values)
  
  # Beobachtete Übereinstimmungen (C_o)
  C_o <- sum(diag(contingency_table))
  
  # Maximale Anzahl der Übereinstimmungen (C_max)
  C_max <- sum(contingency_table)
  
  # Erwartete Übereinstimmungen unter Zufallsbedingungen (C_er)
  row_totals <- rowSums(contingency_table)
  col_totals <- colSums(contingency_table)
  C_er <- sum(row_totals * col_totals) / C_max
  
  # Berechnung von Fricke's Ü-Koeffizient
  Ue <- (C_o - C_er) / (C_max - C_er)
  return(Ue)
}

# Berechnung von Fricke's Ü-Koeffizient für die kombinierten Daten
fricke_ue_value <- calculate_fricke_ue(combined_data_ve_total$Olmo_Rating_ve_total, combined_data_ve_total$GPT_Rating_ve_total)

# Ausgabe des Ergebnisses
print(paste("Fricke's Ü-Koeffizient (ve_total):", round(fricke_ue_value, 4)))



```






##  test retest 
```{r}



# Step 1: Filter the dataset into test and retest subsets
test_data_ve <- data_gpt_ve_filtered %>% filter(test_retest == "Test")
retest_data_ve <- data_gpt_ve_filtered %>% filter(test_retest == "RETEST")

# Step 2: Match test and retest based on 'psp' (this will contain only the 30 test-retest psp rows)
retest_merged_data_ve <- merge(test_data_ve, retest_data_ve, by = "psp", suffixes = c("_test_ve", "_retest_ve"))

# Step 3: Get all columns ending with '-gpt' or '_gpt'
comp_columns_ve <- grep("[-_]gpt$", names(data_gpt_ve_filtered), value = TRUE)

# Step 4: Loop through each 'gpt' column and calculate Cohen's Kappa using the irr package
retest_kappa_results_ve <- lapply(comp_columns_ve, function(col_name) {
  # Cohen's Kappa requires the two sets of categorical data (Test and RETEST)
  test_col_ve <- retest_merged_data_ve[[paste0(col_name, "_test_ve")]]
  retest_col_ve <- retest_merged_data_ve[[paste0(col_name, "_retest_ve")]]
  
  # Überprüfung auf gültige Daten (Nicht-NA-Daten) in beiden Spalten
  if (all(is.na(test_col_ve)) || all(is.na(retest_col_ve))) {
    # Falls alle Daten NA sind, gib eine Zeile mit NA zurück
    return(data.frame(Column = col_name, Kappa = NA, z_value = NA, 
                      Agreement_Percentage = NA, p_value = NA))
  }
  
  # Berechnung von Cohen's Kappa mit dem irr-Paket
  kappa_result_ve <- kappa2(data.frame(test_col_ve, retest_col_ve), weight = "unweighted")
  
  # Extrahiere den Kappa-Wert und p-Wert
  kappa_value_ve <- kappa_result_ve$value
  p_value_ve <- kappa_result_ve$p.value
  agreement_percentage_ve <- mean(test_col_ve == retest_col_ve, na.rm = TRUE) * 100
  
  # Berechnung der erwarteten Übereinstimmung (p_e)
  test_table_ve <- table(test_col_ve, retest_col_ve)
  n_ve <- sum(test_table_ve)  # Anzahl der Paare
  p_e_ve <- sum(rowSums(test_table_ve) * colSums(test_table_ve)) / (n_ve^2)
  
  # Berechnung der beobachteten Übereinstimmung (p_0)
  p_0_ve <- agreement_percentage_ve / 100
  
  # Berechnung des Standardfehlers (SE)
  if (p_e_ve == 1) {
    se_kappa_ve <- NA  # Vermeide Division durch 0, wenn p_e = 1 (perfekte Übereinstimmung)
    z_value_ve <- NA
  } else {
    se_kappa_ve <- sqrt((p_0_ve * (1 - p_0_ve)) / (n_ve * (1 - p_e_ve)^2))
  
    # Berechnung des z-Werts
    z_value_ve <- kappa_value_ve / se_kappa_ve
  }
  
  # Return the column name, Kappa, z-value, agreement %, and p-value
  return(data.frame(Column = col_name, Kappa = kappa_value_ve, z_value = z_value_ve, 
                    Agreement_Percentage = agreement_percentage_ve, p_value = p_value_ve))
})

# Step 5: Combine the results into a single data frame
retest_kappa_summary_ve <- do.call(rbind, retest_kappa_results_ve)

# Step 6: Display the results
print(retest_kappa_summary_ve)


# Berechnung des gesamten Kappa-Werts über alle "_gpt"-Spalten

# Schritt 7: Erstelle zwei Vektoren mit allen Test- und Retest-Werten aus den "_gpt"-Spalten
all_test_values_ve <- unlist(retest_merged_data_ve[paste0(comp_columns_ve, "_test_ve")])
all_retest_values_ve <- unlist(retest_merged_data_ve[paste0(comp_columns_ve, "_retest_ve")])

# Schritt 8: Berechne Cohen's Kappa für alle Daten zusammen
retest_kappa_all_ve <- cohen.kappa(cbind(all_test_values_ve, all_retest_values_ve))

# Schritt 9: Zeige das Gesamtergebnis an
print(retest_kappa_all_ve)


```   
weighted macht heir absolut keinen sinn...




# kappa sample size...
```{r}
# Vergleichs-Spalten für c_match und match extrahieren
gpt_columns_comp <- grep("^(c_match|match).*_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns_comp <- gsub("_gpt$", "_olmo", gpt_columns_comp)

# Überprüfen, ob die entsprechenden OLMO-Spalten existieren
valid_gpt_columns_comp <- gpt_columns_comp[olmo_columns_comp %in% colnames(data_gpt_olmo_processed)]
valid_olmo_columns_comp <- olmo_columns_comp[olmo_columns_comp %in% colnames(data_gpt_olmo_processed)]

# Sicherstellen, dass nur gültige Paare verwendet werden
if(length(valid_gpt_columns_comp) == 0) {
  stop("Keine gültigen Paare von GPT- und OLMO-Spalten für Comparison gefunden!")
}

# Daten für Comparison kombinieren und NA-Werte entfernen
combined_data_comp <- data.frame()
for(i in 1:length(valid_gpt_columns_comp)) {
  combined_data_comp <- rbind(combined_data_comp, data.frame(GPT = data_gpt_olmo_processed[[valid_gpt_columns_comp[i]]],
                                                             OLMO = data_gpt_olmo_processed[[valid_olmo_columns_comp[i]]],
                                                             size_kb_sum = data_gpt_olmo_processed$size_kb_sum))
}
combined_data_comp <- na.omit(combined_data_comp)

# Berechne die Terzile basierend auf der Spalte 'size_kb_sum'
tertile_cutoffs <- quantile(combined_data_comp$size_kb_sum, probs = c(1/3, 2/3))

# Einteilung in small, medium und large Terzile
combined_data_comp$tertile <- cut(combined_data_comp$size_kb_sum, 
                                  breaks = c(-Inf, tertile_cutoffs[1], tertile_cutoffs[2], Inf), 
                                  labels = c("small", "medium", "large"))

# Funktion zum Berechnen von Cohen's Kappa für eine bestimmte Terzile
compute_kappa_for_tertile <- function(data, tertile_label) {
  # Filtere Daten für das gegebene Terzil
  data_filtered <- subset(data, tertile == tertile_label)
  
  # Berechne Cohen's Kappa
  kappa_result <- irr::kappa2(data_filtered[, c("GPT", "OLMO")], "unweighted")
  
  # Berechne den Prozentsatz der Übereinstimmung
  percent_agreement <- sum(data_filtered$GPT == data_filtered$OLMO) / nrow(data_filtered) * 100
  
  # Ergebnisse anzeigen
  cat("Cohen's Kappa für", tertile_label, ":\n")
  cat("Kappa:", kappa_result$value, "\n")
  cat("p-Wert:", kappa_result$p.value, "\n")
  cat("z-Wert:", kappa_result$statistic, "\n")
  cat("Prozentuale Übereinstimmung:", percent_agreement, "%\n\n")
}

# Berechne Cohen's Kappa für small, medium und large
compute_kappa_for_tertile(combined_data_comp, "small")
compute_kappa_for_tertile(combined_data_comp, "medium")
compute_kappa_for_tertile(combined_data_comp, "large")



```



# kappa sample size nur exgtraction prereg
```{r}

# Extrahiere die GPT und OLMO Spalten für c_pre und pre
gpt_columns_extract_prereg <- grep("^(c_pre|pre).*_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns_extract_prereg <- gsub("_gpt$", "_olmo", gpt_columns_extract_prereg)

# Überprüfen, ob die entsprechenden OLMO-Spalten existieren
valid_gpt_columns_extract_prereg <- gpt_columns_extract_prereg[olmo_columns_extract_prereg %in% colnames(data_gpt_olmo_processed)]
valid_olmo_columns_extract_prereg <- olmo_columns_extract_prereg[olmo_columns_extract_prereg %in% colnames(data_gpt_olmo_processed)]

# Sicherstellen, dass nur gültige Paare verwendet werden
if(length(valid_gpt_columns_extract_prereg) == 0) {
  stop("Keine gültigen Paare von GPT- und OLMO-Spalten für extract_prereg gefunden!")
}

# Daten für den Vergleich kombinieren und NA-Werte entfernen
combined_data_extract_prereg <- data.frame()
for(i in 1:length(valid_gpt_columns_extract_prereg)) {
  combined_data_extract_prereg <- rbind(combined_data_extract_prereg, data.frame(GPT = data_gpt_olmo_processed[[valid_gpt_columns_extract_prereg[i]]],
                                                                                OLMO = data_gpt_olmo_processed[[valid_olmo_columns_extract_prereg[i]]],
                                                                                size_kb_prereg = data_gpt_olmo_processed$size_kb_prereg))
}
combined_data_extract_prereg <- na.omit(combined_data_extract_prereg)

# Berechne die Terzile basierend auf der Spalte 'size_kb_prereg'
tertile_cutoffs_prereg <- quantile(combined_data_extract_prereg$size_kb_prereg, probs = c(1/3, 2/3))

# Einteilung in small, medium und large Terzile
combined_data_extract_prereg$tertile_prereg <- cut(combined_data_extract_prereg$size_kb_prereg, 
                                                  breaks = c(-Inf, tertile_cutoffs_prereg[1], tertile_cutoffs_prereg[2], Inf), 
                                                  labels = c("small", "medium", "large"))

# Funktion zum Berechnen von Cohen's Kappa für eine bestimmte Terzile
compute_kappa_for_tertile_prereg <- function(data, tertile_label) {
  # Filtere Daten für das gegebene Terzil
  data_filtered <- subset(data, tertile_prereg == tertile_label)
  
  # Berechne Cohen's Kappa
  kappa_result <- irr::kappa2(data_filtered[, c("GPT", "OLMO")], "unweighted")
  
  # Berechne den Prozentsatz der Übereinstimmung
  percent_agreement <- sum(data_filtered$GPT == data_filtered$OLMO) / nrow(data_filtered) * 100
  
  # Ergebnisse anzeigen
  cat("Cohen's Kappa für", tertile_label, "in extract_prereg:\n")
  cat("Kappa:", kappa_result$value, "\n")
  cat("p-Wert:", kappa_result$p.value, "\n")
  cat("z-Wert:", kappa_result$statistic, "\n")
  cat("Prozentuale Übereinstimmung:", percent_agreement, "%\n\n")
}

# Berechne Cohen's Kappa für small, medium und large in extract_prereg
compute_kappa_for_tertile_prereg(combined_data_extract_prereg, "small")
compute_kappa_for_tertile_prereg(combined_data_extract_prereg, "medium")
compute_kappa_for_tertile_prereg(combined_data_extract_prereg, "large")


```


# kappa sample size nur extraction pubstud
```{r}
# Extrahiere die GPT und OLMO Spalten für c_pap und pap
gpt_columns_extract_pubstud <- grep("^(c_pap|pap).*_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns_extract_pubstud <- gsub("_gpt$", "_olmo", gpt_columns_extract_pubstud)

# Überprüfen, ob die entsprechenden OLMO-Spalten existieren
valid_gpt_columns_extract_pubstud <- gpt_columns_extract_pubstud[olmo_columns_extract_pubstud %in% colnames(data_gpt_olmo_processed)]
valid_olmo_columns_extract_pubstud <- olmo_columns_extract_pubstud[olmo_columns_extract_pubstud %in% colnames(data_gpt_olmo_processed)]

# Sicherstellen, dass nur gültige Paare verwendet werden
if(length(valid_gpt_columns_extract_pubstud) == 0) {
  stop("Keine gültigen Paare von GPT- und OLMO-Spalten für extract_pubstud gefunden!")
}

# Daten für den Vergleich kombinieren und NA-Werte entfernen
combined_data_extract_pubstud <- data.frame()
for(i in 1:length(valid_gpt_columns_extract_pubstud)) {
  combined_data_extract_pubstud <- rbind(combined_data_extract_pubstud, data.frame(GPT = data_gpt_olmo_processed[[valid_gpt_columns_extract_pubstud[i]]],
                                                                                  OLMO = data_gpt_olmo_processed[[valid_olmo_columns_extract_pubstud[i]]],
                                                                                  size_kb_pubstud = data_gpt_olmo_processed$size_kb_pubstud))
}
combined_data_extract_pubstud <- na.omit(combined_data_extract_pubstud)

# Berechne die Terzile basierend auf der Spalte 'size_kb_pubstud'
tertile_cutoffs_pubstud <- quantile(combined_data_extract_pubstud$size_kb_pubstud, probs = c(1/3, 2/3))

# Einteilung in small, medium und large Terzile
combined_data_extract_pubstud$tertile_pubstud <- cut(combined_data_extract_pubstud$size_kb_pubstud, 
                                                    breaks = c(-Inf, tertile_cutoffs_pubstud[1], tertile_cutoffs_pubstud[2], Inf), 
                                                    labels = c("small", "medium", "large"))

# Funktion zum Berechnen von Cohen's Kappa für eine bestimmte Terzile
compute_kappa_for_tertile_pubstud <- function(data, tertile_label) {
  # Filtere Daten für das gegebene Terzil
  data_filtered <- subset(data, tertile_pubstud == tertile_label)
  
  # Berechne Cohen's Kappa
  kappa_result <- irr::kappa2(data_filtered[, c("GPT", "OLMO")], "unweighted")
  
  # Berechne den Prozentsatz der Übereinstimmung
  percent_agreement <- sum(data_filtered$GPT == data_filtered$OLMO) / nrow(data_filtered) * 100
  
  # Ergebnisse anzeigen
  cat("Cohen's Kappa für", tertile_label, "in extract_pubstud:\n")
  cat("Kappa:", kappa_result$value, "\n")
  cat("p-Wert:", kappa_result$p.value, "\n")
  cat("z-Wert:", kappa_result$statistic, "\n")
  cat("Prozentuale Übereinstimmung:", percent_agreement, "%\n\n")
}

# Berechne Cohen's Kappa für small, medium und large in extract_pubstud
compute_kappa_for_tertile_pubstud(combined_data_extract_pubstud, "small")
compute_kappa_for_tertile_pubstud(combined_data_extract_pubstud, "medium")
compute_kappa_for_tertile_pubstud(combined_data_extract_pubstud, "large")



```



# 
```{r}



```




## 
```{r}



```

## 
```{r}



```




# Session info
```{r}

sessionInfo()

```


