---
title: "Analysis MA results"
author: "Luki"
date: "2024-09-19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#todo:
- evtl processing noch einmal überarbeiten, für 2. datensatz ohne comp.
- herausfinden welche Werte man angeben MUSS (pwert, xwert , % etc..)
- kappa auf Basis von diesem ausrechnen für übereinstimmung gpt & olmo
- Assumption checks herausfinden welche und ausrechnen
- weitere analyse....

- Analysis File beschönigen.

- das ziel ist ja: Synthetic peer review: Using large language models to automatically detect deviations from preregistrations

dh. insbesondere die "n" sind interessant um eine Aussage darüber zu machen wie viel % davon entdeckt werden und viel viel fals positive etc....und es wäre auch noch nice zu wissen welche Kategorien man brauchen kann und welche nicht...

# Dependencies

```{r}
library(tidyverse)
library("readxl")
library(janitor)
library(stringr)
library(openxlsx)
library(dplyr)
library(irr)
library(psych)
library(formattable)
library(kableExtra)
library(gt)
library(irr)





```


#Read  data


```{r}
getwd() 


comparison_data_processed <-read_excel("../Data/processed/comparison_data_processed.xlsx")


data_gpt_olmo_processed <-read_excel("../Data/processed/data_gpt_olmo_processed.xlsx")
```
#absolut numbers
## category counts total

hier noch die absoluten zahlen ohne.oo hinschreiben...

```{r}

# Step 1: Select the columns that end with "_comp"
comp_columns <- comparison_data_processed %>%
  select(ends_with("_comp"))

# Step 2: Create a new dataset that counts the occurrences of each category across all "_comp" columns
data_category_counts_total <- comp_columns %>%
  summarise(
    Identical_YES = sum(comp_columns == "Y", na.rm = TRUE),
    Identical_NO = sum(comp_columns == "N", na.rm = TRUE),
    Identical_X = sum(comp_columns == "X", na.rm = TRUE),
    Different_Y_to_N = sum(comp_columns == "P", na.rm = TRUE),
    Different_N_to_Y = sum(comp_columns == "Q", na.rm = TRUE),
    GPT_Y_Olmo_X = sum(comp_columns == "FY", na.rm = TRUE),
    GPT_N_Olmo_X = sum(comp_columns == "FN", na.rm = TRUE),
    GPT_X_Olmo_N = sum(comp_columns == "FXN", na.rm = TRUE),
    GPT_X_Olmo_Y = sum(comp_columns == "FXY", na.rm = TRUE),
    n_answers_total = sum(
      comp_columns == "Y" | comp_columns == "N" | comp_columns == "X" |
      comp_columns == "P" | comp_columns == "Q" | comp_columns == "FY" |
      comp_columns == "FN" | comp_columns == "FXN" | comp_columns == "FXY", na.rm = TRUE
    )
  )

# Step 3: Add percentages as a second row
data_category_counts_percent <- data_category_counts_total %>%
  mutate(across(-n_answers_total, ~ round((.x / data_category_counts_total$n_answers_total) * 100, 2))) %>%
  mutate(n_answers_total = 100)


# Step 4: Combine the two with row labels
data_category_counts_total <- rbind(
  cbind(type = "counts_total_absolute", data_category_counts_total),
  cbind(type = "counts_total_percent", data_category_counts_percent)
)

# Step 4: Print the combined table with both counts and percentages
print(data_category_counts_total)

```


## category counts per item

```{r}


# Step 1: Select the columns that end with "_comp"
comp_columns <- comparison_data_processed %>%
  select(ends_with("_comp"))

# Step 2: Create a dataset that counts occurrences of each category for each column using reframe()
data_category_counts_per_item <- comp_columns %>%
  reframe(across(everything(), ~ tibble(
    Identical_YES = sum(. == "Y", na.rm = TRUE),
    Identical_NO = sum(. == "N", na.rm = TRUE),
    Identical_X = sum(. == "X", na.rm = TRUE),
    Different_Y_to_N = sum(. == "P", na.rm = TRUE),
    Different_N_to_Y = sum(. == "Q", na.rm = TRUE),
    GPT_Y_Olmo_X = sum(. == "FY", na.rm = TRUE),
    GPT_N_Olmo_X = sum(. == "FN", na.rm = TRUE),
    GPT_X_Olmo_N = sum(. == "FXN", na.rm = TRUE),
    GPT_X_Olmo_Y = sum(. == "FXY", na.rm = TRUE)
  )))

# Step 3: Transpose the data so that each category has its own column
data_category_counts_per_item <- data_category_counts_per_item %>%
  pivot_longer(cols = everything(), names_to = "Column", values_to = "Counts") %>%
  unnest_wider(Counts)

# Step 4: Print the result
print(data_category_counts_per_item)


write.xlsx(data_category_counts_per_item, "../Data/processed/data_category_counts_per_item.xlsx")


```




# Kappa retest

## test, retest korrelation Cohens Kappa per category

```{r}


# Step 1: Filter the dataset into test and retest subsets
test_data <- comparison_data_processed %>% filter(test_retest == "Test")
retest_data <- comparison_data_processed %>% filter(test_retest == "RETEST")

# Step 2: Match test and retest based on 'psp'(so this only contains the 30 test-retst Psp rows)
retest_merged_data <- merge(test_data, retest_data, by = "psp", suffixes = c("_test", "_retest"))

# Step 3: Get all columns ending with '_comp'
comp_columns <- grep("_comp$", names(comparison_data_processed), value = TRUE)

# Step 4: Loop through each '_comp' column and calculate Cohen's Kappa
retest_kappa_results <- lapply(comp_columns, function(col_name) {
  # Cohen's Kappa requires the two sets of categorical data (Test and RETEST)
  test_col <- retest_merged_data[[paste0(col_name, "_test")]]
  retest_col <- retest_merged_data[[paste0(col_name, "_retest")]]
  
  # Calculate Cohen's Kappa for the current column
  kappa <- cohen.kappa(cbind(test_col, retest_col))
  
  # Return the column name and the kappa value
  data.frame(Column = col_name, Kappa = kappa$kappa)
})

# Combine the results into a single data frame
retest_kappa_summary <- do.call(rbind, retest_kappa_results)

# Step 5: Display the results
print(retest_kappa_summary)


# die drei antweoten mit NAin kappa_summary sind eigentlich perfekt übereinstimemnd, aber da immer nur die gleiche Antwort, dh de geliche Buchstaben eingesetzt wurde gibt es per Definition eine perfekte Übereinstimmung, da es keine Variabilität gibt.


#write to disk forjamovi
write.xlsx(retest_merged_data, file = "../Data/processed/retest_merged_data.xlsx")







```


## versuch 2 mit zusätzlich z wert, agreement % , p wert
```{r}



# Step 1: Filter the dataset into test and retest subsets
test_data <- comparison_data_processed %>% filter(test_retest == "Test")
retest_data <- comparison_data_processed %>% filter(test_retest == "RETEST")

# Step 2: Match test and retest based on 'psp' (so this only contains the 30 test-retest psp rows)
retest_merged_data <- merge(test_data, retest_data, by = "psp", suffixes = c("_test", "_retest"))

# Step 3: Get all columns ending with '_comp'
comp_columns <- grep("_comp$", names(comparison_data_processed), value = TRUE)

# Step 4: Loop through each '_comp' column and calculate Cohen's Kappa using the irr package
retest_kappa_results <- lapply(comp_columns, function(col_name) {
  # Cohen's Kappa requires the two sets of categorical data (Test and RETEST)
  test_col <- retest_merged_data[[paste0(col_name, "_test")]]
  retest_col <- retest_merged_data[[paste0(col_name, "_retest")]]
  
  # Überprüfung auf gültige Daten (Nicht-NA-Daten) in beiden Spalten
  if (all(is.na(test_col)) || all(is.na(retest_col))) {
    # Falls alle Daten NA sind, gib eine Zeile mit NA zurück
    return(data.frame(Column = col_name, Kappa = NA, z_value = NA, 
                      Agreement_Percentage = NA, p_value = NA))
  }
  
  # Berechnung von Cohen's Kappa mit dem irr-Paket
  kappa_result <- kappa2(data.frame(test_col, retest_col), "unweighted")
  
  # Extrahiere den Kappa-Wert und p-Wert
  kappa_value <- kappa_result$value
  p_value <- kappa_result$p.value
  agreement_percentage <- mean(test_col == retest_col, na.rm = TRUE) * 100
  
  # Berechnung der erwarteten Übereinstimmung (p_e)
  test_table <- table(test_col, retest_col)
  n <- sum(test_table)  # Anzahl der Paare
  p_e <- sum(rowSums(test_table) * colSums(test_table)) / (n^2)
  
  # Berechnung der beobachteten Übereinstimmung (p_0)
  p_0 <- agreement_percentage / 100
  
  # Berechnung des Standardfehlers (SE)
  if (p_e == 1) {
    se_kappa <- NA  # Vermeide Division durch 0, wenn p_e = 1 (perfekte Übereinstimmung)
    z_value <- NA
  } else {
    se_kappa <- sqrt((p_0 * (1 - p_0)) / (n * (1 - p_e)^2))
  
    # Berechnung des z-Werts
    z_value <- kappa_value / se_kappa
  }
  
  # Return the column name, Kappa, z-value, agreement %, and p-value
  return(data.frame(Column = col_name, Kappa = kappa_value, z_value = z_value, 
                    Agreement_Percentage = agreement_percentage, p_value = p_value))
})

# Step 5: Combine the results into a single data frame
retest_kappa_summary <- do.call(rbind, retest_kappa_results)

# Step 6: Display the results
print(retest_kappa_summary)

# Optional: Speichern der zusammengeführten Test-Retest-Daten in einer Excel-Datei
write.xlsx(retest_merged_data, file = "../Data/processed/retest_merged_data.xlsx")


```


## test, retest korrelation gesamt kappa...

```{r}

# Schritt 4: Erstelle zwei Vektoren mit allen Test- und Retest-Werten aus den "_comp"-Spalten
all_test_values <- unlist(retest_merged_data[paste0(comp_columns, "_test")])
all_retest_values <- unlist(retest_merged_data[paste0(comp_columns, "_retest")])

# Schritt 5: Berechne Cohen's Kappa für alle Daten zusammen
retest_kappa_all <- cohen.kappa(cbind(all_test_values, all_retest_values))

# Schritt 6: Zeige das Gesamtergebnis an
retest_kappa_all

```


##test, retest korrelation gesamt kappa...v2 mit p wert etc..

p wert noch leer:/

```{r}
# Lade notwendige Bibliotheken
library(dplyr)
library(irr)  # Für Cohen's Kappa aus dem irr Paket
library(openxlsx)  # Für das Schreiben der Datei

# Step 1: Filter the dataset into test and retest subsets
test_data <- comparison_data_processed %>% filter(test_retest == "Test")
retest_data <- comparison_data_processed %>% filter(test_retest == "RETEST")

# Step 2: Match test and retest based on 'psp' (so this only contains the 30 test-retest psp rows)
retest_merged_data <- merge(test_data, retest_data, by = "psp", suffixes = c("_test", "_retest"))

# Step 3: Get all columns ending with '_comp'
comp_columns <- grep("_comp$", names(comparison_data_processed), value = TRUE)

# Schritt 4: Erstelle zwei Vektoren mit allen Test- und Retest-Werten aus den "_comp"-Spalten
all_test_values <- unlist(retest_merged_data[paste0(comp_columns, "_test")])
all_retest_values <- unlist(retest_merged_data[paste0(comp_columns, "_retest")])

# Schritt 5: Berechne Cohen's Kappa für alle Daten zusammen mit dem irr-Paket
kappa_result_all <- kappa2(data.frame(all_test_values, all_retest_values), "unweighted")

# Extrahiere den Kappa-Wert und den p-Wert
kappa_value <- kappa_result_all$value
p_value <- kappa_result_all$p.value
agreement_percentage <- mean(all_test_values == all_retest_values, na.rm = TRUE) * 100

# Berechnung der erwarteten Übereinstimmung (p_e)
test_table <- table(all_test_values, all_retest_values)
n <- sum(test_table)  # Anzahl der Paare
p_e <- sum(rowSums(test_table) * colSums(test_table)) / (n^2)

# Berechnung der beobachteten Übereinstimmung (p_0)
p_0 <- agreement_percentage / 100

# Berechnung des Standardfehlers (SE)
if (p_e == 1) {
  se_kappa <- NA  # Vermeide Division durch 0, wenn p_e = 1 (perfekte Übereinstimmung)
  z_value <- NA
} else {
  se_kappa <- sqrt((p_0 * (1 - p_0)) / (n * (1 - p_e)^2))
  
  # Berechnung des z-Werts
  z_value <- kappa_value / se_kappa
}

# Schritt 6: Zeige das Gesamtergebnis an
retest_kappa_all_summary <- data.frame(
  Kappa = kappa_value,
  Agreement_Percentage = agreement_percentage,
  z_value = z_value,
  p_value = p_value
)

# Zeige das Gesamtergebnis an
print(retest_kappa_all_summary)

```




##  test-retest Überblick kappas

```{r}
# Schritt 1: Rundung der Kappa-Werte auf 2 Nachkommastellen
retest_kappa_summary <- retest_kappa_summary %>%
  mutate(Kappa = round(Kappa, 2))

# Schritt 2: Aufteilen der Tabelle in Kategorien basierend auf Präfixen (pre, c_pre, pap, c_pap, match)
pre_kappa <- retest_kappa_summary %>% filter(grepl("^(pre|c_pre)", Column)) %>% mutate(Source = "pre")
pap_kappa <- retest_kappa_summary %>% filter(grepl("^(pap|c_pap)", Column)) %>% mutate(Source = "pap")
match_kappa <- retest_kappa_summary %>% filter(grepl("^match", Column)) %>% mutate(Source = "match")

# Schritt 3: Setze Zeilennummern für eine saubere Darstellung in jeder Tabelle
pre_kappa <- pre_kappa %>% mutate(Row = row_number())
pap_kappa <- pap_kappa %>% mutate(Row = row_number())
match_kappa <- match_kappa %>% mutate(Row = row_number())

# Schritt 4: Zusammenfügen der drei Tabellen nebeneinander basierend auf der Zeilennummer
# Verwende full_join, um die Tabellen nach den Zeilen zu verbinden
retest_kappa_summary_overview <- pre_kappa %>%
  full_join(pap_kappa, by = "Row", suffix = c("_pre", "_pap")) %>%
  full_join(match_kappa, by = "Row", suffix = c("", "_match"))

# Schritt 5: Auswahl und Umbenennung der Spalten für Übersichtlichkeit
# Stellen Sie sicher, dass die Spalten korrekt benannt und verfügbar sind
retest_kappa_summary_overview <- retest_kappa_summary_overview %>%
  select(Row, 
         Column_pre = Column_pre, Kappa_pre = Kappa_pre, 
         Column_pap = Column_pap, Kappa_pap = Kappa_pap, 
         Column_match = Column, Kappa_match = Kappa) 








```









## test retest färbung überblick

```{r}

# Färbung der Kappa-Werte direkt in der Tabelle
formattable(retest_kappa_summary_overview, list(
  Kappa_pre = color_tile("red", "green"),  # Kappa_pre-Spalte von dunkelrot bis dunkelgrün
  Kappa_pap = color_tile("red", "green"),  # Kappa_pap-Spalte von dunkelrot bis dunkelgrün
  Kappa_match = color_tile("red", "green") # Kappa_match-Spalte von dunkelrot bis dunkelgrün
))


```





## als nächstes könnte man anschauen welche der items besonders gut und besonders schlecht ist.... die annanhmen überprüfen... und noch weitere Analysen..



falsch positiv, falsch negativ etc... analyse...



## 

```{r}



```





## 

```{r}



```





## 

```{r}



```




## 

```{r}



```




## 

```{r}



```





# Session info

```{r}

sessionInfo()

```


