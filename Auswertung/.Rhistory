getwd()
comparison_data_processed <-read_excel("../Data/processed/comparison_data_processed.xlsx")
data_gpt_olmo_processed <-read_excel("../Data/processed/data_gpt_olmo_processed.xlsx")
View(data_gpt_olmo_processed)
View(data_gpt_olmo_processed)
# Schritt 6: Zeige das Gesamtergebnis an
retest_kappa_all
# Function to calculate Cohen's Kappa for each pair of columns (_gpt and _olmo)
calculate_kappa <- function(data, column_gpt, column_olmo) {
# Ensure both columns exist
if(column_gpt %in% colnames(data) & column_olmo %in% colnames(data)) {
# Calculate Cohen's Kappa
kappa_value <- psych::cohen.kappa(cbind(data[[column_gpt]], data[[column_olmo]]))$kappa
return(kappa_value)
} else {
return(NA)
}
}
# Get all columns that have _gpt or _olmo suffix
gpt_columns <- grep("_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns <- gsub("_gpt$", "_olmo", gpt_columns)
# Create a dataframe to store the kappa values
kappa_results <- data.frame(Variable = gpt_columns, Kappa = NA)
# Loop over all _gpt and _olmo column pairs and calculate Kappa
for(i in 1:length(gpt_columns)) {
kappa_results$Kappa[i] <- calculate_kappa(data_gpt_olmo_processed, gpt_columns[i], olmo_columns[i])
}
# Show the results
print(kappa_results)
# Optionally, save the results to a CSV file
write.csv(kappa_results, "kappa_results.csv", row.names = FALSE)
# Show the results
print(kappa_results)
View(kappa_results)
View(kappa_results)
# Function to calculate Cohen's Kappa, p-value, z-value, and percentage agreement
calculate_kappa_stats <- function(data, column_gpt, column_olmo) {
# Ensure both columns exist
if(column_gpt %in% colnames(data) & column_olmo %in% colnames(data)) {
# Calculate Cohen's Kappa
kappa_result <- psych::cohen.kappa(cbind(data[[column_gpt]], data[[column_olmo]]))
# Extract values
kappa_value <- kappa_result$kappa
p_value <- kappa_result$p.value
z_value <- kappa_result$z
percent_agreement <- sum(data[[column_gpt]] == data[[column_olmo]], na.rm = TRUE) / nrow(data) * 100
return(c(kappa_value, p_value, z_value, percent_agreement))
} else {
return(rep(NA, 4))
}
}
# Get all columns that have _gpt or _olmo suffix
gpt_columns <- grep("_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns <- gsub("_gpt$", "_olmo", gpt_columns)
# Create a dataframe to store the results
kappa_results <- data.frame(Variable = gpt_columns, Kappa = NA, P_Value = NA, Z_Value = NA, Percent_Agreement = NA)
# Loop over all _gpt and _olmo column pairs and calculate statistics
for(i in 1:length(gpt_columns)) {
kappa_stats <- calculate_kappa_stats(data_gpt_olmo_processed, gpt_columns[i], olmo_columns[i])
kappa_results[i, 2:5] <- kappa_stats
}
# Show the results
print(kappa_results)
# Function to calculate Cohen's Kappa, p-value, z-value, and percentage agreement using 'irr' package
calculate_kappa_irr <- function(data, column_gpt, column_olmo) {
# Ensure both columns exist
if(column_gpt %in% colnames(data) & column_olmo %in% colnames(data)) {
# Remove rows with NA values to avoid errors in kappa calculation
valid_data <- data[!is.na(data[[column_gpt]]) & !is.na(data[[column_olmo]]), ]
# Calculate Cohen's Kappa using the 'kappa2' function from the irr package
kappa_result <- irr::kappa2(valid_data[, c(column_gpt, column_olmo)], "unweighted")
# Extract values
kappa_value <- kappa_result$value
p_value <- kappa_result$p.value
z_value <- kappa_result$statistic
percent_agreement <- sum(valid_data[[column_gpt]] == valid_data[[column_olmo]]) / nrow(valid_data) * 100
return(c(kappa_value, p_value, z_value, percent_agreement))
} else {
return(rep(NA, 4))
}
}
# Get all columns that have _gpt or _olmo suffix
gpt_columns <- grep("_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns <- gsub("_gpt$", "_olmo", gpt_columns)
# Create a dataframe to store the results
kappa_results <- data.frame(Variable = gpt_columns, Kappa = NA, P_Value = NA, Z_Value = NA, Percent_Agreement = NA)
# Loop over all _gpt and _olmo column pairs and calculate statistics
for(i in 1:length(gpt_columns)) {
kappa_stats <- calculate_kappa_irr(data_gpt_olmo_processed, gpt_columns[i], olmo_columns[i])
kappa_results[i, 2:5] <- kappa_stats
}
# Show the results
print(kappa_results)
# Function to calculate Cohen's Kappa, p-value, z-value, and percentage agreement using 'irr' package
calculate_kappa_irr_v2 <- function(data, column_gpt, column_olmo) {
# Ensure both columns exist
if(column_gpt %in% colnames(data) & column_olmo %in% colnames(data)) {
# Remove rows with NA values to avoid errors in kappa calculation
valid_data_v2 <- data[!is.na(data[[column_gpt]]) & !is.na(data[[column_olmo]]), ]
# Calculate Cohen's Kappa using the 'kappa2' function from the irr package
kappa_result_v2 <- irr::kappa2(valid_data_v2[, c(column_gpt, column_olmo)], "unweighted")
# Extract values
kappa_value_v2 <- kappa_result$value
p_value_v2 <- kappa_result$p.value
z_value_v2 <- kappa_result$statistic
percent_agreement_v2 <- sum(valid_data[[column_gpt]] == valid_data_v2[[column_olmo]]) / nrow(valid_data_v2) * 100
return(c(kappa_value_v2, p_value_v2, z_value_v2, percent_agreement_v2))
} else {
return(rep(NA, 4))
}
}
# Get all columns that have _gpt or _olmo suffix
gpt_columns_v2 <- grep("_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns_v2 <- gsub("_gpt$", "_olmo", gpt_columns_v2)
# Create a dataframe to store the results
kappa_results_v2 <- data.frame(Variable = gpt_columns_v2, Kappa = NA, P_Value = NA, Z_Value = NA, Percent_Agreement = NA)
# Loop over all _gpt and _olmo column pairs and calculate statistics
for(i in 1:length(gpt_columns)) {
kappa_stats_v2 <- calculate_kappa_irr(data_gpt_olmo_processed, gpt_columns_v2[i], olmo_columns_v2[i])
kappa_results_v2[i, 2:5] <- kappa_stats_v2
}
# Show the results
print(kappa_results_v2)
# Function to calculate Cohen's Kappa, p-value, z-value, and percentage agreement using 'irr' package
calculate_kappa_irr_v2 <- function(data, column_gpt, column_olmo) {
# Ensure both columns exist
if(column_gpt %in% colnames(data) & column_olmo %in% colnames(data)) {
# Remove rows with NA values to avoid errors in kappa calculation
valid_data_v2 <- data[!is.na(data[[column_gpt]]) & !is.na(data[[column_olmo]]), ]
# Calculate Cohen's Kappa using the 'kappa2' function from the irr package
kappa_result_v2 <- irr::kappa2(valid_data_v2[, c(column_gpt, column_olmo)], "unweighted")
# Extract values
kappa_value_v2 <- kappa_result$value
p_value_v2 <- kappa_result$p.value
z_value_v2 <- kappa_result$statistic
percent_agreement_v2 <- sum(valid_data[[column_gpt]] == valid_data_v2[[column_olmo]]) / nrow(valid_data_v2) * 100
return(c(kappa_value_v2, p_value_v2, z_value_v2, percent_agreement_v2))
} else {
return(rep(NA, 4))
}
}
# Get all columns that have _gpt or _olmo suffix
gpt_columns_v2 <- grep("_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns_v2 <- gsub("_gpt$", "_olmo", gpt_columns_v2)
# Create a dataframe to store the results
kappa_results_v2 <- data.frame(Variable = gpt_columns_v2, Kappa = NA, P_Value = NA, Z_Value = NA, Percent_Agreement = NA)
# Loop over all _gpt and _olmo column pairs and calculate statistics
for(i in 1:length(gpt_columns)) {
kappa_stats_v2 <- calculate_kappa_irr_v2(data_gpt_olmo_processed, gpt_columns_v2[i], olmo_columns_v2[i])
kappa_results_v2[i, 2:5] <- kappa_stats_v2
}
# Function to calculate Cohen's Kappa, p-value, z-value, and percentage agreement using 'irr' package
calculate_kappa_irr_v2 <- function(data, column_gpt, column_olmo) {
# Ensure both columns exist
if(column_gpt %in% colnames(data) & column_olmo %in% colnames(data)) {
# Remove rows with NA values to avoid errors in kappa calculation
valid_data_v2 <- data[!is.na(data[[column_gpt]]) & !is.na(data[[column_olmo]]), ]
# Calculate Cohen's Kappa using the 'kappa2' function from the irr package
kappa_result_v2 <- irr::kappa2(valid_data_v2[, c(column_gpt, column_olmo)], "unweighted")
# Extract values
kappa_value_v2 <- kappa_result_v2$value
p_value_v2 <- kappa_result_v2$p.value
z_value_v2 <- kappa_result_v2$statistic
percent_agreement_v2 <- sum(valid_data[[column_gpt]] == valid_data_v2[[column_olmo]]) / nrow(valid_data_v2) * 100
return(c(kappa_value_v2, p_value_v2, z_value_v2, percent_agreement_v2))
} else {
return(rep(NA, 4))
}
}
# Get all columns that have _gpt or _olmo suffix
gpt_columns_v2 <- grep("_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns_v2 <- gsub("_gpt$", "_olmo", gpt_columns_v2)
# Create a dataframe to store the results
kappa_results_v2 <- data.frame(Variable = gpt_columns_v2, Kappa = NA, P_Value = NA, Z_Value = NA, Percent_Agreement = NA)
# Loop over all _gpt and _olmo column pairs and calculate statistics
for(i in 1:length(gpt_columns_v2)) {
kappa_stats_v2 <- calculate_kappa_irr_v2(data_gpt_olmo_processed, gpt_columns_v2[i], olmo_columns_v2[i])
kappa_results_v2[i, 2:5] <- kappa_stats_v2
}
# Function to calculate Cohen's Kappa, p-value, z-value, and percentage agreement using 'irr' package
calculate_kappa_irr_v2 <- function(data, column_gpt, column_olmo) {
# Ensure both columns exist
if(column_gpt %in% colnames(data) & column_olmo %in% colnames(data)) {
# Remove rows with NA values to avoid errors in kappa calculation
valid_data_v2 <- data[!is.na(data[[column_gpt]]) & !is.na(data[[column_olmo]]), ]
# Calculate Cohen's Kappa using the 'kappa2' function from the irr package
kappa_result_v2 <- irr::kappa2(valid_data_v2[, c(column_gpt, column_olmo)], "unweighted")
# Extract values
kappa_value_v2 <- kappa_result_v2$value
p_value_v2 <- kappa_result_v2$p.value
z_value_v2 <- kappa_result_v2$statistic
percent_agreement_v2 <- sum(valid_data_v2[[column_gpt]] == valid_data_v2[[column_olmo]]) / nrow(valid_data_v2) * 100
return(c(kappa_value_v2, p_value_v2, z_value_v2, percent_agreement_v2))
} else {
return(rep(NA, 4))
}
}
# Get all columns that have _gpt or _olmo suffix
gpt_columns_v2 <- grep("_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns_v2 <- gsub("_gpt$", "_olmo", gpt_columns_v2)
# Create a dataframe to store the results
kappa_results_v2 <- data.frame(Variable = gpt_columns_v2, Kappa = NA, P_Value = NA, Z_Value = NA, Percent_Agreement = NA)
# Loop over all _gpt and _olmo column pairs and calculate statistics
for(i in 1:length(gpt_columns_v2)) {
kappa_stats_v2 <- calculate_kappa_irr_v2(data_gpt_olmo_processed, gpt_columns_v2[i], olmo_columns_v2[i])
kappa_results_v2[i, 2:5] <- kappa_stats_v2
}
# Show the results
print(kappa_results_v2)
View(kappa_results_v2)
# Finde alle Spalten mit den Suffixen _gpt und _olmo
gpt_columns <- grep("_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns <- gsub("_gpt$", "_olmo", gpt_columns)
# Kombiniere alle gpt und olmo Spalten in einer langen Form
combined_data <- data.frame(GPT = unlist(data_gpt_olmo_processed[gpt_columns], use.names = FALSE),
OLMO = unlist(data_gpt_olmo_processed[olmo_columns], use.names = FALSE))
# Spalten mit _gpt und _olmo suffix finden
gpt_columns <- grep("_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns <- gsub("_gpt$", "_olmo", gpt_columns)
# Überprüfen, ob die entsprechenden _olmo-Spalten existieren
valid_gpt_columns <- gpt_columns[olmo_columns %in% colnames(data_gpt_olmo_processed)]
valid_olmo_columns <- olmo_columns[olmo_columns %in% colnames(data_gpt_olmo_processed)]
# Sicherstellen, dass nur gültige Paare verwendet werden
if(length(valid_gpt_columns) == 0) {
stop("Keine gültigen Paare von GPT- und OLMO-Spalten gefunden!")
}
# Kombinieren der Spalten in einem DataFrame für den Gesamtvergleich
combined_data <- data.frame()
for(i in 1:length(valid_gpt_columns)) {
# Füge die Werte der GPT und OLMO Spalten zur kombinierten Datensatz hinzu
combined_data <- rbind(combined_data, data.frame(GPT = data_gpt_olmo_processed[[valid_gpt_columns[i]]],
OLMO = data_gpt_olmo_processed[[valid_olmo_columns[i]]]))
}
# Entferne NA-Werte
combined_data <- na.omit(combined_data)
# Berechne Cohen's Kappa für den gesamten Test
kappa_result <- irr::kappa2(combined_data, "unweighted")
# Berechne den Prozentsatz der Übereinstimmung
percent_agreement <- sum(combined_data$GPT == combined_data$OLMO) / nrow(combined_data) * 100
# Ergebnisse anzeigen
cat("Cohen's Kappa für den gesamten Test:\n")
cat("Kappa:", kappa_result$value, "\n")
cat("p-Wert:", kappa_result$p.value, "\n")
cat("z-Wert:", kappa_result$statistic, "\n")
cat("Prozentuale Übereinstimmung:", percent_agreement, "%\n")
immung:", percent_agreement_total, "%\n")
# Spalten mit _gpt und _olmo suffix finden
gpt_columns <- grep("_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns <- gsub("_gpt$", "_olmo", gpt_columns)
# Überprüfen, ob die entsprechenden _olmo-Spalten existieren
valid_gpt_columns <- gpt_columns[olmo_columns %in% colnames(data_gpt_olmo_processed)]
valid_olmo_columns <- olmo_columns[olmo_columns %in% colnames(data_gpt_olmo_processed)]
# Sicherstellen, dass nur gültige Paare verwendet werden
if(length(valid_gpt_columns) == 0) {
stop("Keine gültigen Paare von GPT- und OLMO-Spalten gefunden!")
}
# Kombinieren der Spalten in einem DataFrame für den Gesamtvergleich
combined_data <- data.frame()
for(i in 1:length(valid_gpt_columns)) {
# Füge die Werte der GPT und OLMO Spalten zur kombinierten Datensatz hinzu
combined_data <- rbind(combined_data, data.frame(GPT = data_gpt_olmo_processed[[valid_gpt_columns[i]]],
OLMO = data_gpt_olmo_processed[[valid_olmo_columns[i]]]))
}
# Entferne NA-Werte
combined_data <- na.omit(combined_data)
# Berechne Cohen's Kappa für den gesamten Test
kappa_result <- irr::kappa2(combined_data, "unweighted")
# Berechne den Prozentsatz der Übereinstimmung
percent_agreement <- sum(combined_data$GPT == combined_data$OLMO) / nrow(combined_data) * 100
# Ergebnisse anzeigen
cat("Cohen's Kappa für den gesamten Test:\n")
cat("Kappa:", kappa_result$value, "\n")
cat("p-Wert:", kappa_result$p.value, "\n")
cat("z-Wert:", kappa_result$statistic, "\n")
cat("Prozentuale Übereinstimmung:", percent_agreement, "%\n")
cat("Prozentuale Übereinstimmung:", percent_agreement, "%\n")
View(data_category_counts_total)
# Extrahiere die relevanten Werte für Y, N, X (für tatsächliche und vorhergesagte Werte)
actual_Y <- data_category_counts_total$Identical_YES
actual_N <- data_category_counts_total$Identical_NO
actual_X <- data_category_counts_total$Identical_X
pred_Y_to_N <- data_category_counts_total$Different_Y_to_N
pred_N_to_Y <- data_category_counts_total$Different_N_to_Y
pred_Y_to_X <- data_category_counts_total$GPT_Y_Olmo_X
pred_N_to_X <- data_category_counts_total$GPT_N_Olmo_X
pred_X_to_N <- data_category_counts_total$GPT_X_Olmo_N
pred_X_to_Y <- data_category_counts_total$GPT_X_Olmo_Y
# Erstelle eine leere 3x3 Matrix
confusion_matrix <- matrix(0, nrow = 3, ncol = 3,
dimnames = list(c("Actual_Y", "Actual_N", "Actual_X"),
c("Predicted_Y", "Predicted_N", "Predicted_X")))
# Fülle die Konfusionsmatrix basierend auf den Zählungen aus
confusion_matrix["Actual_Y", "Predicted_Y"] <- actual_Y
# Angenommen, deine 2x11-Tabelle heißt "data_category_counts_total"
# und enthält die Zählungen für jede Kategorie (z.B. Y, N, X).
# Extrahiere nur die relevanten Werte für Y, N, X (jeweils für tatsächliche und vorhergesagte Werte)
actual_Y <- data_category_counts_total$Identical_YES[1]  # Zähle die Y-Zuordnungen
actual_N <- data_category_counts_total$Identical_NO[1]   # Zähle die N-Zuordnungen
actual_X <- data_category_counts_total$Identical_X[1]    # Zähle die X-Zuordnungen
pred_Y_to_N <- data_category_counts_total$Different_Y_to_N[1]  # Y -> N Zählungen
pred_N_to_Y <- data_category_counts_total$Different_N_to_Y[1]  # N -> Y Zählungen
pred_Y_to_X <- data_category_counts_total$GPT_Y_Olmo_X[1]      # Y -> X Zählungen
pred_N_to_X <- data_category_counts_total$GPT_N_Olmo_X[1]      # N -> X Zählungen
pred_X_to_N <- data_category_counts_total$GPT_X_Olmo_N[1]      # X -> N Zählungen
pred_X_to_Y <- data_category_counts_total$GPT_X_Olmo_Y[1]      # X -> Y Zählungen
# Erstelle eine leere 3x3-Matrix
confusion_matrix <- matrix(0, nrow = 3, ncol = 3,
dimnames = list(c("Actual_Y", "Actual_N", "Actual_X"),
c("Predicted_Y", "Predicted_N", "Predicted_X")))
# Fülle die Konfusionsmatrix basierend auf den Zählungen aus
confusion_matrix["Actual_Y", "Predicted_Y"] <- actual_Y
confusion_matrix["Actual_N", "Predicted_N"] <- actual_N
confusion_matrix["Actual_X", "Predicted_X"] <- actual_X
# Füge die 'False Positives' und 'False Negatives' hinzu
confusion_matrix["Actual_Y", "Predicted_N"] <- pred_Y_to_N
confusion_matrix["Actual_N", "Predicted_Y"] <- pred_N_to_Y
confusion_matrix["Actual_Y", "Predicted_X"] <- pred_Y_to_X
confusion_matrix["Actual_X", "Predicted_Y"] <- pred_X_to_Y
confusion_matrix["Actual_N", "Predicted_X"] <- pred_N_to_X
confusion_matrix["Actual_X", "Predicted_N"] <- pred_X_to_N
# Ausgabe der 3x3 Konfusionsmatrix
print(confusion_matrix)
View(data_category_counts_per_item)
write.xlsx(data_category_counts_total, "../Data/processed/data_category_counts_total.xlsx")
View(comparison_data_processed)
print(confusion_matrix)
print(confusion_matrix)
# Using the existing confusion_matrix variable
# Calculate Sensitivity and Specificity for each class (One-vs-Rest)
# Y Class
TP_Y = confusion_matrix.loc['Actual_Y', 'Predicted_Y']
print(confusion_matrix)
# Berechnung der Sensitivität und Spezifität für jede Klasse (One-vs-Rest)
# Y-Klasse (Sensitivität und Spezifität)
TP_Y <- confusion_matrix['Actual_Y', 'Predicted_Y']
FN_Y <- sum(confusion_matrix['Actual_Y', ]) - TP_Y
FP_Y <- sum(confusion_matrix[, 'Predicted_Y']) - TP_Y
TN_Y <- sum(confusion_matrix) - (TP_Y + FN_Y + FP_Y)
sensitivity_Y <- TP_Y / (TP_Y + FN_Y)
specificity_Y <- TN_Y / (TN_Y + FP_Y)
# N-Klasse (Sensitivität und Spezifität)
TP_N <- confusion_matrix['Actual_N', 'Predicted_N']
FN_N <- sum(confusion_matrix['Actual_N', ]) - TP_N
FP_N <- sum(confusion_matrix[, 'Predicted_N']) - TP_N
TN_N <- sum(confusion_matrix) - (TP_N + FN_N + FP_N)
sensitivity_N <- TP_N / (TP_N + FN_N)
specificity_N <- TN_N / (TN_N + FP_N)
# X-Klasse (Sensitivität und Spezifität)
TP_X <- confusion_matrix['Actual_X', 'Predicted_X']
FN_X <- sum(confusion_matrix['Actual_X', ]) - TP_X
FP_X <- sum(confusion_matrix[, 'Predicted_X']) - TP_X
TN_X <- sum(confusion_matrix) - (TP_X + FN_X + FP_X)
sensitivity_X <- TP_X / (TP_X + FN_X)
specificity_X <- TN_X / (TN_X + FP_X)
# Ergebnisse anzeigen
results <- data.frame(
Class = c("Y", "N", "X"),
Sensitivity = c(sensitivity_Y, sensitivity_N, sensitivity_X),
Specificity = c(specificity_Y, specificity_N, specificity_X)
)
print(results)
# Macro-Averaging (Mittelwert der Sensitivitäten und Spezifitäten)
macro_sensitivity <- mean(c(sensitivity_Y, sensitivity_N, sensitivity_X))
macro_specificity <- mean(c(specificity_Y, specificity_N, specificity_X))
# Micro-Averaging (Summe der TP, FN, FP, TN über alle Klassen)
TP_total <- TP_Y + TP_N + TP_X
FN_total <- FN_Y + FN_N + FN_X
FP_total <- FP_Y + FP_N + FP_X
TN_total <- TN_Y + TN_N + TN_X
micro_sensitivity <- TP_total / (TP_total + FN_total)
micro_specificity <- TN_total / (TN_total + FP_total)
# Ergebnisse für Macro- und Micro-Averaging anzeigen
overall_results <- data.frame(
Metric = c("Macro Sensitivity", "Macro Specificity", "Micro Sensitivity", "Micro Specificity"),
Value = c(macro_sensitivity, macro_specificity, micro_sensitivity, micro_specificity)
)
print(overall_results)
# Spalten mit _gpt und _olmo suffix finden
gpt_columns_PromptV <- grep("_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns_PromptV <- gsub("_gpt$", "_olmo", gpt_columns_PromptV)
# Überprüfen, ob die entsprechenden _olmo-Spalten existieren
valid_gpt_columns_PromptV <- gpt_columns_PromptV[olmo_columns_PromptV %in% colnames(data_gpt_olmo_processed)]
valid_olmo_columns_PromptV <- olmo_columns_PromptV[olmo_columns_PromptV %in% colnames(data_gpt_olmo_processed)]
# Sicherstellen, dass nur gültige Paare verwendet werden
if(length(valid_gpt_columns_PromptV) == 0) {
stop("Keine gültigen Paare von GPT- und OLMO-Spalten gefunden!")
}
# Definiere die verschiedenen Versionen, die in prompt_version auftreten (A, B, C, D)
versions_PromptV <- c("A", "B", "C", "D")
# Schleife durch jede Version und berechne das Kappa für jede
for (version_PromptV in versions_PromptV) {
# Filtere die Zeilen, die zur aktuellen Version gehören
version_data_PromptV <- data_gpt_olmo_processed[data_gpt_olmo_processed$prompt_version == version_PromptV, ]
# Kombinieren der Spalten in einem DataFrame für den Vergleich der aktuellen Version
combined_data_PromptV <- data.frame()
for(i in 1:length(valid_gpt_columns_PromptV)) {
# Füge die Werte der GPT und OLMO Spalten zur kombinierten Datensatz hinzu
combined_data_PromptV <- rbind(combined_data_PromptV, data.frame(GPT = version_data_PromptV[[valid_gpt_columns_PromptV[i]]],
OLMO = version_data_PromptV[[valid_olmo_columns_PromptV[i]]]))
}
# Entferne NA-Werte
combined_data_PromptV <- na.omit(combined_data_PromptV)
# Berechne Cohen's Kappa für die aktuelle Version
kappa_result_PromptV <- irr::kappa2(combined_data_PromptV, "unweighted")
# Berechne den Prozentsatz der Übereinstimmung
percent_agreement_PromptV <- sum(combined_data_PromptV$GPT == combined_data_PromptV$OLMO) / nrow(combined_data_PromptV) * 100
# Ergebnisse anzeigen
cat("\nCohen's Kappa für Version", version_PromptV, ":\n")
cat("Kappa:", kappa_result_PromptV$value, "\n")
cat("p-Wert:", kappa_result_PromptV$p.value, "\n")
cat("z-Wert:", kappa_result_PromptV$statistic, "\n")
cat("Prozentuale Übereinstimmung:", percent_agreement_PromptV, "%\n")
}
# Spalten mit _gpt und _olmo suffix finden
gpt_columns_PromptV <- grep("_gpt$", colnames(data_gpt_olmo_processed), value = TRUE)
olmo_columns_PromptV <- gsub("_gpt$", "_olmo", gpt_columns_PromptV)
# Überprüfen, ob die entsprechenden _olmo-Spalten existieren
valid_gpt_columns_PromptV <- gpt_columns_PromptV[olmo_columns_PromptV %in% colnames(data_gpt_olmo_processed)]
valid_olmo_columns_PromptV <- olmo_columns_PromptV[olmo_columns_PromptV %in% colnames(data_gpt_olmo_processed)]
# Sicherstellen, dass nur gültige Paare verwendet werden
if(length(valid_gpt_columns_PromptV) == 0) {
stop("Keine gültigen Paare von GPT- und OLMO-Spalten gefunden!")
}
# Zusammenführen der Daten für den Vergleich
combined_data_all <- data.frame()
for(i in 1:length(valid_gpt_columns_PromptV)) {
combined_data_all <- rbind(combined_data_all, data.frame(
GPT = data_gpt_olmo_processed[[valid_gpt_columns_PromptV[i]]],
OLMO = data_gpt_olmo_processed[[valid_olmo_columns_PromptV[i]]],
prompt_version = data_gpt_olmo_processed$prompt_version
))
}
# Entferne NA-Werte
combined_data_all <- na.omit(combined_data_all)
# Kreuztabelle für GPT-Werte vs. prompt_version
gpt_table <- table(combined_data_all$GPT, combined_data_all$prompt_version)
# Chi-Quadrat-Test für GPT-Werte vs. prompt_version
gpt_chisq <- chisq.test(gpt_table)
# Ergebnisse des Chi-Quadrat-Tests anzeigen
cat("Chi-Quadrat-Test für GPT-Werte vs. prompt_version:\n")
cat("Chi-Quadrat-Wert:", gpt_chisq$statistic, "\n")
cat("p-Wert:", gpt_chisq$p.value, "\n")
cat("Freiheitsgrade:", gpt_chisq$parameter, "\n")
cat("Erwartete Häufigkeiten:\n")
print(gpt_chisq$expected)
# Kreuztabelle für OLMO-Werte vs. prompt_version
olmo_table <- table(combined_data_all$OLMO, combined_data_all$prompt_version)
# Chi-Quadrat-Test für OLMO-Werte vs. prompt_version
olmo_chisq <- chisq.test(olmo_table)
# Ergebnisse des Chi-Quadrat-Tests anzeigen
cat("\nChi-Quadrat-Test für OLMO-Werte vs. prompt_version:\n")
cat("Chi-Quadrat-Wert:", olmo_chisq$statistic, "\n")
cat("p-Wert:", olmo_chisq$p.value, "\n")
cat("Freiheitsgrade:", olmo_chisq$parameter, "\n")
cat("Erwartete Häufigkeiten:\n")
print(olmo_chisq$expected)
# Funktion, um den z-Wert für den Unterschied zwischen zwei Kappa-Werten zu berechnen
compare_kappas <- function(kappa1, se1, kappa2, se2) {
z_value <- (kappa1 - kappa2) / sqrt(se1^2 + se2^2)
p_value <- 2 * (1 - pnorm(abs(z_value)))  # zweiseitiger Test
return(list(z = z_value, p = p_value))
}
# Definiere die verschiedenen Versionen, die in prompt_version auftreten (A, B, C, D)
versions_PromptV <- c("A", "B", "C", "D")
# Liste, um die Kappa-Ergebnisse für jede Version zu speichern
kappa_results_PromptV <- list()
# Schleife durch jede Version und berechne das Kappa für jede
for (version_PromptV in versions_PromptV) {
# Filtere die Zeilen, die zur aktuellen Version gehören
version_data_PromptV <- data_gpt_olmo_processed[data_gpt_olmo_processed$prompt_version == version_PromptV, ]
# Kombinieren der Spalten in einem DataFrame für den Vergleich der aktuellen Version
combined_data_PromptV <- data.frame()
for(i in 1:length(valid_gpt_columns_PromptV)) {
# Füge die Werte der GPT und OLMO Spalten zur kombinierten Datensatz hinzu
combined_data_PromptV <- rbind(combined_data_PromptV, data.frame(GPT = version_data_PromptV[[valid_gpt_columns_PromptV[i]]],
OLMO = version_data_PromptV[[valid_olmo_columns_PromptV[i]]]))
}
# Entferne NA-Werte
combined_data_PromptV <- na.omit(combined_data_PromptV)
# Berechne Cohen's Kappa für die aktuelle Version
kappa_result_PromptV <- irr::kappa2(combined_data_PromptV, "unweighted")
# Speichere Kappa und Standardfehler in der Liste
kappa_results_PromptV[[version_PromptV]] <- list(kappa = kappa_result_PromptV$value,
se = sqrt(1 / kappa_result_PromptV$n[1] + 1 / kappa_result_PromptV$n[2]))
# Ergebnisse anzeigen
cat("\nCohen's Kappa für Version", version_PromptV, ":\n")
cat("Kappa:", kappa_result_PromptV$value, "\n")
cat("p-Wert:", kappa_result_PromptV$p.value, "\n")
cat("z-Wert:", kappa_result_PromptV$statistic, "\n")
}
# Vergleiche der Kappa-Werte zwischen den Versionen
for (i in 1:(length(versions_PromptV) - 1)) {
for (j in (i + 1):length(versions_PromptV)) {
version1 <- versions_PromptV[i]
version2 <- versions_PromptV[j]
kappa1 <- kappa_results_PromptV[[version1]]$kappa
se1 <- kappa_results_PromptV[[version1]]$se
kappa2 <- kappa_results_PromptV[[version2]]$kappa
se2 <- kappa_results_PromptV[[version2]]$se
# Berechne den z-Wert und p-Wert für den Unterschied
result <- compare_kappas(kappa1, se1, kappa2, se2)
# Ergebnisse anzeigen
cat("\nVergleich von Version", version1, "und Version", version2, ":\n")
cat("z-Wert:", result$z, "\n")
cat("p-Wert:", result$p, "\n")
}
}
View(kappa_result)
View(kappa_results_v2)
View(data_category_counts_per_item)
View(kappa_results)
View(kappa_results_v2)
